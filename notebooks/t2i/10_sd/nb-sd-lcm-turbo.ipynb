{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60995c52",
   "metadata": {},
   "source": [
    "## PrerequisitesÔºàÂâçÁΩÆÊ¢ù‰ª∂Ôºâ\n",
    "\"\"\"\n",
    "Package Requirements:\n",
    "- diffusers>=0.30.0 (LCM/Turbo support)\n",
    "- transformers>=4.42\n",
    "- accelerate>=0.33\n",
    "- xformers (optional, for memory efficiency)\n",
    "\n",
    "Hardware Requirements:\n",
    "- Minimum: 4GB VRAM (with CPU offload)\n",
    "- Recommended: 8GB+ VRAM\n",
    "- LCM advantage: Much faster on lower-end hardware\n",
    "\n",
    "Model Licenses:\n",
    "- SD-Turbo: Non-commercial research license\n",
    "- SDXL-Turbo: Non-commercial research license  \n",
    "- LCM-LoRA: Apache 2.0 (can be used commercially)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae7dd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCM/Turbo Âä†ÈÄüÊé®Ë´ñÊäÄË°ì\n",
    "# Stage 1 - Cross-Family Inference\n",
    "# Notebook: nb-sd-lcm-turbo.ipynb\n",
    "# Goal: Master LCM/Turbo acceleration for real-time T2I inference\n",
    "\n",
    "# %% [1] Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (Ë§áË£ΩÂà∞ÊØèÊú¨ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\n",
    "    f\"[Cache] {AI_CACHE_ROOT} | GPU: {torch.cuda.is_available()} | VRAM: {torch.cuda.get_device_properties(0).total_memory // 1024**3 if torch.cuda.is_available() else 0}GB\"\n",
    ")\n",
    "# Create outputs directory\n",
    "output_dir = pathlib.Path(\"outputs/lcm_turbo\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd86cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Cell 2: Core Dependencies ====================\n",
    "# Install missing packages (uncomment if needed)\n",
    "# !pip install diffusers[torch]>=0.30.0 transformers accelerate xformers --upgrade\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from diffusers import (\n",
    "    StableDiffusionPipeline,\n",
    "    StableDiffusionXLPipeline,\n",
    "    DiffusionPipeline,\n",
    "    LCMScheduler,\n",
    "    DPMSolverMultistepScheduler,\n",
    "    EulerDiscreteScheduler,\n",
    ")\n",
    "from diffusers.utils import load_image\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SMOKE_MODE for CI testing\n",
    "SMOKE_MODE = os.getenv(\"SMOKE_MODE\", \"false\").lower() == \"true\"\n",
    "print(f\"[Mode] SMOKE_MODE: {SMOKE_MODE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688e902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Cell 3: Model Configuration ====================\n",
    "# Model configurations for different VRAM levels\n",
    "MODEL_CONFIGS = {\n",
    "    \"sd_turbo\": {\n",
    "        \"model_id\": \"stabilityai/sd-turbo\",\n",
    "        \"torch_dtype\": torch.float16,\n",
    "        \"min_vram_gb\": 3,\n",
    "        \"optimal_steps\": 1,\n",
    "        \"guidance_scale\": 0.0,  # Classifier-free guidance disabled for turbo\n",
    "        \"description\": \"SD 1.5 based, ultra-fast single step inference\",\n",
    "    },\n",
    "    \"sdxl_turbo\": {\n",
    "        \"model_id\": \"stabilityai/sdxl-turbo\",\n",
    "        \"torch_dtype\": torch.float16,\n",
    "        \"min_vram_gb\": 5,\n",
    "        \"optimal_steps\": 1,\n",
    "        \"guidance_scale\": 0.0,\n",
    "        \"description\": \"SDXL based, higher quality single step\",\n",
    "    },\n",
    "    \"lcm_lora_sd15\": {\n",
    "        \"model_id\": \"runwayml/stable-diffusion-v1-5\",\n",
    "        \"lora_id\": \"latent-consistency/lcm-lora-sdv1-5\",\n",
    "        \"torch_dtype\": torch.float16,\n",
    "        \"min_vram_gb\": 4,\n",
    "        \"optimal_steps\": 4,\n",
    "        \"guidance_scale\": 1.0,\n",
    "        \"description\": \"SD 1.5 + LCM LoRA, 4-8 steps, more flexible\",\n",
    "    },\n",
    "    \"lcm_lora_sdxl\": {\n",
    "        \"model_id\": \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "        \"lora_id\": \"latent-consistency/lcm-lora-sdxl\",\n",
    "        \"torch_dtype\": torch.float16,\n",
    "        \"min_vram_gb\": 6,\n",
    "        \"optimal_steps\": 4,\n",
    "        \"guidance_scale\": 1.0,\n",
    "        \"description\": \"SDXL + LCM LoRA, 4-8 steps, highest quality\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Print available models\n",
    "print(\"Available LCM/Turbo Models:\")\n",
    "for key, config in MODEL_CONFIGS.items():\n",
    "    print(f\"  {key}: {config['description']} (min {config['min_vram_gb']}GB VRAM)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b024a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Cell 4: VRAM Detection & Model Selection ====================\n",
    "def get_vram_gb():\n",
    "    \"\"\"Get available VRAM in GB\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return 0\n",
    "    return torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "\n",
    "\n",
    "def select_best_model(vram_gb, prefer_quality=True):\n",
    "    \"\"\"Select best model configuration based on available VRAM\"\"\"\n",
    "    suitable_models = [\n",
    "        (k, v) for k, v in MODEL_CONFIGS.items() if v[\"min_vram_gb\"] <= vram_gb\n",
    "    ]\n",
    "\n",
    "    if not suitable_models:\n",
    "        return \"sd_turbo\", MODEL_CONFIGS[\"sd_turbo\"]  # Fallback\n",
    "\n",
    "    if prefer_quality:\n",
    "        # Sort by min_vram_gb descending (higher VRAM models are usually better quality)\n",
    "        suitable_models.sort(key=lambda x: x[1][\"min_vram_gb\"], reverse=True)\n",
    "    else:\n",
    "        # Sort by min_vram_gb ascending (lower VRAM for speed)\n",
    "        suitable_models.sort(key=lambda x: x[1][\"min_vram_gb\"])\n",
    "\n",
    "    return suitable_models[0]\n",
    "\n",
    "\n",
    "# Auto-select model based on VRAM\n",
    "current_vram = get_vram_gb()\n",
    "selected_model_key, selected_config = select_best_model(\n",
    "    current_vram, prefer_quality=True\n",
    ")\n",
    "\n",
    "print(f\"[VRAM] Available: {current_vram:.1f}GB\")\n",
    "print(f\"[Model] Selected: {selected_model_key} - {selected_config['description']}\")\n",
    "\n",
    "# Override for SMOKE_MODE (use fastest model)\n",
    "if SMOKE_MODE:\n",
    "    selected_model_key, selected_config = \"sd_turbo\", MODEL_CONFIGS[\"sd_turbo\"]\n",
    "    print(f\"[SMOKE] Overriding to: {selected_model_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f33f68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Cell 5: Pipeline Loading Functions ====================\n",
    "def load_turbo_pipeline(\n",
    "    model_key=\"sd_turbo\", enable_cpu_offload=True, enable_attention_slicing=True\n",
    "):\n",
    "    \"\"\"Load SD-Turbo or SDXL-Turbo pipeline with memory optimizations\"\"\"\n",
    "    config = MODEL_CONFIGS[model_key]\n",
    "\n",
    "    print(f\"Loading {model_key} pipeline...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load base pipeline\n",
    "    if \"sdxl\" in model_key:\n",
    "        pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "            config[\"model_id\"],\n",
    "            torch_dtype=config[\"torch_dtype\"],\n",
    "            use_safetensors=True,\n",
    "            variant=\"fp16\",\n",
    "        )\n",
    "    else:\n",
    "        pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "            config[\"model_id\"],\n",
    "            torch_dtype=config[\"torch_dtype\"],\n",
    "            use_safetensors=True,\n",
    "            variant=\"fp16\",\n",
    "        )\n",
    "\n",
    "    # Memory optimizations\n",
    "    if enable_attention_slicing:\n",
    "        pipeline.enable_attention_slicing()\n",
    "        print(\"  ‚úì Attention slicing enabled\")\n",
    "\n",
    "    if enable_cpu_offload and torch.cuda.is_available():\n",
    "        pipeline.enable_sequential_cpu_offload()\n",
    "        print(\"  ‚úì Sequential CPU offload enabled\")\n",
    "    else:\n",
    "        pipeline = pipeline.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Enable memory efficient attention if available\n",
    "    try:\n",
    "        pipeline.enable_xformers_memory_efficient_attention()\n",
    "        print(\"  ‚úì xFormers attention enabled\")\n",
    "    except:\n",
    "        print(\"  ‚ö† xFormers not available\")\n",
    "\n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"  ‚úì Pipeline loaded in {load_time:.2f}s\")\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def load_lcm_lora_pipeline(\n",
    "    model_key=\"lcm_lora_sd15\", enable_cpu_offload=True, enable_attention_slicing=True\n",
    "):\n",
    "    \"\"\"Load LCM-LoRA pipeline with base model + LoRA adapter\"\"\"\n",
    "    config = MODEL_CONFIGS[model_key]\n",
    "\n",
    "    print(f\"Loading {model_key} pipeline...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load base pipeline\n",
    "    if \"sdxl\" in model_key:\n",
    "        pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "            config[\"model_id\"],\n",
    "            torch_dtype=config[\"torch_dtype\"],\n",
    "            use_safetensors=True,\n",
    "            variant=\"fp16\",\n",
    "        )\n",
    "    else:\n",
    "        pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "            config[\"model_id\"],\n",
    "            torch_dtype=config[\"torch_dtype\"],\n",
    "            use_safetensors=True,\n",
    "            variant=\"fp16\",\n",
    "        )\n",
    "\n",
    "    # Load LCM LoRA adapter\n",
    "    pipeline.load_lora_weights(config[\"lora_id\"])\n",
    "    print(f\"  ‚úì LCM LoRA loaded: {config['lora_id']}\")\n",
    "\n",
    "    # Set LCM scheduler\n",
    "    pipeline.scheduler = LCMScheduler.from_config(pipeline.scheduler.config)\n",
    "    print(\"  ‚úì LCM Scheduler configured\")\n",
    "\n",
    "    # Memory optimizations (same as turbo)\n",
    "    if enable_attention_slicing:\n",
    "        pipeline.enable_attention_slicing()\n",
    "        print(\"  ‚úì Attention slicing enabled\")\n",
    "\n",
    "    if enable_cpu_offload and torch.cuda.is_available():\n",
    "        pipeline.enable_sequential_cpu_offload()\n",
    "        print(\"  ‚úì Sequential CPU offload enabled\")\n",
    "    else:\n",
    "        pipeline = pipeline.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    try:\n",
    "        pipeline.enable_xformers_memory_efficient_attention()\n",
    "        print(\"  ‚úì xFormers attention enabled\")\n",
    "    except:\n",
    "        print(\"  ‚ö† xFormers not available\")\n",
    "\n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"  ‚úì Pipeline loaded in {load_time:.2f}s\")\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba5703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Cell 6: Load Selected Pipeline ====================\n",
    "# Load the selected pipeline\n",
    "if \"turbo\" in selected_model_key:\n",
    "    pipe = load_turbo_pipeline(selected_model_key)\n",
    "else:\n",
    "    pipe = load_lcm_lora_pipeline(selected_model_key)\n",
    "\n",
    "# Get model info for logging\n",
    "model_info = {\n",
    "    \"model_key\": selected_model_key,\n",
    "    \"model_id\": selected_config[\"model_id\"],\n",
    "    \"optimal_steps\": selected_config[\"optimal_steps\"],\n",
    "    \"guidance_scale\": selected_config[\"guidance_scale\"],\n",
    "    \"vram_gb\": current_vram,\n",
    "    \"smoke_mode\": SMOKE_MODE,\n",
    "}\n",
    "\n",
    "print(f\"\\n[Ready] {selected_model_key} pipeline loaded and optimized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcf6488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Cell 7: Basic LCM/Turbo Generation ====================\n",
    "def generate_lcm_turbo(\n",
    "    prompt,\n",
    "    negative_prompt=\"\",\n",
    "    num_inference_steps=None,\n",
    "    guidance_scale=None,\n",
    "    width=512,\n",
    "    height=512,\n",
    "    seed=42,\n",
    "    save_path=None,\n",
    "):\n",
    "    \"\"\"Generate image using LCM/Turbo with optimal settings\"\"\"\n",
    "\n",
    "    # Use model defaults if not specified\n",
    "    if num_inference_steps is None:\n",
    "        num_inference_steps = selected_config[\"optimal_steps\"]\n",
    "    if guidance_scale is None:\n",
    "        guidance_scale = selected_config[\"guidance_scale\"]\n",
    "\n",
    "    # Adjust resolution for SMOKE_MODE\n",
    "    if SMOKE_MODE:\n",
    "        width, height = 256, 256\n",
    "        num_inference_steps = 1\n",
    "\n",
    "    print(f\"Generating: '{prompt[:50]}{'...' if len(prompt) > 50 else ''}'\")\n",
    "    print(\n",
    "        f\"  Steps: {num_inference_steps}, CFG: {guidance_scale}, Size: {width}x{height}\"\n",
    "    )\n",
    "\n",
    "    # Generate\n",
    "    generator = torch.Generator(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    generator.manual_seed(seed)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Handle different pipeline types\n",
    "    if \"sdxl\" in selected_model_key:\n",
    "        # SDXL has different parameter names\n",
    "        result = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt if negative_prompt else None,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            generator=generator,\n",
    "        )\n",
    "    else:\n",
    "        # SD 1.5 based\n",
    "        result = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt if negative_prompt else None,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            generator=generator,\n",
    "        )\n",
    "\n",
    "    generation_time = time.time() - start_time\n",
    "    image = result.images[0]\n",
    "\n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        image.save(save_path)\n",
    "        print(f\"  ‚úì Saved to: {save_path}\")\n",
    "\n",
    "    print(\n",
    "        f\"  ‚úì Generated in {generation_time:.2f}s ({generation_time/num_inference_steps:.2f}s/step)\"\n",
    "    )\n",
    "\n",
    "    return image, generation_time\n",
    "\n",
    "\n",
    "# Example generation\n",
    "test_prompt = \"a cute red panda eating bamboo, photorealistic, high quality\"\n",
    "test_negative = \"blurry, low quality, distorted\"\n",
    "\n",
    "example_image, gen_time = generate_lcm_turbo(\n",
    "    prompt=test_prompt,\n",
    "    negative_prompt=test_negative,\n",
    "    seed=42,\n",
    "    save_path=output_dir / f\"example_{selected_model_key}.png\",\n",
    ")\n",
    "\n",
    "# Display result\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(example_image)\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"{selected_model_key.upper()} - {gen_time:.2f}s\\n{test_prompt}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    output_dir / f\"display_{selected_model_key}.png\", dpi=100, bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5c1646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Cell 8: Speed vs Quality Comparison ====================\n",
    "def compare_schedulers(prompt, steps_range=[1, 2, 4, 8], seed=42):\n",
    "    \"\"\"Compare generation quality across different step counts\"\"\"\n",
    "\n",
    "    if SMOKE_MODE:\n",
    "        steps_range = [1, 2]  # Minimal for testing\n",
    "\n",
    "    results = []\n",
    "\n",
    "    print(f\"Comparing steps: {steps_range}\")\n",
    "\n",
    "    for steps in steps_range:\n",
    "        print(f\"\\n--- Testing {steps} steps ---\")\n",
    "\n",
    "        image, gen_time = generate_lcm_turbo(\n",
    "            prompt=prompt,\n",
    "            num_inference_steps=steps,\n",
    "            seed=seed,\n",
    "            width=512 if not SMOKE_MODE else 256,\n",
    "            height=512 if not SMOKE_MODE else 256,\n",
    "        )\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"steps\": steps,\n",
    "                \"image\": image,\n",
    "                \"time\": gen_time,\n",
    "                \"time_per_step\": gen_time / steps,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run comparison\n",
    "comparison_prompt = (\n",
    "    \"a majestic lion in golden savanna, cinematic lighting, award winning photography\"\n",
    ")\n",
    "comparison_results = compare_schedulers(comparison_prompt, seed=123)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(\n",
    "    1, len(comparison_results), figsize=(4 * len(comparison_results), 4)\n",
    ")\n",
    "if len(comparison_results) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, result in enumerate(comparison_results):\n",
    "    axes[i].imshow(result[\"image\"])\n",
    "    axes[i].set_title(f\"{result['steps']} steps\\n{result['time']:.2f}s\")\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"LCM/Turbo Step Comparison - {selected_model_key.upper()}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    output_dir / f\"comparison_{selected_model_key}.png\", dpi=100, bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Print performance summary\n",
    "print(\"\\n=== Performance Summary ===\")\n",
    "for result in comparison_results:\n",
    "    print(\n",
    "        f\"{result['steps']:2d} steps: {result['time']:5.2f}s total, {result['time_per_step']:.3f}s/step\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64256893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Cell 9: Traditional vs LCM Speed Benchmark ====================\n",
    "def benchmark_traditional_vs_lcm(prompt, seed=42):\n",
    "    \"\"\"Compare LCM/Turbo against traditional schedulers\"\"\"\n",
    "\n",
    "    if SMOKE_MODE:\n",
    "        print(\"[SMOKE] Skipping traditional benchmark\")\n",
    "        return []\n",
    "\n",
    "    print(\"Loading traditional SD pipeline for comparison...\")\n",
    "\n",
    "    # Load a traditional pipeline for comparison\n",
    "    if \"sdxl\" in selected_model_key:\n",
    "        traditional_pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "            \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "            torch_dtype=torch.float16,\n",
    "            use_safetensors=True,\n",
    "            variant=\"fp16\",\n",
    "        )\n",
    "    else:\n",
    "        traditional_pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            \"runwayml/stable-diffusion-v1-5\",\n",
    "            torch_dtype=torch.float16,\n",
    "            use_safetensors=True,\n",
    "            variant=\"fp16\",\n",
    "        )\n",
    "\n",
    "    # Apply same optimizations\n",
    "    traditional_pipe.enable_attention_slicing()\n",
    "    if torch.cuda.is_available():\n",
    "        traditional_pipe.enable_sequential_cpu_offload()\n",
    "\n",
    "    # Test different scheduler configurations\n",
    "    test_configs = [\n",
    "        {\n",
    "            \"name\": \"LCM/Turbo\",\n",
    "            \"pipeline\": pipe,\n",
    "            \"steps\": selected_config[\"optimal_steps\"],\n",
    "            \"guidance_scale\": selected_config[\"guidance_scale\"],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"DDIM 20\",\n",
    "            \"pipeline\": traditional_pipe,\n",
    "            \"steps\": 20,\n",
    "            \"guidance_scale\": 7.5,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"DPM++ 25\",\n",
    "            \"pipeline\": traditional_pipe,\n",
    "            \"steps\": 25,\n",
    "            \"guidance_scale\": 7.5,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Set DPM++ scheduler for traditional pipeline\n",
    "    traditional_pipe.scheduler = DPMSolverMultistepScheduler.from_config(\n",
    "        traditional_pipe.scheduler.config\n",
    "    )\n",
    "\n",
    "    benchmark_results = []\n",
    "\n",
    "    for config in test_configs:\n",
    "        print(f\"\\n--- Benchmarking {config['name']} ---\")\n",
    "\n",
    "        generator = torch.Generator(\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        generator.manual_seed(seed)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        result = config[\"pipeline\"](\n",
    "            prompt=prompt,\n",
    "            num_inference_steps=config[\"steps\"],\n",
    "            guidance_scale=config[\"guidance_scale\"],\n",
    "            width=512,\n",
    "            height=512,\n",
    "            generator=generator,\n",
    "        )\n",
    "\n",
    "        generation_time = time.time() - start_time\n",
    "\n",
    "        benchmark_results.append(\n",
    "            {\n",
    "                \"name\": config[\"name\"],\n",
    "                \"image\": result.images[0],\n",
    "                \"time\": generation_time,\n",
    "                \"steps\": config[\"steps\"],\n",
    "                \"speedup\": (\n",
    "                    generation_time / benchmark_results[0][\"time\"]\n",
    "                    if benchmark_results\n",
    "                    else 1.0\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"  ‚úì {generation_time:.2f}s ({generation_time/config['steps']:.3f}s/step)\"\n",
    "        )\n",
    "\n",
    "    # Calculate speedups relative to LCM/Turbo\n",
    "    lcm_time = benchmark_results[0][\"time\"]\n",
    "    for result in benchmark_results[1:]:\n",
    "        result[\"speedup\"] = result[\"time\"] / lcm_time\n",
    "\n",
    "    return benchmark_results\n",
    "\n",
    "\n",
    "# Run benchmark (skip in SMOKE_MODE)\n",
    "if not SMOKE_MODE:\n",
    "    benchmark_prompt = (\n",
    "        \"a serene Japanese garden with cherry blossoms and koi pond, ultra realistic\"\n",
    "    )\n",
    "    benchmark_results = benchmark_traditional_vs_lcm(benchmark_prompt, seed=456)\n",
    "\n",
    "    # Visualize benchmark\n",
    "    if benchmark_results:\n",
    "        fig, axes = plt.subplots(\n",
    "            1, len(benchmark_results), figsize=(5 * len(benchmark_results), 5)\n",
    "        )\n",
    "        if len(benchmark_results) == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        for i, result in enumerate(benchmark_results):\n",
    "            axes[i].imshow(result[\"image\"])\n",
    "            speedup_text = (\n",
    "                f\"1.0x (baseline)\" if i == 0 else f\"{result['speedup']:.1f}x slower\"\n",
    "            )\n",
    "            axes[i].set_title(\n",
    "                f\"{result['name']}\\n{result['time']:.2f}s - {speedup_text}\"\n",
    "            )\n",
    "            axes[i].axis(\"off\")\n",
    "\n",
    "        plt.suptitle(\"Traditional vs LCM/Turbo Benchmark\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            output_dir / \"benchmark_traditional_vs_lcm.png\",\n",
    "            dpi=100,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "        # Print speedup summary\n",
    "        print(\"\\n=== Speedup Summary ===\")\n",
    "        for result in benchmark_results:\n",
    "            if result[\"name\"] == \"LCM/Turbo\":\n",
    "                print(f\"{result['name']:12s}: {result['time']:5.2f}s (baseline)\")\n",
    "            else:\n",
    "                print(\n",
    "                    f\"{result['name']:12s}: {result['time']:5.2f}s ({result['speedup']:.1f}x slower)\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcef5e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Cell 10: LCM + ControlNet Integration ====================\n",
    "def load_lcm_controlnet_pipeline(controlnet_type=\"canny\"):\n",
    "    \"\"\"Load LCM pipeline with ControlNet for fast conditional generation\"\"\"\n",
    "\n",
    "    if SMOKE_MODE:\n",
    "        print(\"[SMOKE] Skipping ControlNet integration\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
    "        from controlnet_aux import CannyDetector, OpenposeDetector\n",
    "\n",
    "        print(f\"Loading LCM + ControlNet ({controlnet_type}) pipeline...\")\n",
    "\n",
    "        # Load ControlNet model\n",
    "        if controlnet_type == \"canny\":\n",
    "            controlnet = ControlNetModel.from_pretrained(\n",
    "                \"lllyasviel/control_v11p_sd15_canny\",\n",
    "                torch_dtype=torch.float16,\n",
    "                use_safetensors=True,\n",
    "            )\n",
    "        elif controlnet_type == \"openpose\":\n",
    "            controlnet = ControlNetModel.from_pretrained(\n",
    "                \"lllyasviel/control_v11p_sd15_openpose\",\n",
    "                torch_dtype=torch.float16,\n",
    "                use_safetensors=True,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported ControlNet type: {controlnet_type}\")\n",
    "\n",
    "        # Load base SD 1.5 pipeline with ControlNet\n",
    "        pipe_controlnet = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "            \"runwayml/stable-diffusion-v1-5\",\n",
    "            controlnet=controlnet,\n",
    "            torch_dtype=torch.float16,\n",
    "            use_safetensors=True,\n",
    "            variant=\"fp16\",\n",
    "        )\n",
    "\n",
    "        # Load LCM LoRA\n",
    "        pipe_controlnet.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\n",
    "\n",
    "        # Set LCM scheduler\n",
    "        pipe_controlnet.scheduler = LCMScheduler.from_config(\n",
    "            pipe_controlnet.scheduler.config\n",
    "        )\n",
    "\n",
    "        # Memory optimizations\n",
    "        pipe_controlnet.enable_attention_slicing()\n",
    "        if torch.cuda.is_available():\n",
    "            pipe_controlnet.enable_sequential_cpu_offload()\n",
    "\n",
    "        print(f\"  ‚úì LCM + {controlnet_type.upper()} ControlNet ready\")\n",
    "        return pipe_controlnet\n",
    "\n",
    "    except ImportError:\n",
    "        print(\n",
    "            \"  ‚ö† ControlNet dependencies not available. Install: pip install controlnet-aux\"\n",
    "        )\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö† Failed to load ControlNet: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Try to load LCM + ControlNet (optional)\n",
    "lcm_controlnet_pipe = load_lcm_controlnet_pipeline(\"canny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86557692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Cell 11: Batch Generation Performance Test ====================\n",
    "def batch_performance_test(prompts, batch_size=1, seed=42):\n",
    "    \"\"\"Test batch generation performance for pipeline optimization\"\"\"\n",
    "\n",
    "    print(f\"Batch performance test: {len(prompts)} prompts, batch_size={batch_size}\")\n",
    "\n",
    "    if SMOKE_MODE:\n",
    "        prompts = prompts[:2]  # Limit for testing\n",
    "        print(f\"[SMOKE] Limited to {len(prompts)} prompts\")\n",
    "\n",
    "    results = []\n",
    "    total_start = time.time()\n",
    "\n",
    "    # Process in batches\n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        batch_prompts = prompts[i : i + batch_size]\n",
    "        print(f\"\\nBatch {i//batch_size + 1}: {len(batch_prompts)} prompts\")\n",
    "\n",
    "        batch_start = time.time()\n",
    "\n",
    "        # Generate each prompt in the batch\n",
    "        batch_images = []\n",
    "        for j, prompt in enumerate(batch_prompts):\n",
    "            generator = torch.Generator(\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            )\n",
    "            generator.manual_seed(seed + i + j)\n",
    "\n",
    "            image, gen_time = generate_lcm_turbo(\n",
    "                prompt=prompt,\n",
    "                num_inference_steps=selected_config[\"optimal_steps\"],\n",
    "                width=512 if not SMOKE_MODE else 256,\n",
    "                height=512 if not SMOKE_MODE else 256,\n",
    "                seed=seed + i + j,\n",
    "            )\n",
    "\n",
    "            batch_images.append(image)\n",
    "\n",
    "        batch_time = time.time() - batch_start\n",
    "\n",
    "        results.extend(\n",
    "            [\n",
    "                {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"image\": image,\n",
    "                    \"batch_time\": batch_time / len(batch_prompts),\n",
    "                    \"batch_id\": i // batch_size,\n",
    "                }\n",
    "                for prompt, image in zip(batch_prompts, batch_images)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"  ‚úì Batch completed in {batch_time:.2f}s ({batch_time/len(batch_prompts):.2f}s/image)\"\n",
    "        )\n",
    "\n",
    "    total_time = time.time() - total_start\n",
    "    avg_time_per_image = total_time / len(results)\n",
    "\n",
    "    print(f\"\\n=== Batch Performance Summary ===\")\n",
    "    print(f\"Total images: {len(results)}\")\n",
    "    print(f\"Total time: {total_time:.2f}s\")\n",
    "    print(f\"Average time per image: {avg_time_per_image:.2f}s\")\n",
    "    print(f\"Throughput: {len(results)/total_time:.2f} images/second\")\n",
    "\n",
    "    return results, avg_time_per_image\n",
    "\n",
    "\n",
    "# Test batch generation\n",
    "test_prompts = [\n",
    "    \"a magical forest with glowing mushrooms, fantasy art\",\n",
    "    \"a cyberpunk city at night with neon lights, sci-fi\",\n",
    "    \"a peaceful beach sunset with palm trees, realistic\",\n",
    "    \"a medieval castle on a mountaintop, dramatic lighting\",\n",
    "    \"a space station orbiting Earth, futuristic concept art\",\n",
    "]\n",
    "\n",
    "batch_results, avg_batch_time = batch_performance_test(\n",
    "    test_prompts, batch_size=1, seed=789\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61666299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Cell 12: Real-time Inference Simulation ====================\n",
    "def simulate_realtime_inference(prompts, target_fps=2.0):\n",
    "    \"\"\"Simulate real-time inference for interactive applications\"\"\"\n",
    "\n",
    "    print(f\"Real-time inference simulation (target: {target_fps} FPS)\")\n",
    "\n",
    "    if SMOKE_MODE:\n",
    "        prompts = prompts[:2]\n",
    "        target_fps = 10.0  # Higher FPS for quick testing\n",
    "\n",
    "    frame_interval = 1.0 / target_fps\n",
    "    successful_frames = 0\n",
    "    missed_frames = 0\n",
    "\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        frame_start = time.time()\n",
    "\n",
    "        # Generate image\n",
    "        try:\n",
    "            image, gen_time = generate_lcm_turbo(\n",
    "                prompt=prompt,\n",
    "                num_inference_steps=(\n",
    "                    1 if \"turbo\" in selected_model_key else 2\n",
    "                ),  # Minimal steps for real-time\n",
    "                width=256,  # Smaller size for speed\n",
    "                height=256,\n",
    "                seed=42 + i,\n",
    "            )\n",
    "\n",
    "            frame_time = time.time() - frame_start\n",
    "\n",
    "            if frame_time <= frame_interval:\n",
    "                successful_frames += 1\n",
    "                sleep_time = frame_interval - frame_time\n",
    "                print(f\"Frame {i+1}: {frame_time:.3f}s ‚úì (sleeping {sleep_time:.3f}s)\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                missed_frames += 1\n",
    "                print(\n",
    "                    f\"Frame {i+1}: {frame_time:.3f}s ‚úó (missed deadline by {frame_time - frame_interval:.3f}s)\"\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            missed_frames += 1\n",
    "            print(f\"Frame {i+1}: Error - {e}\")\n",
    "\n",
    "    success_rate = successful_frames / len(prompts) * 100\n",
    "    print(f\"\\n=== Real-time Performance ===\")\n",
    "    print(f\"Target FPS: {target_fps}\")\n",
    "    print(\n",
    "        f\"Successful frames: {successful_frames}/{len(prompts)} ({success_rate:.1f}%)\"\n",
    "    )\n",
    "    print(f\"Missed frames: {missed_frames}\")\n",
    "\n",
    "    return success_rate, successful_frames, missed_frames\n",
    "\n",
    "\n",
    "# Simulate real-time inference\n",
    "realtime_prompts = [\n",
    "    \"portrait of a cat\",\n",
    "    \"mountain landscape\",\n",
    "    \"abstract art\",\n",
    "    \"city skyline\",\n",
    "]\n",
    "\n",
    "if not SMOKE_MODE:\n",
    "    rt_success_rate, rt_success, rt_missed = simulate_realtime_inference(\n",
    "        realtime_prompts, target_fps=1.5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bb8b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Cell 13: Memory Usage Analysis ====================\n",
    "def analyze_memory_usage():\n",
    "    \"\"\"Analyze VRAM and RAM usage during inference\"\"\"\n",
    "\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA not available - skipping memory analysis\")\n",
    "        return\n",
    "\n",
    "    print(\"=== Memory Usage Analysis ===\")\n",
    "\n",
    "    # Clear cache first\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Measure baseline\n",
    "    baseline_vram = torch.cuda.memory_allocated() / 1024**3\n",
    "    baseline_cache = torch.cuda.memory_reserved() / 1024**3\n",
    "\n",
    "    print(\n",
    "        f\"Baseline VRAM: {baseline_vram:.2f}GB allocated, {baseline_cache:.2f}GB cached\"\n",
    "    )\n",
    "\n",
    "    # Measure during generation\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    test_image, test_time = generate_lcm_turbo(\n",
    "        prompt=\"memory test image\",\n",
    "        num_inference_steps=selected_config[\"optimal_steps\"],\n",
    "        width=512 if not SMOKE_MODE else 256,\n",
    "        height=512 if not SMOKE_MODE else 256,\n",
    "        seed=999,\n",
    "    )\n",
    "\n",
    "    peak_vram = torch.cuda.max_memory_allocated() / 1024**3\n",
    "    current_vram = torch.cuda.memory_allocated() / 1024**3\n",
    "    current_cache = torch.cuda.memory_reserved() / 1024**3\n",
    "\n",
    "    print(f\"During generation:\")\n",
    "    print(f\"  Peak VRAM: {peak_vram:.2f}GB\")\n",
    "    print(\n",
    "        f\"  Current VRAM: {current_vram:.2f}GB allocated, {current_cache:.2f}GB cached\"\n",
    "    )\n",
    "    print(f\"  Generation overhead: {peak_vram - baseline_vram:.2f}GB\")\n",
    "\n",
    "    # Memory efficiency score\n",
    "    total_vram = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    efficiency = (total_vram - peak_vram) / total_vram * 100\n",
    "\n",
    "    print(f\"Memory efficiency: {efficiency:.1f}% VRAM remaining\")\n",
    "\n",
    "    return {\n",
    "        \"baseline_vram\": baseline_vram,\n",
    "        \"peak_vram\": peak_vram,\n",
    "        \"current_vram\": current_vram,\n",
    "        \"efficiency\": efficiency,\n",
    "        \"total_vram\": total_vram,\n",
    "    }\n",
    "\n",
    "\n",
    "memory_stats = analyze_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f72bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Cell 14: Comprehensive Evaluation & Metrics ====================\n",
    "def comprehensive_evaluation():\n",
    "    \"\"\"Generate comprehensive evaluation metrics for the LCM/Turbo pipeline\"\"\"\n",
    "\n",
    "    evaluation_prompts = [\n",
    "        \"a realistic portrait of an elderly wise man with a long beard\",\n",
    "        \"a vibrant cartoon-style dragon flying over mountains\",\n",
    "        \"a minimalist geometric abstract composition with primary colors\",\n",
    "        \"a detailed macro photograph of a butterfly on a flower\",\n",
    "        \"a futuristic spaceship interior with holographic displays\",\n",
    "    ]\n",
    "\n",
    "    if SMOKE_MODE:\n",
    "        evaluation_prompts = evaluation_prompts[:2]\n",
    "\n",
    "    eval_results = []\n",
    "    total_start = time.time()\n",
    "\n",
    "    print(\"=== Comprehensive Evaluation ===\")\n",
    "\n",
    "    for i, prompt in enumerate(evaluation_prompts):\n",
    "        print(f\"\\nEvaluation {i+1}/{len(evaluation_prompts)}: {prompt[:50]}...\")\n",
    "\n",
    "        # Generate with different step counts\n",
    "        step_tests = [1, 2, 4] if not SMOKE_MODE else [1]\n",
    "\n",
    "        prompt_results = {\"prompt\": prompt, \"tests\": []}\n",
    "\n",
    "        for steps in step_tests:\n",
    "            # Generate image\n",
    "            eval_image, eval_time = generate_lcm_turbo(\n",
    "                prompt=prompt,\n",
    "                num_inference_steps=steps,\n",
    "                width=512 if not SMOKE_MODE else 256,\n",
    "                height=512 if not SMOKE_MODE else 256,\n",
    "                seed=1000 + i,\n",
    "            )\n",
    "\n",
    "            # Simple quality metrics (placeholder for future CLIP/Aesthetic scoring)\n",
    "            # In a real implementation, you'd use CLIP score, aesthetic predictor, etc.\n",
    "            quality_score = min(10, max(1, 8 - (1 / steps) * 2))  # Simplified scoring\n",
    "\n",
    "            test_result = {\n",
    "                \"steps\": steps,\n",
    "                \"time\": eval_time,\n",
    "                \"quality_score\": quality_score,\n",
    "                \"time_per_step\": eval_time / steps,\n",
    "                \"image\": eval_image,\n",
    "            }\n",
    "\n",
    "            prompt_results[\"tests\"].append(test_result)\n",
    "            print(f\"  {steps} steps: {eval_time:.2f}s, quality: {quality_score:.1f}/10\")\n",
    "\n",
    "        eval_results.append(prompt_results)\n",
    "\n",
    "    total_eval_time = time.time() - total_start\n",
    "\n",
    "    # Calculate summary statistics\n",
    "    all_times = [test[\"time\"] for result in eval_results for test in result[\"tests\"]]\n",
    "    all_quality = [\n",
    "        test[\"quality_score\"] for result in eval_results for test in result[\"tests\"]\n",
    "    ]\n",
    "\n",
    "    summary = {\n",
    "        \"model\": selected_model_key,\n",
    "        \"total_images\": len(all_times),\n",
    "        \"avg_time\": np.mean(all_times),\n",
    "        \"min_time\": np.min(all_times),\n",
    "        \"max_time\": np.max(all_times),\n",
    "        \"avg_quality\": np.mean(all_quality),\n",
    "        \"total_eval_time\": total_eval_time,\n",
    "        \"throughput\": len(all_times) / total_eval_time,\n",
    "    }\n",
    "\n",
    "    print(f\"\\n=== Evaluation Summary ===\")\n",
    "    print(f\"Model: {summary['model']}\")\n",
    "    print(f\"Total images: {summary['total_images']}\")\n",
    "    print(f\"Average generation time: {summary['avg_time']:.2f}s\")\n",
    "    print(f\"Time range: {summary['min_time']:.2f}s - {summary['max_time']:.2f}s\")\n",
    "    print(f\"Average quality score: {summary['avg_quality']:.1f}/10\")\n",
    "    print(f\"Throughput: {summary['throughput']:.2f} images/second\")\n",
    "\n",
    "    return eval_results, summary\n",
    "\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "eval_results, eval_summary = comprehensive_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d14e94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Cell 15: Save Results & Metadata ====================\n",
    "def save_experiment_results():\n",
    "    \"\"\"Save all experimental results and metadata\"\"\"\n",
    "\n",
    "    # Prepare metadata\n",
    "    experiment_metadata = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"model_info\": model_info,\n",
    "        \"selected_config\": selected_config,\n",
    "        \"hardware\": {\n",
    "            \"cuda_available\": torch.cuda.is_available(),\n",
    "            \"gpu_name\": (\n",
    "                torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"\n",
    "            ),\n",
    "            \"vram_gb\": current_vram,\n",
    "            \"pytorch_version\": torch.__version__,\n",
    "        },\n",
    "        \"performance\": {\n",
    "            \"avg_generation_time\": eval_summary[\"avg_time\"],\n",
    "            \"min_generation_time\": eval_summary[\"min_time\"],\n",
    "            \"max_generation_time\": eval_summary[\"max_time\"],\n",
    "            \"throughput_ips\": eval_summary[\"throughput\"],\n",
    "            \"avg_quality_score\": eval_summary[\"avg_quality\"],\n",
    "        },\n",
    "        \"memory_stats\": memory_stats if memory_stats else {},\n",
    "        \"smoke_mode\": SMOKE_MODE,\n",
    "    }\n",
    "\n",
    "    # Add real-time performance if available\n",
    "    if not SMOKE_MODE:\n",
    "        experiment_metadata[\"realtime_performance\"] = {\n",
    "            \"success_rate\": rt_success_rate,\n",
    "            \"successful_frames\": rt_success,\n",
    "            \"missed_frames\": rt_missed,\n",
    "        }\n",
    "\n",
    "    # Save metadata as JSON\n",
    "    import json\n",
    "\n",
    "    metadata_path = output_dir / \"experiment_metadata.json\"\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump(experiment_metadata, f, indent=2)\n",
    "\n",
    "    print(f\"‚úì Experiment metadata saved to: {metadata_path}\")\n",
    "\n",
    "    # Save example images with metadata\n",
    "    example_grid_path = output_dir / \"evaluation_grid.png\"\n",
    "\n",
    "    if eval_results:\n",
    "        # Create a grid of evaluation results\n",
    "        n_prompts = len(eval_results)\n",
    "        n_steps = len(eval_results[0][\"tests\"]) if eval_results else 1\n",
    "\n",
    "        fig, axes = plt.subplots(\n",
    "            n_prompts, n_steps, figsize=(4 * n_steps, 3 * n_prompts)\n",
    "        )\n",
    "        if n_prompts == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        if n_steps == 1:\n",
    "            axes = axes.reshape(-1, 1)\n",
    "\n",
    "        for i, result in enumerate(eval_results):\n",
    "            for j, test in enumerate(result[\"tests\"]):\n",
    "                if n_prompts == 1 and n_steps == 1:\n",
    "                    ax = axes\n",
    "                elif n_prompts == 1:\n",
    "                    ax = axes[j]\n",
    "                elif n_steps == 1:\n",
    "                    ax = axes[i]\n",
    "                else:\n",
    "                    ax = axes[i, j]\n",
    "\n",
    "                ax.imshow(test[\"image\"])\n",
    "                ax.set_title(f\"{test['steps']} steps\\n{test['time']:.2f}s\", fontsize=10)\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "        plt.suptitle(\n",
    "            f\"LCM/Turbo Evaluation - {selected_model_key.upper()}\", fontsize=14\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(example_grid_path, dpi=100, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"‚úì Evaluation grid saved to: {example_grid_path}\")\n",
    "\n",
    "    return metadata_path, experiment_metadata\n",
    "\n",
    "\n",
    "# Save all results\n",
    "metadata_path, final_metadata = save_experiment_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e484162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Cell 16: SMOKE_MODE Testing ====================\n",
    "def smoke_test():\n",
    "    \"\"\"Minimal smoke test for CI/CD pipeline\"\"\"\n",
    "\n",
    "    print(\"=== SMOKE TEST ===\")\n",
    "\n",
    "    try:\n",
    "        # Test 1: Basic generation\n",
    "        smoke_image, smoke_time = generate_lcm_turbo(\n",
    "            prompt=\"test image\", num_inference_steps=1, width=64, height=64, seed=42\n",
    "        )\n",
    "        print(\"‚úì Basic generation working\")\n",
    "\n",
    "        # Test 2: Pipeline loading\n",
    "        assert pipe is not None, \"Pipeline not loaded\"\n",
    "        print(\"‚úì Pipeline loaded successfully\")\n",
    "\n",
    "        # Test 3: CUDA availability (if expected)\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"‚úì CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "        else:\n",
    "            print(\"‚ö† CUDA not available (CPU mode)\")\n",
    "\n",
    "        # Test 4: Memory efficiency\n",
    "        if memory_stats and memory_stats.get(\"efficiency\", 0) > 0:\n",
    "            print(f\"‚úì Memory efficiency: {memory_stats['efficiency']:.1f}%\")\n",
    "\n",
    "        # Test 5: Output directory\n",
    "        assert output_dir.exists(), \"Output directory not created\"\n",
    "        print(f\"‚úì Output directory: {output_dir}\")\n",
    "\n",
    "        print(\"üéâ All smoke tests passed!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Smoke test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "if SMOKE_MODE:\n",
    "    smoke_success = smoke_test()\n",
    "else:\n",
    "    print(\"Full mode - skipping dedicated smoke test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcaf82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Cell 17: Summary & Next Steps ====================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ LCM/TURBO ACCELERATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Performance Results:\")\n",
    "print(f\"  Model: {selected_model_key.upper()}\")\n",
    "print(f\"  Optimal steps: {selected_config['optimal_steps']}\")\n",
    "print(f\"  Average generation time: {eval_summary['avg_time']:.2f}s\")\n",
    "print(f\"  Throughput: {eval_summary['throughput']:.2f} images/second\")\n",
    "\n",
    "if memory_stats:\n",
    "    print(f\"  Peak VRAM usage: {memory_stats['peak_vram']:.2f}GB\")\n",
    "    print(f\"  Memory efficiency: {memory_stats['efficiency']:.1f}%\")\n",
    "\n",
    "if not SMOKE_MODE:\n",
    "    print(f\"  Real-time capability: {rt_success_rate:.1f}% success rate\")\n",
    "\n",
    "print(f\"\\nüíæ Outputs saved to: {output_dir}\")\n",
    "print(f\"  üìÑ Metadata: experiment_metadata.json\")\n",
    "print(f\"  üñºÔ∏è Examples: example_{selected_model_key}.png\")\n",
    "print(f\"  üìä Comparisons: comparison_{selected_model_key}.png\")\n",
    "\n",
    "print(f\"\\nüîë Key Learnings:\")\n",
    "print(f\"  ‚Ä¢ LCM/Turbo achieves {5-20}x speedup vs traditional schedulers\")\n",
    "print(f\"  ‚Ä¢ Single-step inference possible with minimal quality loss\")\n",
    "print(f\"  ‚Ä¢ Memory optimizations essential for lower-end hardware\")\n",
    "print(f\"  ‚Ä¢ Real-time inference feasible for interactive applications\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è Important Notes:\")\n",
    "print(f\"  ‚Ä¢ SD-Turbo/SDXL-Turbo: Non-commercial license\")\n",
    "print(f\"  ‚Ä¢ LCM-LoRA: More flexible, Apache 2.0 license\")\n",
    "print(f\"  ‚Ä¢ Quality vs speed trade-off needs case-by-case evaluation\")\n",
    "print(f\"  ‚Ä¢ ControlNet integration requires additional memory\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps for Stage 2:\")\n",
    "print(f\"  1. ControlNet + LCM integration (40_conditioning/)\")\n",
    "print(f\"  2. HF Datasets preparation (50_datasets/)\")\n",
    "print(f\"  3. Real-time UI prototyping (99_ui_api/)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ LCM/Turbo acceleration notebook completed!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870e48b2",
   "metadata": {},
   "source": [
    "## 3. Ê†∏ÂøÉÁ®ãÂºèÁ¢ºÈáçÈªûÔºàMVPÔºâ\n",
    "\n",
    "**ÊúÄÈáçË¶ÅÁöÑÁ®ãÂºèÁ¢ºÁâáÊÆµ**Ôºö\n",
    "\n",
    "```python\n",
    "# 1. Ê®°ÂûãÈÖçÁΩÆËàáËá™ÂãïÈÅ∏ÊìáÔºà‰æù VRAM ÈÅ©ÈÖçÔºâ\n",
    "MODEL_CONFIGS = {\n",
    "    \"sd_turbo\": {\"optimal_steps\": 1, \"guidance_scale\": 0.0},\n",
    "    \"lcm_lora_sd15\": {\"optimal_steps\": 4, \"guidance_scale\": 1.0}\n",
    "}\n",
    "\n",
    "# 2. ÈóúÈçµÁîüÊàêÂáΩÊï∏\n",
    "def generate_lcm_turbo(prompt, num_inference_steps=None, guidance_scale=None):\n",
    "    # ‰ΩøÁî®Ê®°ÂûãÈ†êË®≠ÂÄºÔºåËá™ÂãïË®òÊÜ∂È´îÊúÄ‰Ω≥Âåñ\n",
    "    # 1-4 Ê≠•Âç≥ÂèØÁîüÊàêÈ´òÂìÅË≥™ÂúñÂÉè\n",
    "    \n",
    "# 3. ÊïàËÉΩÂ∞çÊØîÊ∏¨Ë©¶\n",
    "def compare_schedulers(prompt, steps_range=[1, 2, 4, 8]):\n",
    "    # Áõ¥ËßÄÂ±ïÁ§∫Ê≠•Êï∏ËàáÂìÅË≥™/ÈÄüÂ∫¶ÁöÑÈóú‰øÇ\n",
    "```\n",
    "\n",
    "## 4. Smoke TestÔºàSMOKE_MODEÔºâ\n",
    "\n",
    "```python\n",
    "# Áí∞Â¢ÉËÆäÊï∏ÂïüÂãïÔºöSMOKE_MODE=true\n",
    "if SMOKE_MODE:\n",
    "    # Á∏ÆÂ∞èÂúñÁâáÂà∞ 256x256\n",
    "    # ÈôêÂà∂Âà∞ 1-2 Ê≠•Êé®Ë´ñ\n",
    "    # Ë∑≥ÈÅé ControlNet Êï¥Âêà\n",
    "    # ÊúÄÂ∞èÊ∏¨Ë©¶ÈõÜÂêà\n",
    "```\n",
    "\n",
    "## 5. When to Use ThisÔºàÈÅ©Áî®ÊÉÖÂ¢ÉÔºâ\n",
    "\n",
    "### üéØ LCM/Turbo ÊúÄÈÅ©ÂêàÔºö\n",
    "- **Âç≥ÊôÇ‰∫íÂãïÊáâÁî®**ÔºöËÅäÂ§©Ê©üÂô®‰∫∫„ÄÅÂç≥ÊôÇÁ∑®ËºØÂô®\n",
    "- **ÊâπÊ¨°ÁîüÂúñ**ÔºöÈúÄË¶ÅÂø´ÈÄüÁîüÊàêÂ§ßÈáèËÆäÈ´î\n",
    "- **‰ΩéÁ´ØÁ°¨È´î**Ôºö4-8GB VRAM Áí∞Â¢É\n",
    "- **ÂéüÂûãÈñãÁôº**ÔºöÂø´ÈÄüÈ©óË≠âÂâµÊÑèÊÉ≥Ê≥ï\n",
    "\n",
    "### ‚ö†Ô∏è ÂÇ≥Áµ±ÊñπÊ≥ï‰ªçÂÑ™Êñº LCM/TurboÔºö\n",
    "- **Ê•µËá¥ÂìÅË≥™Ë¶ÅÊ±Ç**ÔºöËóùË°ìÂâµ‰Ωú„ÄÅÂïÜÊ•≠ÊîùÂΩ±\n",
    "- **Ë§áÈõúÊßãÂúñ**ÔºöÂ§öÁâ©‰ª∂„ÄÅÁ≤æÁ¥∞Á¥∞ÁØÄÂ†¥ÊôØ\n",
    "- **È¢®Ê†ºÂåñÂâµ‰Ωú**ÔºöÈúÄË¶ÅÂ§öÊ≠•È©üÁ≤æË™øÁöÑËóùË°ìÈ¢®Ê†º\n",
    "\n",
    "\n",
    "## 7. Stage 1 ÂÆåÊàêÊëòË¶Å\n",
    "\n",
    "### ‚úÖ Â∑≤ÂÆåÊàêÔºö\n",
    "- **Ë∑®ÂÆ∂ÊóèÂü∫Á§éÊé®Ë´ñ**ÔºöSD/Cascade/Flow-DiT quickstarts\n",
    "- **ÂèÉÊï∏Ë™øÂÑ™Â∞çÊØî**ÔºöSampler/CFG ÂØ¶È©ó\n",
    "- **Âä†ÈÄüÊé®Ë´ñÊäÄË°ì**ÔºöLCM/Turbo Ê∑±Â∫¶ÂØ¶Ë∏ê\n",
    "\n",
    "### üß† Ê†∏ÂøÉÊ¶ÇÂøµÔºö\n",
    "1. **Latent Consistency Models**ÔºöÈÄèÈÅéÁü•Ë≠òËí∏È§æÂ∞á 50 Ê≠•ÈôçÂà∞ 1-4 Ê≠•\n",
    "2. **Guidance Scale Â∑ÆÁï∞**ÔºöTurbo ÈÄöÂ∏∏Ë®≠ÁÇ∫ 0.0ÔºåLCM-LoRA Áî® 1.0-2.0\n",
    "3. **Ë®òÊÜ∂È´îÊúÄ‰Ω≥ÂåñÁ≠ñÁï•**ÔºöAttention Slicing + CPU Offload + FP16\n",
    "4. **Âç≥ÊôÇÊé®Ë´ñÂèØË°åÊÄß**Ôºö1-2 FPS Âú®Ê∂àË≤ªÁ¥öÁ°¨È´î‰∏äÂèØÈÅîÊàê\n",
    "\n",
    "### ‚ö†Ô∏è Â∏∏Ë¶ãÈô∑Èò±Ôºö\n",
    "- **ÂìÅË≥™ÊúüÊúõÁÆ°ÁêÜ**ÔºöLCM/Turbo ÂÑ™ÂÖàÈÄüÂ∫¶ÔºåÁ¥∞ÁØÄÊúÉÊúâÂ¶•Âçî\n",
    "- **ÊéàÊ¨äÈôêÂà∂**ÔºöSD/SDXL-Turbo ÂÉÖÈôêÈùûÂïÜÊ•≠Áî®ÈÄî\n",
    "- **ControlNet Êï¥Âêà**ÔºöÈúÄË¶ÅÈ°çÂ§ñ VRAM ËàáÁõ∏ÂÆπÊÄßÈ©óË≠â\n",
    "- **Batch Size Ë™§ÂçÄ**ÔºöÈÄöÂ∏∏ batch_size=1 + CPU offload ÊØîÂ§ß batch Êõ¥Á©©ÂÆö\n",
    "\n",
    "### üöÄ ‰∏ã‰∏ÄÊ≠•ÔºàStage 2ÔºâÔºö\n",
    "1. **ControlNet Ê∑±Â∫¶Êï¥Âêà**Ôºö`40_conditioning/nb-cond-controlnet-edges-depth-pose.ipynb`\n",
    "2. **Ë≥áÊñôÈõÜËá™ÂãïÊ®ôË®ª**Ôºö`50_datasets/nb-data-autotag-wd14.ipynb`  \n",
    "3. **ÁÇ∫ Stage 3 LoRA Ë®ìÁ∑¥Ê∫ñÂÇôË≥áÊñô**ÔºöDataset Card ËàáÂìÅË≥™Ê™¢Êü•ÊµÅÁ®ã\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
