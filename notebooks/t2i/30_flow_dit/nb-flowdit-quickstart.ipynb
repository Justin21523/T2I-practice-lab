{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "663eaad2",
   "metadata": {},
   "source": [
    " # FLUX/Flow-DiT 快速入門 🌊\n",
    " \n",
    " **學習目標**:\n",
    " 1. 理解 Flow-based DiT 架構差異 (flow matching vs diffusion)\n",
    " 2. 實作 FLUX.1-dev/schnell 與 Playground v2.5 基礎推論\n",
    " 3. 探索 guidance_scale、steps 對 Flow-DiT 的影響\n",
    " 4. 記憶體最佳化策略 (12-16GB VRAM)\n",
    " 5. Flow-DiT vs SD 性能對比分析\n",
    "\n",
    " **前置需求**: 12GB+ VRAM, diffusers>=0.30.0, torch>=2.3\n",
    "\n",
    " ## 📁 共享快取設定 (Shared Cache Bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88db6262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [1] Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (複製到每本 notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dd774c",
   "metadata": {},
   "source": [
    "## 🔧 環境檢查與依賴導入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b10d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from diffusers import FluxPipeline, DiffusionPipeline\n",
    "from transformers import pipeline\n",
    "import time\n",
    "from typing import List, Tuple, Optional\n",
    "import json\n",
    "\n",
    "# Check environment\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f}GB\")\n",
    "\n",
    "# Environment flags\n",
    "SMOKE_MODE = os.getenv(\"SMOKE_MODE\", \"false\").lower() == \"true\"\n",
    "print(f\"SMOKE_MODE: {SMOKE_MODE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f140e441",
   "metadata": {},
   "source": [
    " ## 🌊 Flow-DiT 架構理解\n",
    " \n",
    " ### Flow Matching vs Diffusion\n",
    " \n",
    " **傳統 Diffusion (SD)**:\n",
    " - 噪聲排程: `β_t` → 逐步去噪\n",
    " - DDPM/DDIM sampler\n",
    " - CFG guidance 強依賴\n",
    " \n",
    " **Flow Matching (FLUX)**:\n",
    " - 向量場學習: 直接學習 `noise → image` 的軌跡\n",
    " - Euler/Heun sampler 為主\n",
    " - Guidance 更自然整合\n",
    " \n",
    " ### FLUX 家族模型\n",
    " \n",
    " | 模型 | 參數量 | 特點 | 推薦用途 |\n",
    " |------|-------|------|----------|\n",
    " | FLUX.1-schnell | 12B | 4步快速推論 | 即時生成 |\n",
    " | FLUX.1-dev | 12B | 50步高品質 | 精細創作 |\n",
    " | Playground v2.5 | ? | 美學調優 | 藝術風格 |\n",
    "\n",
    " ## 🚀 FLUX.1-schnell 快速推論 (4-step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b915e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def load_flux_schnell(enable_cpu_offload: bool = True) -> FluxPipeline:\n",
    "    \"\"\"Load FLUX.1-schnell with memory optimization\"\"\"\n",
    "    print(\"Loading FLUX.1-schnell...\")\n",
    "\n",
    "    pipe = FluxPipeline.from_pretrained(\n",
    "        \"black-forest-labs/FLUX.1-schnell\",\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "        device_map=\"auto\" if not enable_cpu_offload else None,\n",
    "        use_safetensors=True,\n",
    "    )\n",
    "\n",
    "    if enable_cpu_offload:\n",
    "        pipe.to(\"cuda\")\n",
    "        pipe.enable_sequential_cpu_offload()\n",
    "\n",
    "    # Enable memory efficient attention\n",
    "    pipe.enable_attention_slicing()\n",
    "    if hasattr(pipe, \"enable_xformers_memory_efficient_attention\"):\n",
    "        try:\n",
    "            pipe.enable_xformers_memory_efficient_attention()\n",
    "        except:\n",
    "            print(\"xFormers not available, using default attention\")\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "# Load FLUX schnell\n",
    "flux_schnell = load_flux_schnell(enable_cpu_offload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c83a9f4",
   "metadata": {},
   "source": [
    "## 🎨 FLUX 基礎推論測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c6b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def generate_flux_image(\n",
    "    pipe: FluxPipeline,\n",
    "    prompt: str,\n",
    "    num_inference_steps: int = 4,\n",
    "    guidance_scale: float = 0.0,  # FLUX schnell typically uses 0.0\n",
    "    width: int = 1024,\n",
    "    height: int = 1024,\n",
    "    generator: Optional[torch.Generator] = None,\n",
    ") -> Tuple[Image.Image, dict]:\n",
    "    \"\"\"Generate image with FLUX pipeline\"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # FLUX schnell parameters\n",
    "    if \"schnell\" in str(pipe.__class__).lower():\n",
    "        num_inference_steps = min(num_inference_steps, 4)  # schnell max 4 steps\n",
    "        guidance_scale = 0.0  # schnell doesn't use guidance\n",
    "\n",
    "    with torch.no_grad():\n",
    "        result = pipe(\n",
    "            prompt=prompt,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            generator=generator,\n",
    "        )\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    metadata = {\n",
    "        \"prompt\": prompt,\n",
    "        \"steps\": num_inference_steps,\n",
    "        \"guidance_scale\": guidance_scale,\n",
    "        \"size\": f\"{width}x{height}\",\n",
    "        \"generation_time\": f\"{end_time - start_time:.2f}s\",\n",
    "        \"model\": \"FLUX.1-schnell\",\n",
    "    }\n",
    "\n",
    "    return result.images[0], metadata\n",
    "\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"a majestic dragon soaring through cloudy mountain peaks, cinematic lighting, epic fantasy art\"\n",
    "if SMOKE_MODE:\n",
    "    test_prompt = \"a simple red apple\"\n",
    "\n",
    "# Generate with FLUX schnell\n",
    "print(f\"Generating with FLUX.1-schnell...\")\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "\n",
    "flux_image, flux_meta = generate_flux_image(\n",
    "    flux_schnell,\n",
    "    prompt=test_prompt,\n",
    "    num_inference_steps=4 if not SMOKE_MODE else 1,\n",
    "    width=1024 if not SMOKE_MODE else 512,\n",
    "    height=1024 if not SMOKE_MODE else 512,\n",
    "    generator=generator,\n",
    ")\n",
    "\n",
    "# Display result\n",
    "print(f\"✅ Generated in {flux_meta['generation_time']}\")\n",
    "print(f\"Metadata: {json.dumps(flux_meta, indent=2)}\")\n",
    "flux_image.show() if not SMOKE_MODE else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552cd62e",
   "metadata": {},
   "source": [
    "## 📊 Memory Usage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc616e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def get_memory_stats() -> dict:\n",
    "    \"\"\"Get current CUDA memory statistics\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return {\"error\": \"CUDA not available\"}\n",
    "\n",
    "    return {\n",
    "        \"allocated_gb\": torch.cuda.memory_allocated() / 1e9,\n",
    "        \"reserved_gb\": torch.cuda.memory_reserved() / 1e9,\n",
    "        \"max_allocated_gb\": torch.cuda.max_memory_allocated() / 1e9,\n",
    "        \"total_gb\": torch.cuda.get_device_properties(0).total_memory / 1e9,\n",
    "    }\n",
    "\n",
    "\n",
    "memory_after_flux = get_memory_stats()\n",
    "print(\"Memory usage after FLUX generation:\")\n",
    "print(json.dumps(memory_after_flux, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee1ff1e",
   "metadata": {},
   "source": [
    "## 🔄 Clean up and load FLUX.1-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa5a493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Clean up schnell model\n",
    "del flux_schnell\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Loading FLUX.1-dev...\")\n",
    "\n",
    "\n",
    "def load_flux_dev(enable_cpu_offload: bool = True) -> FluxPipeline:\n",
    "    \"\"\"Load FLUX.1-dev with memory optimization\"\"\"\n",
    "\n",
    "    pipe = FluxPipeline.from_pretrained(\n",
    "        \"black-forest-labs/FLUX.1-dev\",\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "        device_map=\"auto\" if not enable_cpu_offload else None,\n",
    "        use_safetensors=True,\n",
    "    )\n",
    "\n",
    "    if enable_cpu_offload:\n",
    "        pipe.to(\"cuda\")\n",
    "        pipe.enable_sequential_cpu_offload()\n",
    "\n",
    "    pipe.enable_attention_slicing()\n",
    "    if hasattr(pipe, \"enable_xformers_memory_efficient_attention\"):\n",
    "        try:\n",
    "            pipe.enable_xformers_memory_efficient_attention()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "# Skip FLUX.1-dev in smoke mode (memory intensive)\n",
    "if not SMOKE_MODE:\n",
    "    flux_dev = load_flux_dev(enable_cpu_offload=True)\n",
    "else:\n",
    "    print(\"⚠️ Skipping FLUX.1-dev in SMOKE_MODE\")\n",
    "    flux_dev = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063974d4",
   "metadata": {},
   "source": [
    "## 🎭 FLUX.1-dev 高品質推論 (25-50 steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59575950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "if flux_dev is not None:\n",
    "    print(\"Generating with FLUX.1-dev...\")\n",
    "\n",
    "    # FLUX.1-dev supports guidance and more steps\n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "\n",
    "    flux_dev_image, flux_dev_meta = generate_flux_image(\n",
    "        flux_dev,\n",
    "        prompt=test_prompt,\n",
    "        num_inference_steps=25,  # dev supports more steps\n",
    "        guidance_scale=3.5,  # dev supports guidance\n",
    "        width=1024,\n",
    "        height=1024,\n",
    "        generator=generator,\n",
    "    )\n",
    "\n",
    "    flux_dev_meta[\"model\"] = \"FLUX.1-dev\"\n",
    "    print(f\"✅ Generated in {flux_dev_meta['generation_time']}\")\n",
    "    print(f\"Metadata: {json.dumps(flux_dev_meta, indent=2)}\")\n",
    "    flux_dev_image.show()\n",
    "\n",
    "    memory_after_dev = get_memory_stats()\n",
    "    print(\"Memory usage after FLUX.1-dev:\")\n",
    "    print(json.dumps(memory_after_dev, indent=2))\n",
    "else:\n",
    "    print(\"⚠️ FLUX.1-dev skipped in SMOKE_MODE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404061f4",
   "metadata": {},
   "source": [
    "## 📈 參數影響實驗 (Parameter Experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc11e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def parameter_experiment(pipe: FluxPipeline, model_name: str):\n",
    "    \"\"\"Test different parameters on Flow-DiT models\"\"\"\n",
    "\n",
    "    if SMOKE_MODE:\n",
    "        print(\"⚠️ Skipping parameter experiments in SMOKE_MODE\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n🧪 Parameter experiments for {model_name}\")\n",
    "\n",
    "    base_prompt = \"a cozy coffee shop in cyberpunk Tokyo, neon lights, rain\"\n",
    "    experiments = []\n",
    "\n",
    "    # Different step counts\n",
    "    step_configs = [4, 10, 25] if \"dev\" in model_name.lower() else [1, 4]\n",
    "\n",
    "    for steps in step_configs:\n",
    "        generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "\n",
    "        # Adjust guidance for model type\n",
    "        guidance = 3.5 if \"dev\" in model_name.lower() else 0.0\n",
    "\n",
    "        start_time = time.time()\n",
    "        result = pipe(\n",
    "            prompt=base_prompt,\n",
    "            num_inference_steps=steps,\n",
    "            guidance_scale=guidance,\n",
    "            width=512,  # Smaller for experiments\n",
    "            height=512,\n",
    "            generator=generator\n",
    "        )\n",
    "        generation_time = time.time() - start_time\n",
    "\n",
    "        experiments.append({\n",
    "            \"steps\": steps,\n",
    "            \"guidance\": guidance,\n",
    "            \"time\": f\"{generation_time:.2f}s\",\n",
    "            \"model\": model_name\n",
    "        })\n",
    "\n",
    "        print(f\"Steps {steps}: {generation_time:.2f}s\")\n",
    "\n",
    "    return experiments\n",
    "\n",
    "# Run experiments\n",
    "if flux_dev is not None:\n",
    "    dev_experiments = parameter_experiment(flux_dev, \"FLUX.1-dev\")\n",
    "else:\n",
    "    print(\"⚠️ FLUX.1-dev experiments skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd58fa3",
   "metadata": {},
   "source": [
    "## 🆚 Flow-DiT vs SD 對比分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5be9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def compare_architectures():\n",
    "    \"\"\"Compare Flow-DiT vs traditional Diffusion characteristics\"\"\"\n",
    "\n",
    "    comparison = {\n",
    "        \"architecture\": {\n",
    "            \"Flow-DiT (FLUX)\": {\n",
    "                \"noise_schedule\": \"Flow matching (continuous)\",\n",
    "                \"sampler\": \"Euler/Heun\",\n",
    "                \"guidance\": \"Natural integration\",\n",
    "                \"steps_range\": \"1-50\",\n",
    "                \"memory_efficiency\": \"Good (DiT architecture)\",\n",
    "            },\n",
    "            \"Diffusion (SD)\": {\n",
    "                \"noise_schedule\": \"DDPM/DDIM (discrete)\",\n",
    "                \"sampler\": \"DPM++/DDIM/Euler\",\n",
    "                \"guidance\": \"CFG (classifier-free)\",\n",
    "                \"steps_range\": \"20-150\",\n",
    "                \"memory_efficiency\": \"Moderate (U-Net)\",\n",
    "            },\n",
    "        },\n",
    "        \"performance\": {\n",
    "            \"FLUX.1-schnell\": {\n",
    "                \"optimal_steps\": 4,\n",
    "                \"speed\": \"Very Fast\",\n",
    "                \"quality\": \"Good\",\n",
    "                \"vram_usage\": \"~12GB\",\n",
    "            },\n",
    "            \"FLUX.1-dev\": {\n",
    "                \"optimal_steps\": 25,\n",
    "                \"speed\": \"Medium\",\n",
    "                \"quality\": \"Excellent\",\n",
    "                \"vram_usage\": \"~14GB\",\n",
    "            },\n",
    "            \"SD1.5\": {\n",
    "                \"optimal_steps\": 20,\n",
    "                \"speed\": \"Fast\",\n",
    "                \"quality\": \"Good\",\n",
    "                \"vram_usage\": \"~6GB\",\n",
    "            },\n",
    "            \"SDXL\": {\n",
    "                \"optimal_steps\": 30,\n",
    "                \"speed\": \"Medium\",\n",
    "                \"quality\": \"Excellent\",\n",
    "                \"vram_usage\": \"~8GB\",\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    print(\"🆚 Architecture Comparison:\")\n",
    "    print(json.dumps(comparison, indent=2))\n",
    "\n",
    "    return comparison\n",
    "\n",
    "\n",
    "architecture_comparison = compare_architectures()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbbe09b",
   "metadata": {},
   "source": [
    "## 💾 結果保存與清理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d4cac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Save results\n",
    "output_dir = pathlib.Path(\"outputs\") / \"flowdit_quickstart\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save images and metadata\n",
    "if \"flux_image\" in locals():\n",
    "    flux_image.save(output_dir / \"flux_schnell_result.png\")\n",
    "    with open(output_dir / \"flux_schnell_meta.json\", \"w\") as f:\n",
    "        json.dump(flux_meta, f, indent=2)\n",
    "\n",
    "if \"flux_dev_image\" in locals():\n",
    "    flux_dev_image.save(output_dir / \"flux_dev_result.png\")\n",
    "    with open(output_dir / \"flux_dev_meta.json\", \"w\") as f:\n",
    "        json.dump(flux_dev_meta, f, indent=2)\n",
    "\n",
    "# Save comparison analysis\n",
    "with open(output_dir / \"architecture_comparison.json\", \"w\") as f:\n",
    "    json.dump(architecture_comparison, f, indent=2)\n",
    "\n",
    "print(f\"✅ Results saved to {output_dir}\")\n",
    "\n",
    "# Clean up memory\n",
    "if \"flux_dev\" in locals() and flux_dev is not None:\n",
    "    del flux_dev\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "final_memory = get_memory_stats()\n",
    "print(\"Final memory usage:\")\n",
    "print(json.dumps(final_memory, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17ed0a5",
   "metadata": {},
   "source": [
    "## 🧪 Smoke Test (CI Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf5e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def smoke_test():\n",
    "    \"\"\"Minimal test for CI pipeline\"\"\"\n",
    "\n",
    "    print(\"🧪 Running smoke test...\")\n",
    "\n",
    "    # Test basic imports\n",
    "    assert torch.cuda.is_available(), \"CUDA required\"\n",
    "\n",
    "    # Test pipeline loading (schnell only in smoke mode)\n",
    "    test_pipe = FluxPipeline.from_pretrained(\n",
    "        \"black-forest-labs/FLUX.1-schnell\", torch_dtype=torch.float16, device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Quick generation test\n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "    result = test_pipe(\n",
    "        \"a red apple\", num_inference_steps=1, width=256, height=256, generator=generator\n",
    "    )\n",
    "\n",
    "    assert len(result.images) == 1, \"Should generate 1 image\"\n",
    "    assert result.images[0].size == (256, 256), \"Wrong image size\"\n",
    "\n",
    "    print(\"✅ Smoke test passed!\")\n",
    "\n",
    "    # Cleanup\n",
    "    del test_pipe\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "if SMOKE_MODE:\n",
    "    smoke_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfa1744",
   "metadata": {},
   "source": [
    " ## 📋 完成摘要 (Summary)\n",
    " \n",
    " ### ✅ 已完成\n",
    " 1. **Flow-DiT 架構理解**: Flow matching vs Diffusion 核心差異\n",
    " 2. **FLUX 模型實作**: schnell (4步) 與 dev (25步) 推論\n",
    " 3. **記憶體最佳化**: CPU offload + attention slicing (12-16GB)\n",
    " 4. **參數調優**: steps/guidance 對 Flow-DiT 的影響分析\n",
    " 5. **架構對比**: Flow-DiT vs SD 在速度/品質/記憶體權衡\n",
    " \n",
    " ### 🔑 核心概念\n",
    " - **Flow Matching**: 直接學習 noise→image 軌跡，比 diffusion 更高效\n",
    " - **FLUX.1-schnell**: 4步快速推論，guidance_scale=0.0\n",
    " - **FLUX.1-dev**: 25-50步高品質，支援 guidance_scale=3.5\n",
    " - **DiT vs U-Net**: Transformer 架構 vs 卷積架構的記憶體特性\n",
    " \n",
    " ### ⚠️ 常見坑 (Pitfalls)\n",
    " 1. **VRAM 需求高**: FLUX 需 12GB+，注意 CPU offload\n",
    " 2. **授權限制**: 部分 Flow-DiT 模型僅推論，無法微調\n",
    " 3. **Guidance 差異**: schnell 不支援 guidance，dev 才支援\n",
    " 4. **Step 數限制**: schnell 最多 4步，dev 建議 25步以上\n",
    "  \n",
    " ### ➡️ 下一步\n",
    " - **Stage 2**: ControlNet/T2I-Adapter 在 Flow-DiT 的適用性\n",
    " - **Stage 3**: Flow-DiT 微調可行性評估 (取決於模型授權)\n",
    " - **Stage 4**: Flow-DiT 整合到批次生圖流程"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
