{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "663eaad2",
   "metadata": {},
   "source": [
    " # FLUX/Flow-DiT å¿«é€Ÿå…¥é–€ ğŸŒŠ\n",
    " \n",
    " **å­¸ç¿’ç›®æ¨™**:\n",
    " 1. ç†è§£ Flow-based DiT æ¶æ§‹å·®ç•° (flow matching vs diffusion)\n",
    " 2. å¯¦ä½œ FLUX.1-dev/schnell èˆ‡ Playground v2.5 åŸºç¤æ¨è«–\n",
    " 3. æ¢ç´¢ guidance_scaleã€steps å° Flow-DiT çš„å½±éŸ¿\n",
    " 4. è¨˜æ†¶é«”æœ€ä½³åŒ–ç­–ç•¥ (12-16GB VRAM)\n",
    " 5. Flow-DiT vs SD æ€§èƒ½å°æ¯”åˆ†æ\n",
    "\n",
    " **å‰ç½®éœ€æ±‚**: 12GB+ VRAM, diffusers>=0.30.0, torch>=2.3\n",
    "\n",
    " ## ğŸ“ å…±äº«å¿«å–è¨­å®š (Shared Cache Bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88db6262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [1] Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (è¤‡è£½åˆ°æ¯æœ¬ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dd774c",
   "metadata": {},
   "source": [
    "## ğŸ”§ ç’°å¢ƒæª¢æŸ¥èˆ‡ä¾è³´å°å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b10d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from diffusers import FluxPipeline, DiffusionPipeline\n",
    "from transformers import pipeline\n",
    "import time\n",
    "from typing import List, Tuple, Optional\n",
    "import json\n",
    "\n",
    "# Check environment\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f}GB\")\n",
    "\n",
    "# Environment flags\n",
    "SMOKE_MODE = os.getenv(\"SMOKE_MODE\", \"false\").lower() == \"true\"\n",
    "print(f\"SMOKE_MODE: {SMOKE_MODE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f140e441",
   "metadata": {},
   "source": [
    " ## ğŸŒŠ Flow-DiT æ¶æ§‹ç†è§£\n",
    " \n",
    " ### Flow Matching vs Diffusion\n",
    " \n",
    " **å‚³çµ± Diffusion (SD)**:\n",
    " - å™ªè²æ’ç¨‹: `Î²_t` â†’ é€æ­¥å»å™ª\n",
    " - DDPM/DDIM sampler\n",
    " - CFG guidance å¼·ä¾è³´\n",
    " \n",
    " **Flow Matching (FLUX)**:\n",
    " - å‘é‡å ´å­¸ç¿’: ç›´æ¥å­¸ç¿’ `noise â†’ image` çš„è»Œè·¡\n",
    " - Euler/Heun sampler ç‚ºä¸»\n",
    " - Guidance æ›´è‡ªç„¶æ•´åˆ\n",
    " \n",
    " ### FLUX å®¶æ—æ¨¡å‹\n",
    " \n",
    " | æ¨¡å‹ | åƒæ•¸é‡ | ç‰¹é» | æ¨è–¦ç”¨é€” |\n",
    " |------|-------|------|----------|\n",
    " | FLUX.1-schnell | 12B | 4æ­¥å¿«é€Ÿæ¨è«– | å³æ™‚ç”Ÿæˆ |\n",
    " | FLUX.1-dev | 12B | 50æ­¥é«˜å“è³ª | ç²¾ç´°å‰µä½œ |\n",
    " | Playground v2.5 | ? | ç¾å­¸èª¿å„ª | è—è¡“é¢¨æ ¼ |\n",
    "\n",
    " ## ğŸš€ FLUX.1-schnell å¿«é€Ÿæ¨è«– (4-step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b915e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def load_flux_schnell(enable_cpu_offload: bool = True) -> FluxPipeline:\n",
    "    \"\"\"Load FLUX.1-schnell with memory optimization\"\"\"\n",
    "    print(\"Loading FLUX.1-schnell...\")\n",
    "\n",
    "    pipe = FluxPipeline.from_pretrained(\n",
    "        \"black-forest-labs/FLUX.1-schnell\",\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "        device_map=\"auto\" if not enable_cpu_offload else None,\n",
    "        use_safetensors=True,\n",
    "    )\n",
    "\n",
    "    if enable_cpu_offload:\n",
    "        pipe.to(\"cuda\")\n",
    "        pipe.enable_sequential_cpu_offload()\n",
    "\n",
    "    # Enable memory efficient attention\n",
    "    pipe.enable_attention_slicing()\n",
    "    if hasattr(pipe, \"enable_xformers_memory_efficient_attention\"):\n",
    "        try:\n",
    "            pipe.enable_xformers_memory_efficient_attention()\n",
    "        except:\n",
    "            print(\"xFormers not available, using default attention\")\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "# Load FLUX schnell\n",
    "flux_schnell = load_flux_schnell(enable_cpu_offload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c83a9f4",
   "metadata": {},
   "source": [
    "## ğŸ¨ FLUX åŸºç¤æ¨è«–æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c6b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def generate_flux_image(\n",
    "    pipe: FluxPipeline,\n",
    "    prompt: str,\n",
    "    num_inference_steps: int = 4,\n",
    "    guidance_scale: float = 0.0,  # FLUX schnell typically uses 0.0\n",
    "    width: int = 1024,\n",
    "    height: int = 1024,\n",
    "    generator: Optional[torch.Generator] = None,\n",
    ") -> Tuple[Image.Image, dict]:\n",
    "    \"\"\"Generate image with FLUX pipeline\"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # FLUX schnell parameters\n",
    "    if \"schnell\" in str(pipe.__class__).lower():\n",
    "        num_inference_steps = min(num_inference_steps, 4)  # schnell max 4 steps\n",
    "        guidance_scale = 0.0  # schnell doesn't use guidance\n",
    "\n",
    "    with torch.no_grad():\n",
    "        result = pipe(\n",
    "            prompt=prompt,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            generator=generator,\n",
    "        )\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    metadata = {\n",
    "        \"prompt\": prompt,\n",
    "        \"steps\": num_inference_steps,\n",
    "        \"guidance_scale\": guidance_scale,\n",
    "        \"size\": f\"{width}x{height}\",\n",
    "        \"generation_time\": f\"{end_time - start_time:.2f}s\",\n",
    "        \"model\": \"FLUX.1-schnell\",\n",
    "    }\n",
    "\n",
    "    return result.images[0], metadata\n",
    "\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"a majestic dragon soaring through cloudy mountain peaks, cinematic lighting, epic fantasy art\"\n",
    "if SMOKE_MODE:\n",
    "    test_prompt = \"a simple red apple\"\n",
    "\n",
    "# Generate with FLUX schnell\n",
    "print(f\"Generating with FLUX.1-schnell...\")\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "\n",
    "flux_image, flux_meta = generate_flux_image(\n",
    "    flux_schnell,\n",
    "    prompt=test_prompt,\n",
    "    num_inference_steps=4 if not SMOKE_MODE else 1,\n",
    "    width=1024 if not SMOKE_MODE else 512,\n",
    "    height=1024 if not SMOKE_MODE else 512,\n",
    "    generator=generator,\n",
    ")\n",
    "\n",
    "# Display result\n",
    "print(f\"âœ… Generated in {flux_meta['generation_time']}\")\n",
    "print(f\"Metadata: {json.dumps(flux_meta, indent=2)}\")\n",
    "flux_image.show() if not SMOKE_MODE else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552cd62e",
   "metadata": {},
   "source": [
    "## ğŸ“Š Memory Usage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc616e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def get_memory_stats() -> dict:\n",
    "    \"\"\"Get current CUDA memory statistics\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return {\"error\": \"CUDA not available\"}\n",
    "\n",
    "    return {\n",
    "        \"allocated_gb\": torch.cuda.memory_allocated() / 1e9,\n",
    "        \"reserved_gb\": torch.cuda.memory_reserved() / 1e9,\n",
    "        \"max_allocated_gb\": torch.cuda.max_memory_allocated() / 1e9,\n",
    "        \"total_gb\": torch.cuda.get_device_properties(0).total_memory / 1e9,\n",
    "    }\n",
    "\n",
    "\n",
    "memory_after_flux = get_memory_stats()\n",
    "print(\"Memory usage after FLUX generation:\")\n",
    "print(json.dumps(memory_after_flux, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee1ff1e",
   "metadata": {},
   "source": [
    "## ğŸ”„ Clean up and load FLUX.1-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa5a493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Clean up schnell model\n",
    "del flux_schnell\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Loading FLUX.1-dev...\")\n",
    "\n",
    "\n",
    "def load_flux_dev(enable_cpu_offload: bool = True) -> FluxPipeline:\n",
    "    \"\"\"Load FLUX.1-dev with memory optimization\"\"\"\n",
    "\n",
    "    pipe = FluxPipeline.from_pretrained(\n",
    "        \"black-forest-labs/FLUX.1-dev\",\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "        device_map=\"auto\" if not enable_cpu_offload else None,\n",
    "        use_safetensors=True,\n",
    "    )\n",
    "\n",
    "    if enable_cpu_offload:\n",
    "        pipe.to(\"cuda\")\n",
    "        pipe.enable_sequential_cpu_offload()\n",
    "\n",
    "    pipe.enable_attention_slicing()\n",
    "    if hasattr(pipe, \"enable_xformers_memory_efficient_attention\"):\n",
    "        try:\n",
    "            pipe.enable_xformers_memory_efficient_attention()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "# Skip FLUX.1-dev in smoke mode (memory intensive)\n",
    "if not SMOKE_MODE:\n",
    "    flux_dev = load_flux_dev(enable_cpu_offload=True)\n",
    "else:\n",
    "    print(\"âš ï¸ Skipping FLUX.1-dev in SMOKE_MODE\")\n",
    "    flux_dev = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063974d4",
   "metadata": {},
   "source": [
    "## ğŸ­ FLUX.1-dev é«˜å“è³ªæ¨è«– (25-50 steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59575950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "if flux_dev is not None:\n",
    "    print(\"Generating with FLUX.1-dev...\")\n",
    "\n",
    "    # FLUX.1-dev supports guidance and more steps\n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "\n",
    "    flux_dev_image, flux_dev_meta = generate_flux_image(\n",
    "        flux_dev,\n",
    "        prompt=test_prompt,\n",
    "        num_inference_steps=25,  # dev supports more steps\n",
    "        guidance_scale=3.5,  # dev supports guidance\n",
    "        width=1024,\n",
    "        height=1024,\n",
    "        generator=generator,\n",
    "    )\n",
    "\n",
    "    flux_dev_meta[\"model\"] = \"FLUX.1-dev\"\n",
    "    print(f\"âœ… Generated in {flux_dev_meta['generation_time']}\")\n",
    "    print(f\"Metadata: {json.dumps(flux_dev_meta, indent=2)}\")\n",
    "    flux_dev_image.show()\n",
    "\n",
    "    memory_after_dev = get_memory_stats()\n",
    "    print(\"Memory usage after FLUX.1-dev:\")\n",
    "    print(json.dumps(memory_after_dev, indent=2))\n",
    "else:\n",
    "    print(\"âš ï¸ FLUX.1-dev skipped in SMOKE_MODE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404061f4",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ åƒæ•¸å½±éŸ¿å¯¦é©— (Parameter Experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc11e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def parameter_experiment(pipe: FluxPipeline, model_name: str):\n",
    "    \"\"\"Test different parameters on Flow-DiT models\"\"\"\n",
    "\n",
    "    if SMOKE_MODE:\n",
    "        print(\"âš ï¸ Skipping parameter experiments in SMOKE_MODE\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nğŸ§ª Parameter experiments for {model_name}\")\n",
    "\n",
    "    base_prompt = \"a cozy coffee shop in cyberpunk Tokyo, neon lights, rain\"\n",
    "    experiments = []\n",
    "\n",
    "    # Different step counts\n",
    "    step_configs = [4, 10, 25] if \"dev\" in model_name.lower() else [1, 4]\n",
    "\n",
    "    for steps in step_configs:\n",
    "        generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "\n",
    "        # Adjust guidance for model type\n",
    "        guidance = 3.5 if \"dev\" in model_name.lower() else 0.0\n",
    "\n",
    "        start_time = time.time()\n",
    "        result = pipe(\n",
    "            prompt=base_prompt,\n",
    "            num_inference_steps=steps,\n",
    "            guidance_scale=guidance,\n",
    "            width=512,  # Smaller for experiments\n",
    "            height=512,\n",
    "            generator=generator\n",
    "        )\n",
    "        generation_time = time.time() - start_time\n",
    "\n",
    "        experiments.append({\n",
    "            \"steps\": steps,\n",
    "            \"guidance\": guidance,\n",
    "            \"time\": f\"{generation_time:.2f}s\",\n",
    "            \"model\": model_name\n",
    "        })\n",
    "\n",
    "        print(f\"Steps {steps}: {generation_time:.2f}s\")\n",
    "\n",
    "    return experiments\n",
    "\n",
    "# Run experiments\n",
    "if flux_dev is not None:\n",
    "    dev_experiments = parameter_experiment(flux_dev, \"FLUX.1-dev\")\n",
    "else:\n",
    "    print(\"âš ï¸ FLUX.1-dev experiments skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd58fa3",
   "metadata": {},
   "source": [
    "## ğŸ†š Flow-DiT vs SD å°æ¯”åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5be9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def compare_architectures():\n",
    "    \"\"\"Compare Flow-DiT vs traditional Diffusion characteristics\"\"\"\n",
    "\n",
    "    comparison = {\n",
    "        \"architecture\": {\n",
    "            \"Flow-DiT (FLUX)\": {\n",
    "                \"noise_schedule\": \"Flow matching (continuous)\",\n",
    "                \"sampler\": \"Euler/Heun\",\n",
    "                \"guidance\": \"Natural integration\",\n",
    "                \"steps_range\": \"1-50\",\n",
    "                \"memory_efficiency\": \"Good (DiT architecture)\",\n",
    "            },\n",
    "            \"Diffusion (SD)\": {\n",
    "                \"noise_schedule\": \"DDPM/DDIM (discrete)\",\n",
    "                \"sampler\": \"DPM++/DDIM/Euler\",\n",
    "                \"guidance\": \"CFG (classifier-free)\",\n",
    "                \"steps_range\": \"20-150\",\n",
    "                \"memory_efficiency\": \"Moderate (U-Net)\",\n",
    "            },\n",
    "        },\n",
    "        \"performance\": {\n",
    "            \"FLUX.1-schnell\": {\n",
    "                \"optimal_steps\": 4,\n",
    "                \"speed\": \"Very Fast\",\n",
    "                \"quality\": \"Good\",\n",
    "                \"vram_usage\": \"~12GB\",\n",
    "            },\n",
    "            \"FLUX.1-dev\": {\n",
    "                \"optimal_steps\": 25,\n",
    "                \"speed\": \"Medium\",\n",
    "                \"quality\": \"Excellent\",\n",
    "                \"vram_usage\": \"~14GB\",\n",
    "            },\n",
    "            \"SD1.5\": {\n",
    "                \"optimal_steps\": 20,\n",
    "                \"speed\": \"Fast\",\n",
    "                \"quality\": \"Good\",\n",
    "                \"vram_usage\": \"~6GB\",\n",
    "            },\n",
    "            \"SDXL\": {\n",
    "                \"optimal_steps\": 30,\n",
    "                \"speed\": \"Medium\",\n",
    "                \"quality\": \"Excellent\",\n",
    "                \"vram_usage\": \"~8GB\",\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    print(\"ğŸ†š Architecture Comparison:\")\n",
    "    print(json.dumps(comparison, indent=2))\n",
    "\n",
    "    return comparison\n",
    "\n",
    "\n",
    "architecture_comparison = compare_architectures()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbbe09b",
   "metadata": {},
   "source": [
    "## ğŸ’¾ çµæœä¿å­˜èˆ‡æ¸…ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d4cac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Save results\n",
    "output_dir = pathlib.Path(\"outputs\") / \"flowdit_quickstart\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save images and metadata\n",
    "if \"flux_image\" in locals():\n",
    "    flux_image.save(output_dir / \"flux_schnell_result.png\")\n",
    "    with open(output_dir / \"flux_schnell_meta.json\", \"w\") as f:\n",
    "        json.dump(flux_meta, f, indent=2)\n",
    "\n",
    "if \"flux_dev_image\" in locals():\n",
    "    flux_dev_image.save(output_dir / \"flux_dev_result.png\")\n",
    "    with open(output_dir / \"flux_dev_meta.json\", \"w\") as f:\n",
    "        json.dump(flux_dev_meta, f, indent=2)\n",
    "\n",
    "# Save comparison analysis\n",
    "with open(output_dir / \"architecture_comparison.json\", \"w\") as f:\n",
    "    json.dump(architecture_comparison, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_dir}\")\n",
    "\n",
    "# Clean up memory\n",
    "if \"flux_dev\" in locals() and flux_dev is not None:\n",
    "    del flux_dev\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "final_memory = get_memory_stats()\n",
    "print(\"Final memory usage:\")\n",
    "print(json.dumps(final_memory, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17ed0a5",
   "metadata": {},
   "source": [
    "## ğŸ§ª Smoke Test (CI Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf5e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def smoke_test():\n",
    "    \"\"\"Minimal test for CI pipeline\"\"\"\n",
    "\n",
    "    print(\"ğŸ§ª Running smoke test...\")\n",
    "\n",
    "    # Test basic imports\n",
    "    assert torch.cuda.is_available(), \"CUDA required\"\n",
    "\n",
    "    # Test pipeline loading (schnell only in smoke mode)\n",
    "    test_pipe = FluxPipeline.from_pretrained(\n",
    "        \"black-forest-labs/FLUX.1-schnell\", torch_dtype=torch.float16, device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Quick generation test\n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "    result = test_pipe(\n",
    "        \"a red apple\", num_inference_steps=1, width=256, height=256, generator=generator\n",
    "    )\n",
    "\n",
    "    assert len(result.images) == 1, \"Should generate 1 image\"\n",
    "    assert result.images[0].size == (256, 256), \"Wrong image size\"\n",
    "\n",
    "    print(\"âœ… Smoke test passed!\")\n",
    "\n",
    "    # Cleanup\n",
    "    del test_pipe\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "if SMOKE_MODE:\n",
    "    smoke_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfa1744",
   "metadata": {},
   "source": [
    " ## ğŸ“‹ å®Œæˆæ‘˜è¦ (Summary)\n",
    " \n",
    " ### âœ… å·²å®Œæˆ\n",
    " 1. **Flow-DiT æ¶æ§‹ç†è§£**: Flow matching vs Diffusion æ ¸å¿ƒå·®ç•°\n",
    " 2. **FLUX æ¨¡å‹å¯¦ä½œ**: schnell (4æ­¥) èˆ‡ dev (25æ­¥) æ¨è«–\n",
    " 3. **è¨˜æ†¶é«”æœ€ä½³åŒ–**: CPU offload + attention slicing (12-16GB)\n",
    " 4. **åƒæ•¸èª¿å„ª**: steps/guidance å° Flow-DiT çš„å½±éŸ¿åˆ†æ\n",
    " 5. **æ¶æ§‹å°æ¯”**: Flow-DiT vs SD åœ¨é€Ÿåº¦/å“è³ª/è¨˜æ†¶é«”æ¬Šè¡¡\n",
    " \n",
    " ### ğŸ”‘ æ ¸å¿ƒæ¦‚å¿µ\n",
    " - **Flow Matching**: ç›´æ¥å­¸ç¿’ noiseâ†’image è»Œè·¡ï¼Œæ¯” diffusion æ›´é«˜æ•ˆ\n",
    " - **FLUX.1-schnell**: 4æ­¥å¿«é€Ÿæ¨è«–ï¼Œguidance_scale=0.0\n",
    " - **FLUX.1-dev**: 25-50æ­¥é«˜å“è³ªï¼Œæ”¯æ´ guidance_scale=3.5\n",
    " - **DiT vs U-Net**: Transformer æ¶æ§‹ vs å·ç©æ¶æ§‹çš„è¨˜æ†¶é«”ç‰¹æ€§\n",
    " \n",
    " ### âš ï¸ å¸¸è¦‹å‘ (Pitfalls)\n",
    " 1. **VRAM éœ€æ±‚é«˜**: FLUX éœ€ 12GB+ï¼Œæ³¨æ„ CPU offload\n",
    " 2. **æˆæ¬Šé™åˆ¶**: éƒ¨åˆ† Flow-DiT æ¨¡å‹åƒ…æ¨è«–ï¼Œç„¡æ³•å¾®èª¿\n",
    " 3. **Guidance å·®ç•°**: schnell ä¸æ”¯æ´ guidanceï¼Œdev æ‰æ”¯æ´\n",
    " 4. **Step æ•¸é™åˆ¶**: schnell æœ€å¤š 4æ­¥ï¼Œdev å»ºè­° 25æ­¥ä»¥ä¸Š\n",
    "  \n",
    " ### â¡ï¸ ä¸‹ä¸€æ­¥\n",
    " - **Stage 2**: ControlNet/T2I-Adapter åœ¨ Flow-DiT çš„é©ç”¨æ€§\n",
    " - **Stage 3**: Flow-DiT å¾®èª¿å¯è¡Œæ€§è©•ä¼° (å–æ±ºæ–¼æ¨¡å‹æˆæ¬Š)\n",
    " - **Stage 4**: Flow-DiT æ•´åˆåˆ°æ‰¹æ¬¡ç”Ÿåœ–æµç¨‹"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
