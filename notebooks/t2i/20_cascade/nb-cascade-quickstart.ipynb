{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8999359c",
   "metadata": {},
   "source": [
    "# # ğŸ”ï¸ Stable Cascade Quickstart\n",
    "# \n",
    "# **å­¸ç¿’ç›®æ¨™**: æŒæ¡ Stable Cascade é›™éšæ®µæ¶æ§‹æ¨è«–ï¼Œç†è§£èˆ‡ SD çš„å·®ç•°ï¼Œå¯¦ä½œä½ VRAM å„ªåŒ–ç­–ç•¥\n",
    "# \n",
    "# **Cascade ç‰¹è‰²**:\n",
    "# - ğŸ’ é›™éšæ®µè¨­è¨ˆï¼šStage B (å£“ç¸®) + Stage C (ç”Ÿæˆ)\n",
    "# - ğŸ–¼ï¸ åŸç”Ÿé«˜è§£æåº¦æ”¯æ´ (1024x1024+)  \n",
    "# - ğŸ¯ æ›´å¥½çš„æ–‡æœ¬-åœ–åƒå°é½Š\n",
    "# - âš¡ ç›¸å°é«˜æ•ˆçš„æ¨è«–é€Ÿåº¦\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca807a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [1] Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (è¤‡è£½åˆ°æ¯æœ¬ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01393a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ğŸ“¦ Package imports & environment validation\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from diffusers import StableCascadeDecoderPipeline, StableCascadePriorPipeline\n",
    "from diffusers.utils import load_image\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "import gc\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "\n",
    "# Environment flags\n",
    "SMOKE_MODE = os.getenv(\"SMOKE_MODE\", \"false\").lower() == \"true\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "print(f\"ğŸš€ Environment: {DEVICE} | Precision: {DTYPE} | Smoke Mode: {SMOKE_MODE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0484f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ğŸ—ï¸ Cascade Architecture Overview & Model Loading\n",
    "print(\"ğŸ“š Stable Cascade Architecture:\")\n",
    "print(\"Stage C (Prior): Text â†’ Latent Space (24x24)\")\n",
    "print(\"Stage B (Decoder): Latent (24x24) â†’ High-res Image (1024x1024)\")\n",
    "print(\"\\nğŸ”½ Loading Cascade pipelines...\")\n",
    "\n",
    "# Prior pipeline (Stage C) - text to latent\n",
    "prior_pipeline = StableCascadePriorPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-cascade-prior\",\n",
    "    torch_dtype=DTYPE,\n",
    "    variant=\"bf16\" if DTYPE == torch.bfloat16 else \"fp16\",\n",
    ")\n",
    "\n",
    "# Decoder pipeline (Stage B) - latent to image\n",
    "decoder_pipeline = StableCascadeDecoderPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-cascade\",\n",
    "    torch_dtype=DTYPE,\n",
    "    variant=\"bf16\" if DTYPE == torch.bfloat16 else \"fp16\",\n",
    ")\n",
    "\n",
    "print(\"âœ… Cascade pipelines loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3936eb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# âš™ï¸ Low-VRAM optimization strategies\n",
    "def setup_low_vram_cascade(prior_pipe, decoder_pipe, enable_optimizations=True):\n",
    "    \"\"\"Apply memory optimizations for 8-12GB VRAM\"\"\"\n",
    "    if not enable_optimizations:\n",
    "        return prior_pipe, decoder_pipe\n",
    "\n",
    "    print(\"ğŸ”§ Applying low-VRAM optimizations...\")\n",
    "\n",
    "    # Enable attention slicing (reduces VRAM usage)\n",
    "    prior_pipe.enable_attention_slicing()\n",
    "    decoder_pipe.enable_attention_slicing()\n",
    "\n",
    "    # Enable memory efficient attention (xFormers)\n",
    "    try:\n",
    "        prior_pipe.enable_xformers_memory_efficient_attention()\n",
    "        decoder_pipe.enable_xformers_memory_efficient_attention()\n",
    "        print(\"âœ… xFormers memory efficient attention enabled\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ xFormers not available: {e}\")\n",
    "\n",
    "    # Sequential CPU offload for very low VRAM (< 8GB)\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory // (1024**3)\n",
    "    if vram_gb < 10:\n",
    "        print(f\"ğŸ”„ VRAM ({vram_gb}GB < 10GB) - enabling sequential CPU offload\")\n",
    "        prior_pipe.enable_sequential_cpu_offload()\n",
    "        decoder_pipe.enable_sequential_cpu_offload()\n",
    "    else:\n",
    "        # Model CPU offload for moderate VRAM\n",
    "        prior_pipe.enable_model_cpu_offload()\n",
    "        decoder_pipe.enable_model_cpu_offload()\n",
    "\n",
    "    return prior_pipe, decoder_pipe\n",
    "\n",
    "\n",
    "# Apply optimizations\n",
    "prior_pipeline, decoder_pipeline = setup_low_vram_cascade(\n",
    "    prior_pipeline, decoder_pipeline, enable_optimizations=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9dfcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ğŸš€ Minimal Working Example (MVP)\n",
    "def generate_cascade_image(\n",
    "    prompt: str,\n",
    "    negative_prompt: str = \"\",\n",
    "    height: int = 1024,\n",
    "    width: int = 1024,\n",
    "    prior_guidance_scale: float = 4.0,\n",
    "    decoder_guidance_scale: float = 0.0,\n",
    "    num_inference_steps_prior: int = 20,\n",
    "    num_inference_steps_decoder: int = 10,\n",
    "    seed: Optional[int] = None,\n",
    ") -> Tuple[Image.Image, dict]:\n",
    "    \"\"\"\n",
    "    Generate image using Stable Cascade two-stage process\n",
    "\n",
    "    Returns:\n",
    "        tuple: (generated_image, metadata_dict)\n",
    "    \"\"\"\n",
    "    # Set seed for reproducibility\n",
    "    generator = torch.Generator(device=DEVICE)\n",
    "    if seed is not None:\n",
    "        generator.manual_seed(seed)\n",
    "\n",
    "    # Clear GPU cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Stage C: Text to latent (Prior)\n",
    "    print(f\"ğŸ¯ Stage C (Prior): '{prompt[:50]}...'\")\n",
    "    prior_output = prior_pipeline(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        guidance_scale=prior_guidance_scale,\n",
    "        num_inference_steps=num_inference_steps_prior,\n",
    "        generator=generator,\n",
    "    )\n",
    "\n",
    "    # Stage B: Latent to image (Decoder)\n",
    "    print(\"ğŸ–¼ï¸ Stage B (Decoder): Latent â†’ High-res image\")\n",
    "    decoder_output = decoder_pipeline(\n",
    "        image_embeddings=prior_output.image_embeddings,\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        guidance_scale=decoder_guidance_scale,\n",
    "        num_inference_steps=num_inference_steps_decoder,\n",
    "        generator=generator,\n",
    "    )\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Prepare metadata\n",
    "    metadata = {\n",
    "        \"prompt\": prompt,\n",
    "        \"negative_prompt\": negative_prompt,\n",
    "        \"dimensions\": f\"{width}x{height}\",\n",
    "        \"prior_steps\": num_inference_steps_prior,\n",
    "        \"decoder_steps\": num_inference_steps_decoder,\n",
    "        \"prior_cfg\": prior_guidance_scale,\n",
    "        \"decoder_cfg\": decoder_guidance_scale,\n",
    "        \"seed\": seed,\n",
    "        \"generation_time\": round(elapsed_time, 2),\n",
    "        \"model\": \"stable-cascade\",\n",
    "    }\n",
    "\n",
    "    print(f\"âœ… Generated in {elapsed_time:.2f}s\")\n",
    "    return decoder_output.images[0], metadata\n",
    "\n",
    "\n",
    "# MVP Example\n",
    "prompt = \"a majestic mountain landscape with aurora borealis, oil painting style, highly detailed\"\n",
    "image, meta = generate_cascade_image(\n",
    "    prompt=prompt,\n",
    "    height=1024 if not SMOKE_MODE else 512,\n",
    "    width=1024 if not SMOKE_MODE else 512,\n",
    "    num_inference_steps_prior=20 if not SMOKE_MODE else 5,\n",
    "    num_inference_steps_decoder=10 if not SMOKE_MODE else 3,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š Generation metadata: {json.dumps(meta, indent=2)}\")\n",
    "image.save(\"cascade_mountain_aurora.png\")\n",
    "print(\"ğŸ’¾ Saved: cascade_mountain_aurora.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42be602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ğŸ¯ Parameter exploration & comparison\n",
    "def cascade_parameter_study():\n",
    "    \"\"\"Compare different parameter settings for Cascade\"\"\"\n",
    "\n",
    "    base_prompt = \"a serene japanese garden with cherry blossoms\"\n",
    "    configs = (\n",
    "        [\n",
    "            {\"name\": \"Fast\", \"prior_steps\": 10, \"decoder_steps\": 5, \"prior_cfg\": 2.0},\n",
    "            {\n",
    "                \"name\": \"Balanced\",\n",
    "                \"prior_steps\": 20,\n",
    "                \"decoder_steps\": 10,\n",
    "                \"prior_cfg\": 4.0,\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Quality\",\n",
    "                \"prior_steps\": 30,\n",
    "                \"decoder_steps\": 15,\n",
    "                \"prior_cfg\": 6.0,\n",
    "            },\n",
    "        ]\n",
    "        if not SMOKE_MODE\n",
    "        else [{\"name\": \"Smoke\", \"prior_steps\": 3, \"decoder_steps\": 2, \"prior_cfg\": 2.0}]\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for config in configs:\n",
    "        print(f\"\\nğŸ§ª Testing {config['name']} preset...\")\n",
    "\n",
    "        image, meta = generate_cascade_image(\n",
    "            prompt=base_prompt,\n",
    "            height=768 if not SMOKE_MODE else 512,\n",
    "            width=768 if not SMOKE_MODE else 512,\n",
    "            num_inference_steps_prior=config[\"prior_steps\"],\n",
    "            num_inference_steps_decoder=config[\"decoder_steps\"],\n",
    "            prior_guidance_scale=config[\"prior_cfg\"],\n",
    "            seed=123,\n",
    "        )\n",
    "\n",
    "        # Save comparison image\n",
    "        filename = f\"cascade_garden_{config['name'].lower()}.png\"\n",
    "        image.save(filename)\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"preset\": config[\"name\"],\n",
    "                \"time\": meta[\"generation_time\"],\n",
    "                \"prior_steps\": config[\"prior_steps\"],\n",
    "                \"decoder_steps\": config[\"decoder_steps\"],\n",
    "                \"filename\": filename,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Print comparison table\n",
    "    print(\"\\nğŸ“ˆ Parameter Study Results:\")\n",
    "    print(\"Preset    | Time(s) | Prior Steps | Decoder Steps | Quality Notes\")\n",
    "    print(\"----------|---------|-------------|---------------|---------------\")\n",
    "    for r in results:\n",
    "        print(\n",
    "            f\"{r['preset']:<9} | {r['time']:<7} | {r['prior_steps']:<11} | {r['decoder_steps']:<13} | See {r['filename']}\"\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "study_results = cascade_parameter_study()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56ec652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ğŸ“Š Cascade vs SD benchmark comparison\n",
    "def cascade_vs_sd_benchmark():\n",
    "    \"\"\"Quick benchmark comparison between Cascade and SD\"\"\"\n",
    "\n",
    "    test_prompt = \"a cyberpunk cityscape at night, neon lights, detailed\"\n",
    "\n",
    "    print(\"ğŸ Cascade vs SD Benchmark\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Cascade timing (already loaded)\n",
    "    cascade_start = time.time()\n",
    "    cascade_img, cascade_meta = generate_cascade_image(\n",
    "        prompt=test_prompt,\n",
    "        height=768 if not SMOKE_MODE else 512,\n",
    "        width=768 if not SMOKE_MODE else 512,\n",
    "        num_inference_steps_prior=15 if not SMOKE_MODE else 3,\n",
    "        num_inference_steps_decoder=8 if not SMOKE_MODE else 2,\n",
    "        seed=999,\n",
    "    )\n",
    "    cascade_time = time.time() - cascade_start\n",
    "\n",
    "    # For fair comparison, we'd load SD here (commented out for this demo)\n",
    "    # sd_pipeline = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
    "    sd_time_estimate = \"~8-12s\"  # Typical SD1.5 timing for reference\n",
    "\n",
    "    print(f\"\\nâš¡ Performance Comparison:\")\n",
    "    print(f\"Cascade: {cascade_time:.2f}s @ {cascade_meta['dimensions']}\")\n",
    "    print(f\"SD 1.5:  {sd_time_estimate} @ 512x512 (estimated)\")\n",
    "    print(f\"\\nğŸ¨ Quality Notes:\")\n",
    "    print(\"- Cascade: Better text alignment, native high-res\")\n",
    "    print(\"- SD 1.5: Faster inference, wider ecosystem support\")\n",
    "\n",
    "    cascade_img.save(\"cascade_cyberpunk_benchmark.png\")\n",
    "\n",
    "    return {\n",
    "        \"cascade_time\": cascade_time,\n",
    "        \"cascade_resolution\": cascade_meta[\"dimensions\"],\n",
    "        \"sd_estimate\": sd_time_estimate,\n",
    "    }\n",
    "\n",
    "\n",
    "benchmark_results = cascade_vs_sd_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07693f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ğŸ§ª Smoke test (CI-compatible)\n",
    "def smoke_test_cascade():\n",
    "    \"\"\"Minimal test for CI/automated testing\"\"\"\n",
    "    print(\"ğŸ§ª Running Cascade smoke test...\")\n",
    "\n",
    "    # Ultra-minimal generation\n",
    "    test_img, test_meta = generate_cascade_image(\n",
    "        prompt=\"a red apple\",\n",
    "        height=512,\n",
    "        width=512,\n",
    "        num_inference_steps_prior=3,\n",
    "        num_inference_steps_decoder=2,\n",
    "        seed=1337,\n",
    "    )\n",
    "\n",
    "    # Validate output\n",
    "    assert test_img.size == (512, 512), f\"Expected 512x512, got {test_img.size}\"\n",
    "    assert (\n",
    "        test_meta[\"generation_time\"] < 30\n",
    "    ), f\"Too slow: {test_meta['generation_time']}s\"\n",
    "\n",
    "    test_img.save(\"cascade_smoke_test.png\")\n",
    "    print(\"âœ… Smoke test passed!\")\n",
    "\n",
    "    return test_img, test_meta\n",
    "\n",
    "\n",
    "if SMOKE_MODE:\n",
    "    smoke_img, smoke_meta = smoke_test_cascade()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a8bb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ğŸ’¾ Save configuration and cleanup\n",
    "cascade_config = {\n",
    "    \"model_name\": \"stable-cascade\",\n",
    "    \"prior_model\": \"stabilityai/stable-cascade-prior\",\n",
    "    \"decoder_model\": \"stabilityai/stable-cascade\",\n",
    "    \"dtype\": str(DTYPE),\n",
    "    \"device\": DEVICE,\n",
    "    \"optimizations\": {\n",
    "        \"attention_slicing\": True,\n",
    "        \"xformers\": True,\n",
    "        \"cpu_offload\": (\n",
    "            \"sequential\"\n",
    "            if torch.cuda.get_device_properties(0).total_memory // (1024**3) < 10\n",
    "            else \"model\"\n",
    "        ),\n",
    "    },\n",
    "    \"recommended_params\": {\n",
    "        \"prior_steps\": 20,\n",
    "        \"decoder_steps\": 10,\n",
    "        \"prior_guidance_scale\": 4.0,\n",
    "        \"decoder_guidance_scale\": 0.0,\n",
    "        \"resolution\": \"1024x1024\",\n",
    "    },\n",
    "    \"benchmark_results\": benchmark_results,\n",
    "}\n",
    "\n",
    "# Save config for future reference\n",
    "with open(\"cascade_config.json\", \"w\") as f:\n",
    "    json.dump(cascade_config, f, indent=2)\n",
    "\n",
    "print(\"ğŸ’¾ Configuration saved to cascade_config.json\")\n",
    "\n",
    "# Cleanup memory\n",
    "del prior_pipeline, decoder_pipeline\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"ğŸ§¹ Memory cleanup completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcbc854",
   "metadata": {},
   "source": [
    "# ## ğŸ“‹ Stage Summary\n",
    "# \n",
    "# ### âœ… Completed\n",
    "# - ğŸ—ï¸ Stable Cascade é›™éšæ®µæ¶æ§‹ç†è§£èˆ‡å¯¦ä½œ\n",
    "# - âš™ï¸ 8-12GB VRAM ä½è¨˜æ†¶é«”å„ªåŒ–ç­–ç•¥\n",
    "# - ğŸ¯ åƒæ•¸èª¿å„ªèˆ‡å“è³ª/é€Ÿåº¦æ¬Šè¡¡åˆ†æ\n",
    "# - ğŸ“Š Cascade vs SD åŸºæº–æ¯”è¼ƒ\n",
    "# - ğŸ§ª CI-compatible ç…™éœ§æ¸¬è©¦\n",
    "# \n",
    "# ### ğŸ§  Key Concepts\n",
    "# - **é›™éšæ®µè¨­è¨ˆ**: Stage C (textâ†’latent) + Stage B (latentâ†’image)\n",
    "# - **åŸç”Ÿé«˜è§£æåº¦**: ç›´æ¥æ”¯æ´ 1024x1024+ ç„¡éœ€é¡å¤–è¶…åˆ†\n",
    "# - **è¨˜æ†¶é«”ç®¡ç†**: attention slicing + CPU offload ç­–ç•¥\n",
    "# - **åƒæ•¸æ¬Šè¡¡**: prior_steps vs decoder_steps çš„é€Ÿåº¦/å“è³ªå¹³è¡¡\n",
    "# \n",
    "# ### âš ï¸ Common Pitfalls  \n",
    "# - decoder_guidance_scale é€šå¸¸è¨­ç‚º 0.0ï¼ˆèˆ‡ SD ä¸åŒï¼‰\n",
    "# - é›™ pipeline è¼‰å…¥éœ€è¦æ›´å¤š VRAMï¼ˆéœ€è¦ CPU offloadï¼‰\n",
    "# - Stage C çš„ guidance_scale æ¯” SD å»ºè­°å€¼æ›´ä½ (2.0-6.0)\n",
    "# \n",
    "# ### ğŸ¯ Next Steps\n",
    "# - **ControlNet**: Cascade æ¢ä»¶æ§åˆ¶å¯¦ä½œï¼ˆè‹¥æ”¯æ´ï¼‰\n",
    "# - **Fine-tuning**: Stage B/C çš„ LoRA å¾®èª¿æ¢ç´¢\n",
    "# - **Img2Img**: Cascade åœ–ç”Ÿåœ–æµç¨‹å¯¦ä½œ\n",
    "# - **Pipeline Integration**: èˆ‡ SD æ··åˆä½¿ç”¨ç­–ç•¥"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
