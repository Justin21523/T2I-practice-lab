{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8999359c",
   "metadata": {},
   "source": [
    "# # 🏔️ Stable Cascade Quickstart\n",
    "# \n",
    "# **學習目標**: 掌握 Stable Cascade 雙階段架構推論，理解與 SD 的差異，實作低 VRAM 優化策略\n",
    "# \n",
    "# **Cascade 特色**:\n",
    "# - 💎 雙階段設計：Stage B (壓縮) + Stage C (生成)\n",
    "# - 🖼️ 原生高解析度支援 (1024x1024+)  \n",
    "# - 🎯 更好的文本-圖像對齊\n",
    "# - ⚡ 相對高效的推論速度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca807a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [1] Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (複製到每本 notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01393a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 📦 Package imports & environment validation\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from diffusers import StableCascadeDecoderPipeline, StableCascadePriorPipeline\n",
    "from diffusers.utils import load_image\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "import gc\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "\n",
    "# Environment flags\n",
    "SMOKE_MODE = os.getenv(\"SMOKE_MODE\", \"false\").lower() == \"true\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "print(f\"🚀 Environment: {DEVICE} | Precision: {DTYPE} | Smoke Mode: {SMOKE_MODE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0484f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 🏗️ Cascade Architecture Overview & Model Loading\n",
    "print(\"📚 Stable Cascade Architecture:\")\n",
    "print(\"Stage C (Prior): Text → Latent Space (24x24)\")\n",
    "print(\"Stage B (Decoder): Latent (24x24) → High-res Image (1024x1024)\")\n",
    "print(\"\\n🔽 Loading Cascade pipelines...\")\n",
    "\n",
    "# Prior pipeline (Stage C) - text to latent\n",
    "prior_pipeline = StableCascadePriorPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-cascade-prior\",\n",
    "    torch_dtype=DTYPE,\n",
    "    variant=\"bf16\" if DTYPE == torch.bfloat16 else \"fp16\",\n",
    ")\n",
    "\n",
    "# Decoder pipeline (Stage B) - latent to image\n",
    "decoder_pipeline = StableCascadeDecoderPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-cascade\",\n",
    "    torch_dtype=DTYPE,\n",
    "    variant=\"bf16\" if DTYPE == torch.bfloat16 else \"fp16\",\n",
    ")\n",
    "\n",
    "print(\"✅ Cascade pipelines loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3936eb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ⚙️ Low-VRAM optimization strategies\n",
    "def setup_low_vram_cascade(prior_pipe, decoder_pipe, enable_optimizations=True):\n",
    "    \"\"\"Apply memory optimizations for 8-12GB VRAM\"\"\"\n",
    "    if not enable_optimizations:\n",
    "        return prior_pipe, decoder_pipe\n",
    "\n",
    "    print(\"🔧 Applying low-VRAM optimizations...\")\n",
    "\n",
    "    # Enable attention slicing (reduces VRAM usage)\n",
    "    prior_pipe.enable_attention_slicing()\n",
    "    decoder_pipe.enable_attention_slicing()\n",
    "\n",
    "    # Enable memory efficient attention (xFormers)\n",
    "    try:\n",
    "        prior_pipe.enable_xformers_memory_efficient_attention()\n",
    "        decoder_pipe.enable_xformers_memory_efficient_attention()\n",
    "        print(\"✅ xFormers memory efficient attention enabled\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ xFormers not available: {e}\")\n",
    "\n",
    "    # Sequential CPU offload for very low VRAM (< 8GB)\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory // (1024**3)\n",
    "    if vram_gb < 10:\n",
    "        print(f\"🔄 VRAM ({vram_gb}GB < 10GB) - enabling sequential CPU offload\")\n",
    "        prior_pipe.enable_sequential_cpu_offload()\n",
    "        decoder_pipe.enable_sequential_cpu_offload()\n",
    "    else:\n",
    "        # Model CPU offload for moderate VRAM\n",
    "        prior_pipe.enable_model_cpu_offload()\n",
    "        decoder_pipe.enable_model_cpu_offload()\n",
    "\n",
    "    return prior_pipe, decoder_pipe\n",
    "\n",
    "\n",
    "# Apply optimizations\n",
    "prior_pipeline, decoder_pipeline = setup_low_vram_cascade(\n",
    "    prior_pipeline, decoder_pipeline, enable_optimizations=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9dfcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 🚀 Minimal Working Example (MVP)\n",
    "def generate_cascade_image(\n",
    "    prompt: str,\n",
    "    negative_prompt: str = \"\",\n",
    "    height: int = 1024,\n",
    "    width: int = 1024,\n",
    "    prior_guidance_scale: float = 4.0,\n",
    "    decoder_guidance_scale: float = 0.0,\n",
    "    num_inference_steps_prior: int = 20,\n",
    "    num_inference_steps_decoder: int = 10,\n",
    "    seed: Optional[int] = None,\n",
    ") -> Tuple[Image.Image, dict]:\n",
    "    \"\"\"\n",
    "    Generate image using Stable Cascade two-stage process\n",
    "\n",
    "    Returns:\n",
    "        tuple: (generated_image, metadata_dict)\n",
    "    \"\"\"\n",
    "    # Set seed for reproducibility\n",
    "    generator = torch.Generator(device=DEVICE)\n",
    "    if seed is not None:\n",
    "        generator.manual_seed(seed)\n",
    "\n",
    "    # Clear GPU cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Stage C: Text to latent (Prior)\n",
    "    print(f\"🎯 Stage C (Prior): '{prompt[:50]}...'\")\n",
    "    prior_output = prior_pipeline(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        guidance_scale=prior_guidance_scale,\n",
    "        num_inference_steps=num_inference_steps_prior,\n",
    "        generator=generator,\n",
    "    )\n",
    "\n",
    "    # Stage B: Latent to image (Decoder)\n",
    "    print(\"🖼️ Stage B (Decoder): Latent → High-res image\")\n",
    "    decoder_output = decoder_pipeline(\n",
    "        image_embeddings=prior_output.image_embeddings,\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        guidance_scale=decoder_guidance_scale,\n",
    "        num_inference_steps=num_inference_steps_decoder,\n",
    "        generator=generator,\n",
    "    )\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Prepare metadata\n",
    "    metadata = {\n",
    "        \"prompt\": prompt,\n",
    "        \"negative_prompt\": negative_prompt,\n",
    "        \"dimensions\": f\"{width}x{height}\",\n",
    "        \"prior_steps\": num_inference_steps_prior,\n",
    "        \"decoder_steps\": num_inference_steps_decoder,\n",
    "        \"prior_cfg\": prior_guidance_scale,\n",
    "        \"decoder_cfg\": decoder_guidance_scale,\n",
    "        \"seed\": seed,\n",
    "        \"generation_time\": round(elapsed_time, 2),\n",
    "        \"model\": \"stable-cascade\",\n",
    "    }\n",
    "\n",
    "    print(f\"✅ Generated in {elapsed_time:.2f}s\")\n",
    "    return decoder_output.images[0], metadata\n",
    "\n",
    "\n",
    "# MVP Example\n",
    "prompt = \"a majestic mountain landscape with aurora borealis, oil painting style, highly detailed\"\n",
    "image, meta = generate_cascade_image(\n",
    "    prompt=prompt,\n",
    "    height=1024 if not SMOKE_MODE else 512,\n",
    "    width=1024 if not SMOKE_MODE else 512,\n",
    "    num_inference_steps_prior=20 if not SMOKE_MODE else 5,\n",
    "    num_inference_steps_decoder=10 if not SMOKE_MODE else 3,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"📊 Generation metadata: {json.dumps(meta, indent=2)}\")\n",
    "image.save(\"cascade_mountain_aurora.png\")\n",
    "print(\"💾 Saved: cascade_mountain_aurora.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42be602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 🎯 Parameter exploration & comparison\n",
    "def cascade_parameter_study():\n",
    "    \"\"\"Compare different parameter settings for Cascade\"\"\"\n",
    "\n",
    "    base_prompt = \"a serene japanese garden with cherry blossoms\"\n",
    "    configs = (\n",
    "        [\n",
    "            {\"name\": \"Fast\", \"prior_steps\": 10, \"decoder_steps\": 5, \"prior_cfg\": 2.0},\n",
    "            {\n",
    "                \"name\": \"Balanced\",\n",
    "                \"prior_steps\": 20,\n",
    "                \"decoder_steps\": 10,\n",
    "                \"prior_cfg\": 4.0,\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Quality\",\n",
    "                \"prior_steps\": 30,\n",
    "                \"decoder_steps\": 15,\n",
    "                \"prior_cfg\": 6.0,\n",
    "            },\n",
    "        ]\n",
    "        if not SMOKE_MODE\n",
    "        else [{\"name\": \"Smoke\", \"prior_steps\": 3, \"decoder_steps\": 2, \"prior_cfg\": 2.0}]\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for config in configs:\n",
    "        print(f\"\\n🧪 Testing {config['name']} preset...\")\n",
    "\n",
    "        image, meta = generate_cascade_image(\n",
    "            prompt=base_prompt,\n",
    "            height=768 if not SMOKE_MODE else 512,\n",
    "            width=768 if not SMOKE_MODE else 512,\n",
    "            num_inference_steps_prior=config[\"prior_steps\"],\n",
    "            num_inference_steps_decoder=config[\"decoder_steps\"],\n",
    "            prior_guidance_scale=config[\"prior_cfg\"],\n",
    "            seed=123,\n",
    "        )\n",
    "\n",
    "        # Save comparison image\n",
    "        filename = f\"cascade_garden_{config['name'].lower()}.png\"\n",
    "        image.save(filename)\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"preset\": config[\"name\"],\n",
    "                \"time\": meta[\"generation_time\"],\n",
    "                \"prior_steps\": config[\"prior_steps\"],\n",
    "                \"decoder_steps\": config[\"decoder_steps\"],\n",
    "                \"filename\": filename,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Print comparison table\n",
    "    print(\"\\n📈 Parameter Study Results:\")\n",
    "    print(\"Preset    | Time(s) | Prior Steps | Decoder Steps | Quality Notes\")\n",
    "    print(\"----------|---------|-------------|---------------|---------------\")\n",
    "    for r in results:\n",
    "        print(\n",
    "            f\"{r['preset']:<9} | {r['time']:<7} | {r['prior_steps']:<11} | {r['decoder_steps']:<13} | See {r['filename']}\"\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "study_results = cascade_parameter_study()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56ec652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 📊 Cascade vs SD benchmark comparison\n",
    "def cascade_vs_sd_benchmark():\n",
    "    \"\"\"Quick benchmark comparison between Cascade and SD\"\"\"\n",
    "\n",
    "    test_prompt = \"a cyberpunk cityscape at night, neon lights, detailed\"\n",
    "\n",
    "    print(\"🏁 Cascade vs SD Benchmark\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Cascade timing (already loaded)\n",
    "    cascade_start = time.time()\n",
    "    cascade_img, cascade_meta = generate_cascade_image(\n",
    "        prompt=test_prompt,\n",
    "        height=768 if not SMOKE_MODE else 512,\n",
    "        width=768 if not SMOKE_MODE else 512,\n",
    "        num_inference_steps_prior=15 if not SMOKE_MODE else 3,\n",
    "        num_inference_steps_decoder=8 if not SMOKE_MODE else 2,\n",
    "        seed=999,\n",
    "    )\n",
    "    cascade_time = time.time() - cascade_start\n",
    "\n",
    "    # For fair comparison, we'd load SD here (commented out for this demo)\n",
    "    # sd_pipeline = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
    "    sd_time_estimate = \"~8-12s\"  # Typical SD1.5 timing for reference\n",
    "\n",
    "    print(f\"\\n⚡ Performance Comparison:\")\n",
    "    print(f\"Cascade: {cascade_time:.2f}s @ {cascade_meta['dimensions']}\")\n",
    "    print(f\"SD 1.5:  {sd_time_estimate} @ 512x512 (estimated)\")\n",
    "    print(f\"\\n🎨 Quality Notes:\")\n",
    "    print(\"- Cascade: Better text alignment, native high-res\")\n",
    "    print(\"- SD 1.5: Faster inference, wider ecosystem support\")\n",
    "\n",
    "    cascade_img.save(\"cascade_cyberpunk_benchmark.png\")\n",
    "\n",
    "    return {\n",
    "        \"cascade_time\": cascade_time,\n",
    "        \"cascade_resolution\": cascade_meta[\"dimensions\"],\n",
    "        \"sd_estimate\": sd_time_estimate,\n",
    "    }\n",
    "\n",
    "\n",
    "benchmark_results = cascade_vs_sd_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07693f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 🧪 Smoke test (CI-compatible)\n",
    "def smoke_test_cascade():\n",
    "    \"\"\"Minimal test for CI/automated testing\"\"\"\n",
    "    print(\"🧪 Running Cascade smoke test...\")\n",
    "\n",
    "    # Ultra-minimal generation\n",
    "    test_img, test_meta = generate_cascade_image(\n",
    "        prompt=\"a red apple\",\n",
    "        height=512,\n",
    "        width=512,\n",
    "        num_inference_steps_prior=3,\n",
    "        num_inference_steps_decoder=2,\n",
    "        seed=1337,\n",
    "    )\n",
    "\n",
    "    # Validate output\n",
    "    assert test_img.size == (512, 512), f\"Expected 512x512, got {test_img.size}\"\n",
    "    assert (\n",
    "        test_meta[\"generation_time\"] < 30\n",
    "    ), f\"Too slow: {test_meta['generation_time']}s\"\n",
    "\n",
    "    test_img.save(\"cascade_smoke_test.png\")\n",
    "    print(\"✅ Smoke test passed!\")\n",
    "\n",
    "    return test_img, test_meta\n",
    "\n",
    "\n",
    "if SMOKE_MODE:\n",
    "    smoke_img, smoke_meta = smoke_test_cascade()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a8bb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 💾 Save configuration and cleanup\n",
    "cascade_config = {\n",
    "    \"model_name\": \"stable-cascade\",\n",
    "    \"prior_model\": \"stabilityai/stable-cascade-prior\",\n",
    "    \"decoder_model\": \"stabilityai/stable-cascade\",\n",
    "    \"dtype\": str(DTYPE),\n",
    "    \"device\": DEVICE,\n",
    "    \"optimizations\": {\n",
    "        \"attention_slicing\": True,\n",
    "        \"xformers\": True,\n",
    "        \"cpu_offload\": (\n",
    "            \"sequential\"\n",
    "            if torch.cuda.get_device_properties(0).total_memory // (1024**3) < 10\n",
    "            else \"model\"\n",
    "        ),\n",
    "    },\n",
    "    \"recommended_params\": {\n",
    "        \"prior_steps\": 20,\n",
    "        \"decoder_steps\": 10,\n",
    "        \"prior_guidance_scale\": 4.0,\n",
    "        \"decoder_guidance_scale\": 0.0,\n",
    "        \"resolution\": \"1024x1024\",\n",
    "    },\n",
    "    \"benchmark_results\": benchmark_results,\n",
    "}\n",
    "\n",
    "# Save config for future reference\n",
    "with open(\"cascade_config.json\", \"w\") as f:\n",
    "    json.dump(cascade_config, f, indent=2)\n",
    "\n",
    "print(\"💾 Configuration saved to cascade_config.json\")\n",
    "\n",
    "# Cleanup memory\n",
    "del prior_pipeline, decoder_pipeline\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"🧹 Memory cleanup completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcbc854",
   "metadata": {},
   "source": [
    "# ## 📋 Stage Summary\n",
    "# \n",
    "# ### ✅ Completed\n",
    "# - 🏗️ Stable Cascade 雙階段架構理解與實作\n",
    "# - ⚙️ 8-12GB VRAM 低記憶體優化策略\n",
    "# - 🎯 參數調優與品質/速度權衡分析\n",
    "# - 📊 Cascade vs SD 基準比較\n",
    "# - 🧪 CI-compatible 煙霧測試\n",
    "# \n",
    "# ### 🧠 Key Concepts\n",
    "# - **雙階段設計**: Stage C (text→latent) + Stage B (latent→image)\n",
    "# - **原生高解析度**: 直接支援 1024x1024+ 無需額外超分\n",
    "# - **記憶體管理**: attention slicing + CPU offload 策略\n",
    "# - **參數權衡**: prior_steps vs decoder_steps 的速度/品質平衡\n",
    "# \n",
    "# ### ⚠️ Common Pitfalls  \n",
    "# - decoder_guidance_scale 通常設為 0.0（與 SD 不同）\n",
    "# - 雙 pipeline 載入需要更多 VRAM（需要 CPU offload）\n",
    "# - Stage C 的 guidance_scale 比 SD 建議值更低 (2.0-6.0)\n",
    "# \n",
    "# ### 🎯 Next Steps\n",
    "# - **ControlNet**: Cascade 條件控制實作（若支援）\n",
    "# - **Fine-tuning**: Stage B/C 的 LoRA 微調探索\n",
    "# - **Img2Img**: Cascade 圖生圖流程實作\n",
    "# - **Pipeline Integration**: 與 SD 混合使用策略"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
