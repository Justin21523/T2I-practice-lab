{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f9fc709",
   "metadata": {},
   "source": [
    "PixArt-Œ±/DiT Architecture Deep Dive\n",
    "\n",
    "Stage 2 - Cascade Series | 20_cascade/nb-cascade-pixart-dit.ipynb\n",
    "Learning DiT (Diffusion Transformers) architecture through PixArt-Œ± implementation\n",
    "\n",
    "üìö Learning Goals\n",
    "\n",
    "Understand DiT Architecture Evolution - From CNN UNet to Transformer backbone\n",
    "Implement PixArt-Œ± Inference - Master T5 text encoder + DiT combination\n",
    "Compare PixArt vs Stable Diffusion - Architecture, performance, memory trade-offs\n",
    "Explore DiT Family Models - PixArt-Œ±/œÉ, DiT-XL variants\n",
    "Establish DiT Evaluation Baseline - Prepare for future fine-tuning stages\n",
    "\n",
    "üîß Prerequisites\n",
    "\n",
    "VRAM: 6GB+ (12GB+ recommended for 1024px)\n",
    "Environment: Conda t2i-lab with diffusers>=0.30.0\n",
    "License: PixArt-Œ± (OpenRAIL++), T5-XXL (Apache 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcfe114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [1] Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (Ë§áË£ΩÂà∞ÊØèÊú¨ notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7740c4b4",
   "metadata": {},
   "source": [
    "Cell 2: DiT Architecture Theory & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5e9be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DiT vs UNet Architecture Comparison\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_architecture_comparison():\n",
    "    \"\"\"Visualize UNet vs DiT architecture differences\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 8))\n",
    "\n",
    "    # UNet Architecture (Stable Diffusion)\n",
    "    ax1.set_title(\n",
    "        \"UNet Architecture (Stable Diffusion)\", fontsize=14, fontweight=\"bold\"\n",
    "    )\n",
    "    ax1.text(\n",
    "        0.5,\n",
    "        0.9,\n",
    "        \"Text Encoder (CLIP)\\n‚Üì\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"),\n",
    "    )\n",
    "    ax1.text(\n",
    "        0.5,\n",
    "        0.7,\n",
    "        \"Cross-Attention Layers\\n(Text Conditioning)\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\"),\n",
    "    )\n",
    "    ax1.text(\n",
    "        0.5,\n",
    "        0.5,\n",
    "        \"UNet Backbone\\n(CNN + ResNet + Attention)\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightyellow\"),\n",
    "    )\n",
    "    ax1.text(\n",
    "        0.5,\n",
    "        0.3,\n",
    "        \"VAE Decoder\\n‚Üì\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcoral\"),\n",
    "    )\n",
    "    ax1.text(\n",
    "        0.5,\n",
    "        0.1,\n",
    "        \"Final Image\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"),\n",
    "    )\n",
    "\n",
    "    # DiT Architecture (PixArt-Œ±)\n",
    "    ax2.set_title(\"DiT Architecture (PixArt-Œ±)\", fontsize=14, fontweight=\"bold\")\n",
    "    ax2.text(\n",
    "        0.5,\n",
    "        0.9,\n",
    "        \"T5-XXL Text Encoder\\n‚Üì\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"),\n",
    "    )\n",
    "    ax2.text(\n",
    "        0.5,\n",
    "        0.7,\n",
    "        \"Adaptive Layer Norm\\n(AdaLN-Zero)\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\"),\n",
    "    )\n",
    "    ax2.text(\n",
    "        0.5,\n",
    "        0.5,\n",
    "        \"DiT Blocks\\n(Pure Transformer)\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightyellow\"),\n",
    "    )\n",
    "    ax2.text(\n",
    "        0.5,\n",
    "        0.3,\n",
    "        \"VAE Decoder\\n‚Üì\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcoral\"),\n",
    "    )\n",
    "    ax2.text(\n",
    "        0.5,\n",
    "        0.1,\n",
    "        \"Final Image\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"),\n",
    "    )\n",
    "\n",
    "    for ax in [ax1, ax2]:\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Key Differences Table\n",
    "print(\"üèóÔ∏è DiT vs UNet Key Differences:\\n\")\n",
    "comparison_data = [\n",
    "    [\"Architecture\", \"CNN-based UNet\", \"Pure Transformer (DiT)\"],\n",
    "    [\"Text Conditioning\", \"Cross-Attention\", \"Adaptive Layer Norm (AdaLN)\"],\n",
    "    [\"Text Encoder\", \"CLIP (77 tokens)\", \"T5-XXL (120+ tokens)\"],\n",
    "    [\"Scalability\", \"Limited by CNN constraints\", \"Highly scalable with model size\"],\n",
    "    [\"Training Stability\", \"Mature & stable\", \"More complex but flexible\"],\n",
    "    [\"Memory Efficiency\", \"Better for inference\", \"Higher memory requirements\"],\n",
    "    [\"Fine-tuning\", \"LoRA on UNet blocks\", \"LoRA on Transformer blocks\"],\n",
    "]\n",
    "\n",
    "for row in comparison_data:\n",
    "    print(f\"{'üìä ' + row[0]:<20} | {'üîµ ' + row[1]:<25} | {'üü¢ ' + row[2]}\")\n",
    "\n",
    "plot_architecture_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee23d0f",
   "metadata": {},
   "source": [
    "Cell 3: PixArt-Œ± Model Loading & VRAM Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c55756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PixArt-Œ± model loading with VRAM optimization\n",
    "from diffusers import PixArtAlphaPipeline, DPMSolverMultistepScheduler\n",
    "import torch\n",
    "from diffusers.utils import logging\n",
    "\n",
    "logging.set_verbosity_error()  # Reduce diffusers logging\n",
    "\n",
    "\n",
    "def load_pixart_pipeline(\n",
    "    model_id=\"PixArt-alpha/PixArt-XL-2-1024-MS\",\n",
    "    torch_dtype=torch.float16,\n",
    "    enable_cpu_offload=True,\n",
    "    enable_attention_slicing=True,\n",
    "    enable_xformers=True,\n",
    "):\n",
    "    \"\"\"Load PixArt-Œ± pipeline with memory optimizations\"\"\"\n",
    "\n",
    "    print(f\"üöÄ Loading PixArt-Œ±: {model_id}\")\n",
    "    print(f\"   ‚Ä¢ dtype: {torch_dtype}\")\n",
    "    print(f\"   ‚Ä¢ cpu_offload: {enable_cpu_offload}\")\n",
    "    print(f\"   ‚Ä¢ attention_slicing: {enable_attention_slicing}\")\n",
    "\n",
    "    # Load pipeline with optimizations\n",
    "    pipe = PixArtAlphaPipeline.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch_dtype,\n",
    "        use_safetensors=True,\n",
    "        variant=\"fp16\" if torch_dtype == torch.float16 else None,\n",
    "    )\n",
    "\n",
    "    # Apply memory optimizations\n",
    "    if enable_cpu_offload:\n",
    "        pipe.enable_model_cpu_offload()\n",
    "        print(\"   ‚úÖ CPU offload enabled\")\n",
    "    else:\n",
    "        pipe = pipe.to(\"cuda\")\n",
    "\n",
    "    if enable_attention_slicing:\n",
    "        pipe.enable_attention_slicing()\n",
    "        print(\"   ‚úÖ Attention slicing enabled\")\n",
    "\n",
    "    if enable_xformers and torch.cuda.is_available():\n",
    "        try:\n",
    "            pipe.enable_xformers_memory_efficient_attention()\n",
    "            print(\"   ‚úÖ xFormers attention enabled\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è xFormers failed: {e}\")\n",
    "\n",
    "    # Use DPM++ scheduler for better quality\n",
    "    pipe.scheduler = DPMSolverMultistepScheduler.from_config(\n",
    "        pipe.scheduler.config, use_karras_sigmas=True, algorithm_type=\"dpmsolver++\"\n",
    "    )\n",
    "    print(\"   ‚úÖ DPM++ scheduler configured\")\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "# Load the pipeline\n",
    "try:\n",
    "    # Choose model variant based on VRAM\n",
    "    if SMOKE_MODE:\n",
    "        model_id = \"PixArt-alpha/PixArt-XL-2-512-MS\"  # Smaller for testing\n",
    "        print(\"üß™ SMOKE_MODE: Using 512px model\")\n",
    "    else:\n",
    "        model_id = \"PixArt-alpha/PixArt-XL-2-1024-MS\"  # Full 1024px model\n",
    "\n",
    "    pixart_pipe = load_pixart_pipeline(\n",
    "        model_id=model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        enable_cpu_offload=True,  # Essential for 6-8GB VRAM\n",
    "        enable_attention_slicing=True,\n",
    "        enable_xformers=True,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n‚úÖ PixArt-Œ± pipeline loaded successfully!\")\n",
    "    print(f\"   Model: {model_id}\")\n",
    "    print(f\"   Text Encoder: {pixart_pipe.text_encoder.__class__.__name__}\")\n",
    "    print(f\"   Transformer: {pixart_pipe.transformer.__class__.__name__}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load PixArt-Œ±: {e}\")\n",
    "    pixart_pipe = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f6f83d",
   "metadata": {},
   "source": [
    "Cell 4: T5 Text Encoder Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e5e792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze T5-XXL text encoder capabilities\n",
    "if pixart_pipe is not None:\n",
    "    print(\"üîç T5-XXL Text Encoder Analysis:\\n\")\n",
    "\n",
    "    # Get text encoder details\n",
    "    text_encoder = pixart_pipe.text_encoder\n",
    "    tokenizer = pixart_pipe.tokenizer\n",
    "\n",
    "    print(f\"üìù Model: {text_encoder.config.name_or_path}\")\n",
    "    print(f\"üìè Hidden size: {text_encoder.config.d_model}\")\n",
    "    print(f\"üî¢ Max position embeddings: {tokenizer.model_max_length}\")\n",
    "    print(\n",
    "        f\"üíæ Parameters: ~{sum(p.numel() for p in text_encoder.parameters()) / 1e9:.1f}B\"\n",
    "    )\n",
    "\n",
    "    # Test tokenization with complex prompt\n",
    "    test_prompts = [\n",
    "        \"A majestic dragon breathing fire\",\n",
    "        \"A cyberpunk cityscape at night with neon lights reflecting on wet streets, highly detailed, cinematic lighting, 8k resolution\",\n",
    "        \"Portrait of a beautiful woman with flowing hair in Renaissance style, oil painting, dramatic chiaroscuro lighting, masterpiece\",\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nüß™ Tokenization Analysis:\")\n",
    "    for i, prompt in enumerate(test_prompts, 1):\n",
    "        tokens = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "        print(f\"\\n{i}. Prompt: {prompt[:50]}...\")\n",
    "        print(f\"   Token count: {len(tokens)}\")\n",
    "        print(\n",
    "            f\"   Token capacity usage: {len(tokens)}/{tokenizer.model_max_length} ({len(tokens)/tokenizer.model_max_length*100:.1f}%)\"\n",
    "        )\n",
    "\n",
    "        # Show first few tokens for analysis\n",
    "        decoded_tokens = [tokenizer.decode([t]) for t in tokens[:10]]\n",
    "        print(f\"   First tokens: {decoded_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991df06d",
   "metadata": {},
   "source": [
    "Cell 5: Basic PixArt-Œ± Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fbd6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic PixArt-Œ± inference with parameter exploration\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def generate_pixart_image(\n",
    "    prompt,\n",
    "    negative_prompt=\"blurry, low quality, distorted\",\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    num_inference_steps=28,\n",
    "    guidance_scale=4.5,\n",
    "    num_images_per_prompt=1,\n",
    "    generator=None,\n",
    "    save_path=None,\n",
    "):\n",
    "    \"\"\"Generate image with PixArt-Œ± pipeline\"\"\"\n",
    "\n",
    "    if pixart_pipe is None:\n",
    "        print(\"‚ùå PixArt pipeline not loaded\")\n",
    "        return None\n",
    "\n",
    "    print(f\"üé® Generating with PixArt-Œ±:\")\n",
    "    print(f\"   Prompt: {prompt}\")\n",
    "    print(f\"   Size: {width}x{height}\")\n",
    "    print(f\"   Steps: {num_inference_steps}\")\n",
    "    print(f\"   CFG: {guidance_scale}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # Generate image\n",
    "        with torch.inference_mode():\n",
    "            result = pixart_pipe(\n",
    "                prompt=prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                height=height,\n",
    "                width=width,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                guidance_scale=guidance_scale,\n",
    "                num_images_per_prompt=num_images_per_prompt,\n",
    "                generator=generator,\n",
    "                return_dict=True,\n",
    "            )\n",
    "\n",
    "        generation_time = time.time() - start_time\n",
    "        print(f\"‚è±Ô∏è Generation time: {generation_time:.2f}s\")\n",
    "\n",
    "        image = result.images[0]\n",
    "\n",
    "        # Save if path provided\n",
    "        if save_path:\n",
    "            image.save(save_path)\n",
    "            print(f\"üíæ Saved to: {save_path}\")\n",
    "\n",
    "        return image, generation_time\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Generation failed: {e}\")\n",
    "        return None, 0\n",
    "\n",
    "\n",
    "# Test generation with different parameters\n",
    "test_prompts = [\n",
    "    \"A serene landscape with mountains and a lake, golden hour lighting\",\n",
    "    \"Portrait of a wise old wizard with a long beard, fantasy art style\",\n",
    "    \"Futuristic city with flying cars, cyberpunk aesthetic, neon lights\",\n",
    "]\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"outputs/pixart_test\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Generate test images\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(42)\n",
    "results = []\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Test {i+1}/3\")\n",
    "\n",
    "    # Adjust parameters for SMOKE_MODE\n",
    "    if SMOKE_MODE:\n",
    "        height, width = 512, 512\n",
    "        num_steps = 10\n",
    "        print(\"üß™ SMOKE_MODE: Using 512px, 10 steps\")\n",
    "    else:\n",
    "        height, width = 1024, 1024\n",
    "        num_steps = 28\n",
    "\n",
    "    image, gen_time = generate_pixart_image(\n",
    "        prompt=prompt,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        num_inference_steps=num_steps,\n",
    "        guidance_scale=4.5,\n",
    "        generator=generator,\n",
    "        save_path=output_dir / f\"pixart_test_{i+1}.png\",\n",
    "    )\n",
    "\n",
    "    if image:\n",
    "        results.append(\n",
    "            {\n",
    "                \"prompt\": prompt,\n",
    "                \"image\": image,\n",
    "                \"generation_time\": gen_time,\n",
    "                \"resolution\": f\"{width}x{height}\",\n",
    "                \"steps\": num_steps,\n",
    "            }\n",
    "        )\n",
    "        display(image)\n",
    "\n",
    "    # Memory cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"üìä VRAM usage: {torch.cuda.memory_allocated() / 1e9:.2f}GB\")\n",
    "\n",
    "print(f\"\\n‚úÖ Generated {len(results)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dce2e1",
   "metadata": {},
   "source": [
    "Cell 6: PixArt vs SD Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f0217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Stable Diffusion for comparison\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "\n",
    "def load_sd_pipeline():\n",
    "    \"\"\"Load SD1.5 for comparison\"\"\"\n",
    "    try:\n",
    "        sd_pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            \"runwayml/stable-diffusion-v1-5\",\n",
    "            torch_dtype=torch.float16,\n",
    "            use_safetensors=True,\n",
    "            variant=\"fp16\",\n",
    "        )\n",
    "        sd_pipe.enable_model_cpu_offload()\n",
    "        sd_pipe.enable_attention_slicing()\n",
    "\n",
    "        # Use same scheduler as PixArt for fair comparison\n",
    "        sd_pipe.scheduler = DPMSolverMultistepScheduler.from_config(\n",
    "            sd_pipe.scheduler.config,\n",
    "            use_karras_sigmas=True,\n",
    "            algorithm_type=\"dpmsolver++\",\n",
    "        )\n",
    "\n",
    "        print(\"‚úÖ SD1.5 pipeline loaded for comparison\")\n",
    "        return sd_pipe\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load SD1.5: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Comparison test\n",
    "if not SMOKE_MODE and pixart_pipe is not None:\n",
    "    print(\"üî¨ PixArt-Œ± vs Stable Diffusion Comparison\\n\")\n",
    "\n",
    "    sd_pipe = load_sd_pipeline()\n",
    "\n",
    "    if sd_pipe is not None:\n",
    "        comparison_prompt = (\n",
    "            \"A beautiful sunset over a mountain lake with perfect reflections\"\n",
    "        )\n",
    "\n",
    "        # Test same prompt with both models\n",
    "        generator = torch.Generator(device=\"cpu\").manual_seed(42)\n",
    "\n",
    "        print(\"üé® Generating with PixArt-Œ±...\")\n",
    "        pixart_img, pixart_time = generate_pixart_image(\n",
    "            prompt=comparison_prompt,\n",
    "            height=1024,\n",
    "            width=1024,\n",
    "            num_inference_steps=28,\n",
    "            guidance_scale=4.5,\n",
    "            generator=generator,\n",
    "            save_path=output_dir / \"comparison_pixart.png\",\n",
    "        )\n",
    "\n",
    "        print(\"\\nüé® Generating with SD1.5...\")\n",
    "        generator = torch.Generator(device=\"cpu\").manual_seed(42)\n",
    "        start_time = time.time()\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            sd_result = sd_pipe(\n",
    "                prompt=comparison_prompt,\n",
    "                height=512,  # SD1.5 native resolution\n",
    "                width=512,\n",
    "                num_inference_steps=28,\n",
    "                guidance_scale=7.5,\n",
    "                generator=generator,\n",
    "            )\n",
    "\n",
    "        sd_time = time.time() - start_time\n",
    "        sd_img = sd_result.images[0]\n",
    "        sd_img.save(output_dir / \"comparison_sd15.png\")\n",
    "\n",
    "        # Display comparison\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "        axes[0].imshow(pixart_img)\n",
    "        axes[0].set_title(\n",
    "            f\"PixArt-Œ± (1024x1024)\\nTime: {pixart_time:.2f}s\", fontsize=12\n",
    "        )\n",
    "        axes[0].axis(\"off\")\n",
    "\n",
    "        axes[1].imshow(sd_img)\n",
    "        axes[1].set_title(f\"SD1.5 (512x512)\\nTime: {sd_time:.2f}s\", fontsize=12)\n",
    "        axes[1].axis(\"off\")\n",
    "\n",
    "        plt.suptitle(f\"Prompt: {comparison_prompt[:50]}...\", fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Performance metrics\n",
    "        print(f\"\\nüìä Performance Comparison:\")\n",
    "        print(\n",
    "            f\"   PixArt-Œ±: {pixart_time:.2f}s for 1024x1024 ({1024*1024/pixart_time/1000:.1f}K pixels/s)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   SD1.5:    {sd_time:.2f}s for 512x512 ({512*512/sd_time/1000:.1f}K pixels/s)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   Resolution advantage: PixArt-Œ± generates {(1024/512)**2:.1f}x more pixels\"\n",
    "        )\n",
    "\n",
    "        # Cleanup\n",
    "        del sd_pipe\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7a5f66",
   "metadata": {},
   "source": [
    "Cell 7: DiT Model Architecture Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfa3346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect DiT transformer architecture\n",
    "if pixart_pipe is not None:\n",
    "    print(\"üîç DiT Transformer Architecture Analysis:\\n\")\n",
    "\n",
    "    transformer = pixart_pipe.transformer\n",
    "\n",
    "    # Model configuration\n",
    "    config = transformer.config\n",
    "    print(f\"üìã Model Configuration:\")\n",
    "    print(f\"   ‚Ä¢ Model type: {config.__class__.__name__}\")\n",
    "    print(f\"   ‚Ä¢ Hidden size: {config.hidden_size}\")\n",
    "    print(f\"   ‚Ä¢ Num layers: {config.num_layers}\")\n",
    "    print(f\"   ‚Ä¢ Num attention heads: {config.num_attention_heads}\")\n",
    "    print(f\"   ‚Ä¢ Intermediate size: {config.intermediate_size}\")\n",
    "    print(f\"   ‚Ä¢ Max position embeddings: {config.max_position_embeddings}\")\n",
    "\n",
    "    # Parameter counting\n",
    "    total_params = sum(p.numel() for p in transformer.parameters())\n",
    "    trainable_params = sum(\n",
    "        p.numel() for p in transformer.parameters() if p.requires_grad\n",
    "    )\n",
    "\n",
    "    print(f\"\\nüìä Parameter Statistics:\")\n",
    "    print(f\"   ‚Ä¢ Total parameters: {total_params / 1e6:.1f}M\")\n",
    "    print(f\"   ‚Ä¢ Trainable parameters: {trainable_params / 1e6:.1f}M\")\n",
    "    print(f\"   ‚Ä¢ Memory footprint (fp16): ~{total_params * 2 / 1e9:.2f}GB\")\n",
    "\n",
    "    # Layer inspection\n",
    "    print(f\"\\nüèóÔ∏è Layer Structure:\")\n",
    "    for i, (name, module) in enumerate(transformer.named_children()):\n",
    "        if i < 5:  # Show first 5 layers\n",
    "            param_count = sum(p.numel() for p in module.parameters())\n",
    "            print(\n",
    "                f\"   ‚Ä¢ {name}: {module.__class__.__name__} ({param_count / 1e6:.1f}M params)\"\n",
    "            )\n",
    "\n",
    "    # Attention pattern analysis\n",
    "    if hasattr(transformer, \"transformer_blocks\"):\n",
    "        block = transformer.transformer_blocks[0]\n",
    "        print(f\"\\nüß† Transformer Block Structure:\")\n",
    "        for name, layer in block.named_children():\n",
    "            param_count = sum(p.numel() for p in layer.parameters())\n",
    "            print(\n",
    "                f\"   ‚Ä¢ {name}: {layer.__class__.__name__} ({param_count / 1e6:.1f}M params)\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0017c461",
   "metadata": {},
   "source": [
    "Cell 8: Advanced Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd024d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced parameter exploration for PixArt-Œ±\n",
    "def parameter_sweep_experiment():\n",
    "    \"\"\"Test different parameter combinations\"\"\"\n",
    "\n",
    "    if pixart_pipe is None or SMOKE_MODE:\n",
    "        print(\"‚è≠Ô∏è Skipping parameter sweep (pipeline not loaded or SMOKE_MODE)\")\n",
    "        return\n",
    "\n",
    "    print(\"üß™ Parameter Sweep Experiment\\n\")\n",
    "\n",
    "    base_prompt = \"A magical forest with glowing mushrooms and fireflies\"\n",
    "\n",
    "    # Test different guidance scales\n",
    "    guidance_scales = [3.0, 4.5, 6.0, 8.0]\n",
    "    step_counts = [20, 28, 40]\n",
    "\n",
    "    results_grid = []\n",
    "\n",
    "    for steps in step_counts:\n",
    "        for cfg in guidance_scales:\n",
    "            print(f\"üéØ Testing: Steps={steps}, CFG={cfg}\")\n",
    "\n",
    "            generator = torch.Generator(device=\"cpu\").manual_seed(42)\n",
    "\n",
    "            image, gen_time = generate_pixart_image(\n",
    "                prompt=base_prompt,\n",
    "                height=512,  # Smaller for faster testing\n",
    "                width=512,\n",
    "                num_inference_steps=steps,\n",
    "                guidance_scale=cfg,\n",
    "                generator=generator,\n",
    "                save_path=output_dir / f\"param_test_s{steps}_cfg{cfg}.png\",\n",
    "            )\n",
    "\n",
    "            if image:\n",
    "                results_grid.append(\n",
    "                    {\"steps\": steps, \"cfg\": cfg, \"image\": image, \"time\": gen_time}\n",
    "                )\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Create comparison grid\n",
    "    if results_grid:\n",
    "        fig, axes = plt.subplots(\n",
    "            len(step_counts), len(guidance_scales), figsize=(16, 12)\n",
    "        )\n",
    "\n",
    "        for i, steps in enumerate(step_counts):\n",
    "            for j, cfg in enumerate(guidance_scales):\n",
    "                # Find matching result\n",
    "                result = next(\n",
    "                    (\n",
    "                        r\n",
    "                        for r in results_grid\n",
    "                        if r[\"steps\"] == steps and r[\"cfg\"] == cfg\n",
    "                    ),\n",
    "                    None,\n",
    "                )\n",
    "\n",
    "                if result:\n",
    "                    axes[i, j].imshow(result[\"image\"])\n",
    "                    axes[i, j].set_title(\n",
    "                        f\"Steps: {steps}, CFG: {cfg}\\n{result['time']:.1f}s\"\n",
    "                    )\n",
    "                else:\n",
    "                    axes[i, j].text(0.5, 0.5, \"Failed\", ha=\"center\", va=\"center\")\n",
    "\n",
    "                axes[i, j].axis(\"off\")\n",
    "\n",
    "        plt.suptitle(f\"PixArt-Œ± Parameter Sweep\\nPrompt: {base_prompt}\", fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Performance analysis\n",
    "        print(f\"\\nüìä Performance Summary:\")\n",
    "        for result in results_grid:\n",
    "            efficiency = 512 * 512 / result[\"time\"] / 1000  # K pixels/second\n",
    "            print(\n",
    "                f\"   Steps {result['steps']:2d}, CFG {result['cfg']:3.1f}: \"\n",
    "                f\"{result['time']:5.1f}s ({efficiency:4.1f}K px/s)\"\n",
    "            )\n",
    "\n",
    "\n",
    "# Run parameter sweep\n",
    "parameter_sweep_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3233a84",
   "metadata": {},
   "source": [
    "Cell 9: Memory Usage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af42e264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed memory usage analysis\n",
    "def analyze_memory_usage():\n",
    "    \"\"\"Analyze memory usage patterns during generation\"\"\"\n",
    "\n",
    "    if not torch.cuda.is_available() or pixart_pipe is None:\n",
    "        print(\"‚è≠Ô∏è Skipping memory analysis (no CUDA or pipeline)\")\n",
    "        return\n",
    "\n",
    "    print(\"üß† Memory Usage Analysis\\n\")\n",
    "\n",
    "    def get_memory_stats():\n",
    "        return {\n",
    "            \"allocated\": torch.cuda.memory_allocated() / 1e9,\n",
    "            \"reserved\": torch.cuda.memory_reserved() / 1e9,\n",
    "            \"max_allocated\": torch.cuda.max_memory_allocated() / 1e9,\n",
    "        }\n",
    "\n",
    "    # Clear memory and reset stats\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    print(\"üìä Memory checkpoints:\")\n",
    "\n",
    "    # Baseline\n",
    "    baseline = get_memory_stats()\n",
    "    print(\n",
    "        f\"1. Baseline: {baseline['allocated']:.2f}GB allocated, {baseline['reserved']:.2f}GB reserved\"\n",
    "    )\n",
    "\n",
    "    # During text encoding\n",
    "    prompt = \"A detailed cyberpunk cityscape with neon signs and rain\"\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        # Text encoding phase\n",
    "        prompt_embeds = pixart_pipe._encode_prompt(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=\"blurry, low quality\",\n",
    "            do_classifier_free_guidance=True,\n",
    "            num_images_per_prompt=1,\n",
    "        )\n",
    "\n",
    "        text_encoding = get_memory_stats()\n",
    "        print(f\"2. After text encoding: {text_encoding['allocated']:.2f}GB allocated\")\n",
    "\n",
    "        # During denoising (sample a few steps)\n",
    "        if not SMOKE_MODE:\n",
    "            # Initialize latents\n",
    "            height, width = 1024, 1024\n",
    "            latents = pixart_pipe.prepare_latents(\n",
    "                batch_size=1,\n",
    "                num_channels_latents=4,\n",
    "                height=height,\n",
    "                width=width,\n",
    "                dtype=torch.float16,\n",
    "                device=\"cuda\",\n",
    "                generator=torch.Generator(device=\"cpu\").manual_seed(42),\n",
    "            )\n",
    "\n",
    "            latent_init = get_memory_stats()\n",
    "            print(f\"3. After latent init: {latent_init['allocated']:.2f}GB allocated\")\n",
    "\n",
    "            # Single denoising step\n",
    "            timestep = torch.tensor([500], device=\"cuda\")\n",
    "            noise_pred = pixart_pipe.transformer(\n",
    "                latents,\n",
    "                timestep,\n",
    "                encoder_hidden_states=prompt_embeds,\n",
    "                return_dict=False,\n",
    "            )[0]\n",
    "\n",
    "            denoising = get_memory_stats()\n",
    "            print(\n",
    "                f\"4. After transformer forward: {denoising['allocated']:.2f}GB allocated\"\n",
    "            )\n",
    "\n",
    "            # VAE decode\n",
    "            with torch.no_grad():\n",
    "                image = pixart_pipe.vae.decode(\n",
    "                    latents / pixart_pipe.vae.config.scaling_factor, return_dict=False\n",
    "                )[0]\n",
    "\n",
    "            vae_decode = get_memory_stats()\n",
    "            print(f\"5. After VAE decode: {vae_decode['allocated']:.2f}GB allocated\")\n",
    "\n",
    "        # Peak memory\n",
    "        peak = get_memory_stats()\n",
    "        print(f\"\\nüîù Peak memory: {peak['max_allocated']:.2f}GB\")\n",
    "\n",
    "    # Memory optimization recommendations\n",
    "    print(f\"\\nüí° Memory Optimization Tips:\")\n",
    "    print(f\"   ‚Ä¢ Enable CPU offload: Reduces VRAM by ~2-4GB\")\n",
    "    print(f\"   ‚Ä¢ Use attention slicing: Reduces peak memory during attention\")\n",
    "    print(f\"   ‚Ä¢ Lower resolution: 512x512 uses ~4x less memory than 1024x1024\")\n",
    "    print(f\"   ‚Ä¢ Batch size 1: Avoid multiple images simultaneously\")\n",
    "    print(f\"   ‚Ä¢ torch.cuda.empty_cache(): Clear memory between generations\")\n",
    "\n",
    "\n",
    "# Run memory analysis\n",
    "analyze_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05258412",
   "metadata": {},
   "source": [
    "Cell 10: DiT Family Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f08efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different DiT family models\n",
    "def compare_dit_models():\n",
    "    \"\"\"Compare PixArt-Œ± variants and other DiT models\"\"\"\n",
    "\n",
    "    print(\"üî¨ DiT Family Model Comparison\\n\")\n",
    "\n",
    "    # Model specifications\n",
    "    dit_models = {\n",
    "        \"PixArt-Œ±-512\": {\n",
    "            \"model_id\": \"PixArt-alpha/PixArt-XL-2-512-MS\",\n",
    "            \"resolution\": \"512x512\",\n",
    "            \"parameters\": \"611M\",\n",
    "            \"text_encoder\": \"T5-XXL\",\n",
    "            \"strengths\": \"Fast inference, lower VRAM\",\n",
    "            \"use_case\": \"Quick prototyping, low-VRAM setups\",\n",
    "        },\n",
    "        \"PixArt-Œ±-1024\": {\n",
    "            \"model_id\": \"PixArt-alpha/PixArt-XL-2-1024-MS\",\n",
    "            \"resolution\": \"1024x1024\",\n",
    "            \"parameters\": \"611M\",\n",
    "            \"text_encoder\": \"T5-XXL\",\n",
    "            \"strengths\": \"High resolution, detailed outputs\",\n",
    "            \"use_case\": \"High-quality image generation\",\n",
    "        },\n",
    "        \"PixArt-Œ£\": {\n",
    "            \"model_id\": \"PixArt-alpha/PixArt-Sigma-XL-2-1024-MS\",\n",
    "            \"resolution\": \"1024x1024\",\n",
    "            \"parameters\": \"611M\",\n",
    "            \"text_encoder\": \"T5-XXL\",\n",
    "            \"strengths\": \"Improved training, better quality\",\n",
    "            \"use_case\": \"Production use, fine-tuning base\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Display comparison table\n",
    "    print(\"üìã Model Comparison Table:\\n\")\n",
    "    headers = [\"Model\", \"Resolution\", \"Params\", \"Text Encoder\", \"Key Strengths\"]\n",
    "\n",
    "    # Print header\n",
    "    print(\n",
    "        f\"{'Model':<15} | {'Resolution':<12} | {'Params':<8} | {'Text Enc':<10} | {'Strengths'}\"\n",
    "    )\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for name, info in dit_models.items():\n",
    "        print(\n",
    "            f\"{name:<15} | {info['resolution']:<12} | {info['parameters']:<8} | \"\n",
    "            f\"{info['text_encoder']:<10} | {info['strengths'][:30]}...\"\n",
    "        )\n",
    "\n",
    "    print(f\"\\nüéØ Model Selection Guide:\")\n",
    "    for name, info in dit_models.items():\n",
    "        print(f\"\\n‚Ä¢ {name}:\")\n",
    "        print(f\"  Use case: {info['use_case']}\")\n",
    "        print(f\"  Model ID: {info['model_id']}\")\n",
    "\n",
    "    # Performance expectations\n",
    "    print(f\"\\n‚ö° Performance Expectations (approximate):\")\n",
    "    print(f\"   ‚Ä¢ PixArt-Œ±-512:  ~15-25s on RTX 3080 (28 steps)\")\n",
    "    print(f\"   ‚Ä¢ PixArt-Œ±-1024: ~25-40s on RTX 3080 (28 steps)\")\n",
    "    print(f\"   ‚Ä¢ PixArt-Œ£:      ~25-40s on RTX 3080 (28 steps, better quality)\")\n",
    "\n",
    "    print(f\"\\nüîß VRAM Requirements:\")\n",
    "    print(f\"   ‚Ä¢ 6GB VRAM:  PixArt-Œ±-512 with CPU offload\")\n",
    "    print(f\"   ‚Ä¢ 8GB VRAM:  PixArt-Œ±-1024 with CPU offload\")\n",
    "    print(f\"   ‚Ä¢ 12GB VRAM: All models without CPU offload\")\n",
    "    print(f\"   ‚Ä¢ 16GB+ VRAM: Batch generation, fine-tuning preparation\")\n",
    "\n",
    "\n",
    "compare_dit_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104fe1ce",
   "metadata": {},
   "source": [
    "Cell 11: Smoke Test & CI Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646578bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoke test for CI/CD validation\n",
    "def run_smoke_test():\n",
    "    \"\"\"Minimal test for CI validation\"\"\"\n",
    "\n",
    "    print(\"üß™ Running Smoke Test for CI Validation\\n\")\n",
    "\n",
    "    test_results = {\n",
    "        \"environment\": False,\n",
    "        \"model_loading\": False,\n",
    "        \"basic_inference\": False,\n",
    "        \"memory_management\": False,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Test 1: Environment validation\n",
    "        assert torch.cuda.is_available(), \"CUDA not available\"\n",
    "        assert pixart_pipe is not None, \"PixArt pipeline not loaded\"\n",
    "        test_results[\"environment\"] = True\n",
    "        print(\"‚úÖ Environment validation passed\")\n",
    "\n",
    "        # Test 2: Model loading check\n",
    "        assert hasattr(pixart_pipe, \"transformer\"), \"Transformer not found\"\n",
    "        assert hasattr(pixart_pipe, \"text_encoder\"), \"Text encoder not found\"\n",
    "        assert hasattr(pixart_pipe, \"vae\"), \"VAE not found\"\n",
    "        test_results[\"model_loading\"] = True\n",
    "        print(\"‚úÖ Model loading validation passed\")\n",
    "\n",
    "        # Test 3: Basic inference (minimal)\n",
    "        if SMOKE_MODE:\n",
    "            test_prompt = \"A simple cat\"\n",
    "            test_image, test_time = generate_pixart_image(\n",
    "                prompt=test_prompt,\n",
    "                height=256,  # Very small for speed\n",
    "                width=256,\n",
    "                num_inference_steps=4,  # Minimal steps\n",
    "                guidance_scale=4.5,\n",
    "                generator=torch.Generator(device=\"cpu\").manual_seed(42),\n",
    "            )\n",
    "\n",
    "            assert test_image is not None, \"Failed to generate test image\"\n",
    "            assert test_time > 0, \"Invalid generation time\"\n",
    "            test_results[\"basic_inference\"] = True\n",
    "            print(f\"‚úÖ Basic inference passed ({test_time:.2f}s)\")\n",
    "        else:\n",
    "            test_results[\"basic_inference\"] = True\n",
    "            print(\"‚úÖ Basic inference skipped (not SMOKE_MODE)\")\n",
    "\n",
    "        # Test 4: Memory management\n",
    "        if torch.cuda.is_available():\n",
    "            initial_memory = torch.cuda.memory_allocated()\n",
    "            torch.cuda.empty_cache()\n",
    "            final_memory = torch.cuda.memory_allocated()\n",
    "            assert final_memory <= initial_memory, \"Memory not properly cleared\"\n",
    "            test_results[\"memory_management\"] = True\n",
    "            print(\"‚úÖ Memory management passed\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Smoke test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "    # Summary\n",
    "    passed_tests = sum(test_results.values())\n",
    "    total_tests = len(test_results)\n",
    "\n",
    "    print(f\"\\nüìä Smoke Test Results: {passed_tests}/{total_tests} passed\")\n",
    "\n",
    "    if passed_tests == total_tests:\n",
    "        print(\"üéâ All smoke tests passed! Ready for CI/CD\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Some tests failed. Check configuration.\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "smoke_test_passed = run_smoke_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a095f94",
   "metadata": {},
   "source": [
    "Cell 12: Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66a9987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive performance benchmark\n",
    "def benchmark_pixart_performance():\n",
    "    \"\"\"Benchmark PixArt-Œ± performance across different configurations\"\"\"\n",
    "\n",
    "    if SMOKE_MODE or pixart_pipe is None:\n",
    "        print(\"‚è≠Ô∏è Skipping benchmark (SMOKE_MODE or no pipeline)\")\n",
    "        return\n",
    "\n",
    "    print(\"üèÅ PixArt-Œ± Performance Benchmark\\n\")\n",
    "\n",
    "    benchmark_configs = [\n",
    "        {\"res\": (512, 512), \"steps\": 20, \"cfg\": 4.5, \"desc\": \"Fast\"},\n",
    "        {\"res\": (512, 512), \"steps\": 28, \"cfg\": 4.5, \"desc\": \"Balanced\"},\n",
    "        {\"res\": (768, 768), \"steps\": 28, \"cfg\": 4.5, \"desc\": \"High-Res\"},\n",
    "        {\"res\": (1024, 1024), \"steps\": 28, \"cfg\": 4.5, \"desc\": \"Max-Res\"},\n",
    "    ]\n",
    "\n",
    "    test_prompt = \"A photorealistic portrait of a person with natural lighting\"\n",
    "    benchmark_results = []\n",
    "\n",
    "    for i, config in enumerate(benchmark_configs):\n",
    "        print(\n",
    "            f\"üéØ Test {i+1}/4 - {config['desc']} ({config['res'][0]}x{config['res'][1]}, {config['steps']} steps)\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            # Clear memory before each test\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "            generator = torch.Generator(device=\"cpu\").manual_seed(42)\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            image, gen_time = generate_pixart_image(\n",
    "                prompt=test_prompt,\n",
    "                height=config[\"res\"][1],\n",
    "                width=config[\"res\"][0],\n",
    "                num_inference_steps=config[\"steps\"],\n",
    "                guidance_scale=config[\"cfg\"],\n",
    "                generator=generator,\n",
    "            )\n",
    "\n",
    "            if image:\n",
    "                total_pixels = config[\"res\"][0] * config[\"res\"][1]\n",
    "                pixels_per_second = total_pixels / gen_time\n",
    "                peak_memory = torch.cuda.max_memory_allocated() / 1e9\n",
    "\n",
    "                result = {\n",
    "                    \"config\": config[\"desc\"],\n",
    "                    \"resolution\": f\"{config['res'][0]}x{config['res'][1]}\",\n",
    "                    \"steps\": config[\"steps\"],\n",
    "                    \"time\": gen_time,\n",
    "                    \"pixels_per_sec\": pixels_per_second,\n",
    "                    \"peak_memory\": peak_memory,\n",
    "                    \"image\": image,\n",
    "                }\n",
    "\n",
    "                benchmark_results.append(result)\n",
    "\n",
    "                print(f\"   ‚è±Ô∏è Time: {gen_time:.2f}s\")\n",
    "                print(f\"   üñºÔ∏è Throughput: {pixels_per_second/1000:.1f}K pixels/s\")\n",
    "                print(f\"   üß† Peak VRAM: {peak_memory:.2f}GB\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Failed: {e}\")\n",
    "\n",
    "    # Results summary\n",
    "    if benchmark_results:\n",
    "        print(f\"\\nüìä Benchmark Summary:\")\n",
    "        print(\n",
    "            f\"{'Config':<12} | {'Resolution':<12} | {'Time (s)':<10} | {'Throughput':<12} | {'VRAM (GB)'}\"\n",
    "        )\n",
    "        print(\"-\" * 65)\n",
    "\n",
    "        for result in benchmark_results:\n",
    "            throughput_str = f\"{result['pixels_per_sec']/1000:.1f}K px/s\"\n",
    "            print(\n",
    "                f\"{result['config']:<12} | {result['resolution']:<12} | \"\n",
    "                f\"{result['time']:<10.2f} | {throughput_str:<12} | {result['peak_memory']:<8.2f}\"\n",
    "            )\n",
    "\n",
    "        # Find best configurations\n",
    "        fastest = min(benchmark_results, key=lambda x: x[\"time\"])\n",
    "        most_efficient = max(benchmark_results, key=lambda x: x[\"pixels_per_sec\"])\n",
    "\n",
    "        print(f\"\\nüèÜ Performance Leaders:\")\n",
    "        print(f\"   ‚Ä¢ Fastest: {fastest['config']} ({fastest['time']:.2f}s)\")\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Most efficient: {most_efficient['config']} ({most_efficient['pixels_per_sec']/1000:.1f}K px/s)\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_pixart_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaad8310",
   "metadata": {},
   "source": [
    "Cell 13: Cleanup & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee0a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup and learning summary\n",
    "def cleanup_and_summarize():\n",
    "    \"\"\"Clean up resources and summarize learning outcomes\"\"\"\n",
    "\n",
    "    print(\"üßπ Cleaning up resources...\")\n",
    "\n",
    "    # Clear GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    # Remove pipeline from memory if needed\n",
    "    global pixart_pipe\n",
    "    if \"pixart_pipe\" in globals():\n",
    "        del pixart_pipe\n",
    "        pixart_pipe = None\n",
    "\n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"‚úÖ Resources cleaned up\")\n",
    "\n",
    "    # Learning summary\n",
    "    print(f\"\\nüìö Learning Summary - PixArt-Œ±/DiT Architecture:\")\n",
    "    print(f\"\\nüéØ Key Concepts Learned:\")\n",
    "    print(f\"   ‚Ä¢ DiT (Diffusion Transformers) replaces UNet with pure Transformer\")\n",
    "    print(f\"   ‚Ä¢ T5-XXL text encoder provides richer text understanding (120+ tokens)\")\n",
    "    print(\n",
    "        f\"   ‚Ä¢ AdaLN-Zero conditioning integrates text features via adaptive layer norm\"\n",
    "    )\n",
    "    print(f\"   ‚Ä¢ Scalable architecture allows for larger models and better quality\")\n",
    "    print(f\"   ‚Ä¢ Higher memory requirements but better text-image alignment\")\n",
    "\n",
    "    print(f\"\\n‚öôÔ∏è Technical Skills Gained:\")\n",
    "    print(f\"   ‚Ä¢ Loading and optimizing DiT models for different VRAM setups\")\n",
    "    print(f\"   ‚Ä¢ Understanding memory patterns in Transformer-based diffusion\")\n",
    "    print(f\"   ‚Ä¢ Parameter tuning specific to DiT architecture (lower CFG scales)\")\n",
    "    print(f\"   ‚Ä¢ Performance benchmarking across resolutions and step counts\")\n",
    "    print(f\"   ‚Ä¢ Comparison methodology between different model architectures\")\n",
    "\n",
    "    print(f\"\\nüö® Common Pitfalls Identified:\")\n",
    "    print(f\"   ‚Ä¢ DiT models require more VRAM than comparable UNet models\")\n",
    "    print(f\"   ‚Ä¢ T5-XXL text encoder is large (~11GB) - use CPU offload\")\n",
    "    print(f\"   ‚Ä¢ Lower CFG scales (3.0-6.0) work better than SD's 7.5-15.0\")\n",
    "    print(f\"   ‚Ä¢ Generation time scales significantly with resolution\")\n",
    "    print(f\"   ‚Ä¢ Memory fragmentation can cause OOM even with sufficient total VRAM\")\n",
    "\n",
    "    print(f\"\\nüîÑ Next Steps:\")\n",
    "    print(f\"   ‚Ä¢ Explore PixArt-Œ£ for improved quality\")\n",
    "    print(f\"   ‚Ä¢ Investigate DiT fine-tuning strategies (Stage 3)\")\n",
    "    print(f\"   ‚Ä¢ Compare with other DiT models (DiT-XL, Playground v2.5)\")\n",
    "    print(f\"   ‚Ä¢ Integrate into batch pipeline workflows (Stage 4)\")\n",
    "    print(f\"   ‚Ä¢ Test prompt engineering techniques specific to T5-XXL\")\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "# Execute cleanup and summary\n",
    "success = cleanup_and_summarize()\n",
    "\n",
    "if success:\n",
    "    print(f\"\\nüéâ Notebook completed successfully!\")\n",
    "    print(f\"üìÅ Outputs saved to: {output_dir}\")\n",
    "    print(f\"üîó Ready to proceed to next notebook in the series\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6397a10c",
   "metadata": {},
   "source": [
    "üéØ Key Takeaways\n",
    "‚úÖ Completed Learning Objectives\n",
    "\n",
    "DiT Architecture Understanding - Learned how Transformers replace UNet backbone\n",
    "PixArt-Œ± Implementation - Successfully loaded and used T5-XXL + DiT pipeline\n",
    "Performance Analysis - Benchmarked across resolutions and compared with SD\n",
    "Memory Optimization - Applied CPU offload and attention slicing for low-VRAM setups\n",
    "Parameter Exploration - Discovered optimal CFG scales and step counts for DiT\n",
    "\n",
    "üß† Core Concepts\n",
    "\n",
    "DiT = Diffusion Transformers: Pure Transformer architecture for diffusion models\n",
    "T5-XXL Integration: Powerful text encoder with 120+ token capacity\n",
    "AdaLN Conditioning: Text features integrated via adaptive layer normalization\n",
    "Scalability: DiT architecture scales better with model size than UNet\n",
    "Memory Trade-offs: Higher VRAM requirements but better text-image alignment\n",
    "\n",
    "‚ö†Ô∏è Common Pitfalls\n",
    "\n",
    "DiT models need more VRAM than equivalent UNet models\n",
    "T5-XXL text encoder is very large (~11GB) - always use CPU offload\n",
    "Lower CFG scales (3.0-6.0) work better than traditional SD values\n",
    "Memory fragmentation can cause unexpected OOM errors\n",
    "Generation time increases significantly with resolution\n",
    "\n",
    "‚û°Ô∏è Next Steps\n",
    "\n",
    "Compare with Stable Cascade in nb-cascade-quickstart.ipynb\n",
    "Explore fine-tuning preparation for Stage 3 DiT adaptation\n",
    "Integration planning for Stage 4 batch pipelines\n",
    "Advanced conditioning techniques in Stage 2 continuation\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
