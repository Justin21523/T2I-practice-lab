{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b6b04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ControlNet 基礎實作 - Canny/Depth/OpenPose\n",
    "# Stage 2 | 40_conditioning/nb-sd-controlnet-basics.ipynb\n",
    "\n",
    "# %% [1] Shared Cache Bootstrap\n",
    "import os, pathlib, torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Shared cache configuration (複製到每本 notebook)\n",
    "AI_CACHE_ROOT = os.getenv(\"AI_CACHE_ROOT\", \"../ai_warehouse/cache\")\n",
    "\n",
    "for k, v in {\n",
    "    \"HF_HOME\": f\"{AI_CACHE_ROOT}/hf\",\n",
    "    \"TRANSFORMERS_CACHE\": f\"{AI_CACHE_ROOT}/hf/transformers\",\n",
    "    \"HF_DATASETS_CACHE\": f\"{AI_CACHE_ROOT}/hf/datasets\",\n",
    "    \"HUGGINGFACE_HUB_CACHE\": f\"{AI_CACHE_ROOT}/hf/hub\",\n",
    "    \"TORCH_HOME\": f\"{AI_CACHE_ROOT}/torch\",\n",
    "}.items():\n",
    "    os.environ[k] = v\n",
    "    pathlib.Path(v).mkdir(parents=True, exist_ok=True)\n",
    "print(\"[Cache]\", AI_CACHE_ROOT, \"| GPU:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8816e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 2: Dependencies Installation & Imports\n",
    "# Install required packages (run once)\n",
    "# !pip install diffusers[torch] transformers accelerate xformers controlnet-aux opencv-python pillow\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Optional, Union\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Core ML libraries\n",
    "import torch\n",
    "from diffusers import (\n",
    "    StableDiffusionControlNetPipeline,\n",
    "    ControlNetModel,\n",
    "    UniPCMultistepScheduler,\n",
    ")\n",
    "from controlnet_aux import CannyDetector, MidasDetector, OpenposeDetector\n",
    "\n",
    "print(\n",
    "    f\"🔧 Torch: {torch.__version__} | Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b270ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 3: ControlNet Pipeline Setup (SD1.5 + 3 ControlNets)\n",
    "\n",
    "\n",
    "def setup_controlnet_pipeline(\n",
    "    base_model: str = \"runwayml/stable-diffusion-v1-5\",\n",
    "    device: str = \"cuda\",\n",
    "    enable_memory_efficient: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Setup ControlNet pipelines for Canny, Depth, and OpenPose\n",
    "    Returns dict with separate pipelines for memory efficiency\n",
    "    \"\"\"\n",
    "\n",
    "    # ControlNet model IDs\n",
    "    controlnet_models = {\n",
    "        \"canny\": \"lllyasviel/sd-controlnet-canny\",\n",
    "        \"depth\": \"lllyasviel/sd-controlnet-depth\",\n",
    "        \"openpose\": \"lllyasviel/sd-controlnet-openpose\",\n",
    "    }\n",
    "\n",
    "    pipelines = {}\n",
    "\n",
    "    for control_type, model_id in controlnet_models.items():\n",
    "        print(f\"Loading {control_type} ControlNet...\")\n",
    "\n",
    "        # Load ControlNet model\n",
    "        controlnet = ControlNetModel.from_pretrained(\n",
    "            model_id, torch_dtype=torch.float16, use_safetensors=True\n",
    "        )\n",
    "\n",
    "        # Create pipeline\n",
    "        pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "            base_model,\n",
    "            controlnet=controlnet,\n",
    "            torch_dtype=torch.float16,\n",
    "            safety_checker=None,\n",
    "            requires_safety_checker=False,\n",
    "        )\n",
    "\n",
    "        # Memory optimizations\n",
    "        if enable_memory_efficient:\n",
    "            pipe.enable_model_cpu_offload()  # Offload to CPU when not in use\n",
    "            pipe.enable_attention_slicing()  # Reduce attention memory\n",
    "            if hasattr(pipe, \"enable_xformers_memory_efficient_attention\"):\n",
    "                pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "        # Faster scheduler\n",
    "        pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "        pipelines[control_type] = pipe\n",
    "\n",
    "        # Clear VRAM between loads\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    print(f\"✅ Loaded {len(pipelines)} ControlNet pipelines\")\n",
    "    return pipelines\n",
    "\n",
    "\n",
    "# Setup preprocessors\n",
    "def setup_preprocessors():\n",
    "    \"\"\"Initialize ControlNet preprocessors\"\"\"\n",
    "    return {\n",
    "        \"canny\": CannyDetector(),\n",
    "        \"depth\": MidasDetector.from_pretrained(\"lllyasviel/Annotators\"),\n",
    "        \"openpose\": OpenposeDetector.from_pretrained(\"lllyasviel/Annotators\"),\n",
    "    }\n",
    "\n",
    "\n",
    "# Load pipelines and preprocessors\n",
    "print(\"🚀 Setting up ControlNet pipelines...\")\n",
    "cn_pipelines = setup_controlnet_pipeline()\n",
    "cn_preprocessors = setup_preprocessors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0ec13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 4: Image Preprocessing Functions\n",
    "\n",
    "\n",
    "def preprocess_canny(\n",
    "    image: Image.Image, low_threshold: int = 100, high_threshold: int = 200\n",
    ") -> Image.Image:\n",
    "    \"\"\"Extract Canny edges from input image\"\"\"\n",
    "    canny_image = cn_preprocessors[\"canny\"](image, low_threshold, high_threshold)\n",
    "    return canny_image\n",
    "\n",
    "\n",
    "def preprocess_depth(image: Image.Image) -> Image.Image:\n",
    "    \"\"\"Extract depth map from input image using MiDaS\"\"\"\n",
    "    depth_image = cn_preprocessors[\"depth\"](image)\n",
    "    return depth_image\n",
    "\n",
    "\n",
    "def preprocess_openpose(image: Image.Image) -> Image.Image:\n",
    "    \"\"\"Extract OpenPose keypoints from input image\"\"\"\n",
    "    pose_image = cn_preprocessors[\"openpose\"](image)\n",
    "    return pose_image\n",
    "\n",
    "\n",
    "def create_preprocessing_comparison(input_image: Image.Image) -> Image.Image:\n",
    "    \"\"\"Create side-by-side comparison of all preprocessing methods\"\"\"\n",
    "\n",
    "    # Process with all methods\n",
    "    canny_img = preprocess_canny(input_image)\n",
    "    depth_img = preprocess_depth(input_image)\n",
    "    pose_img = preprocess_openpose(input_image)\n",
    "\n",
    "    # Create comparison grid\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    axes[0].imshow(input_image)\n",
    "    axes[0].set_title(\"Original\", fontsize=12)\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(canny_img, cmap=\"gray\")\n",
    "    axes[1].set_title(\"Canny Edges\", fontsize=12)\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    axes[2].imshow(depth_img)\n",
    "    axes[2].set_title(\"Depth Map\", fontsize=12)\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    axes[3].imshow(pose_img)\n",
    "    axes[3].set_title(\"OpenPose\", fontsize=12)\n",
    "    axes[3].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return canny_img, depth_img, pose_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56425613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 5: MVP Example - Single ControlNet (Canny)\n",
    "\n",
    "\n",
    "def generate_with_controlnet(\n",
    "    pipeline_dict: dict,\n",
    "    control_type: str,\n",
    "    prompt: str,\n",
    "    control_image: Image.Image,\n",
    "    negative_prompt: str = \"blurry, distorted, low quality\",\n",
    "    num_inference_steps: int = 20,\n",
    "    guidance_scale: float = 7.5,\n",
    "    controlnet_conditioning_scale: float = 1.0,\n",
    "    seed: int = 42,\n",
    ") -> Image.Image:\n",
    "    \"\"\"\n",
    "    Generate image with specific ControlNet type\n",
    "    \"\"\"\n",
    "\n",
    "    pipe = pipeline_dict[control_type]\n",
    "    generator = torch.manual_seed(seed)\n",
    "\n",
    "    # Adjust steps for smoke mode\n",
    "    if SMOKE_MODE:\n",
    "        num_inference_steps = min(num_inference_steps, 4)\n",
    "        print(f\"🔥 SMOKE_MODE: Using {num_inference_steps} steps\")\n",
    "\n",
    "    try:\n",
    "        result = pipe(\n",
    "            prompt=prompt,\n",
    "            image=control_image,\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "            generator=generator,\n",
    "        )\n",
    "\n",
    "        return result.images[0]\n",
    "\n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        print(\"🚨 CUDA OOM! Trying CPU offload...\")\n",
    "        pipe.enable_model_cpu_offload()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        result = pipe(\n",
    "            prompt=prompt,\n",
    "            image=control_image,\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_inference_steps=num_inference_steps // 2,  # Reduce steps\n",
    "            guidance_scale=guidance_scale,\n",
    "            controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "            generator=generator,\n",
    "        )\n",
    "        return result.images[0]\n",
    "\n",
    "\n",
    "# MVP Example with sample image\n",
    "def create_sample_image() -> Image.Image:\n",
    "    \"\"\"Create a simple test image for demonstration\"\"\"\n",
    "    img = Image.new(\"RGB\", (512, 512), \"white\")\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    # Draw simple shapes for testing\n",
    "    draw.rectangle([100, 100, 400, 300], outline=\"black\", width=3)\n",
    "    draw.ellipse([200, 150, 300, 250], outline=\"blue\", width=2)\n",
    "    draw.line([50, 50, 450, 450], fill=\"red\", width=2)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "# Run MVP example\n",
    "print(\"🎨 Running MVP example with Canny ControlNet...\")\n",
    "sample_img = create_sample_image()\n",
    "canny_control = preprocess_canny(sample_img)\n",
    "\n",
    "mvp_result = generate_with_controlnet(\n",
    "    cn_pipelines,\n",
    "    \"canny\",\n",
    "    prompt=\"a beautiful landscape painting, oil on canvas, detailed\",\n",
    "    control_image=canny_control,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Display results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].imshow(sample_img)\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(canny_control, cmap=\"gray\")\n",
    "axes[1].set_title(\"Canny Control\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "axes[2].imshow(mvp_result)\n",
    "axes[2].set_title(\"Generated Result\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053677f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 6: Multi-ControlNet Comparison Grid\n",
    "\n",
    "\n",
    "def compare_all_controlnets(\n",
    "    input_image: Image.Image, prompt: str, seed: int = 42\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Generate images with all three ControlNet types for comparison\n",
    "    \"\"\"\n",
    "\n",
    "    # Preprocess input image for all control types\n",
    "    control_images = {\n",
    "        \"canny\": preprocess_canny(input_image),\n",
    "        \"depth\": preprocess_depth(input_image),\n",
    "        \"openpose\": preprocess_openpose(input_image),\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for control_type, control_image in control_images.items():\n",
    "        print(f\"Generating with {control_type}...\")\n",
    "\n",
    "        try:\n",
    "            result = generate_with_controlnet(\n",
    "                cn_pipelines,\n",
    "                control_type,\n",
    "                prompt=prompt,\n",
    "                control_image=control_image,\n",
    "                seed=seed,\n",
    "                num_inference_steps=12 if not SMOKE_MODE else 4,\n",
    "            )\n",
    "            results[control_type] = {\"control\": control_image, \"generated\": result}\n",
    "\n",
    "            # Clear VRAM between generations\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to generate with {control_type}: {e}\")\n",
    "            results[control_type] = None\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run comparison\n",
    "print(\"🔄 Comparing all ControlNet types...\")\n",
    "comparison_prompt = \"a cyberpunk street scene, neon lights, futuristic architecture\"\n",
    "\n",
    "comparison_results = compare_all_controlnets(sample_img, comparison_prompt, seed=42)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "control_types = [\"canny\", \"depth\", \"openpose\"]\n",
    "\n",
    "for i, control_type in enumerate(control_types):\n",
    "    if comparison_results[control_type]:\n",
    "        # Original input\n",
    "        axes[i, 0].imshow(sample_img)\n",
    "        axes[i, 0].set_title(f\"Input Image\")\n",
    "        axes[i, 0].axis(\"off\")\n",
    "\n",
    "        # Control image\n",
    "        control_img = comparison_results[control_type][\"control\"]\n",
    "        if control_type == \"canny\":\n",
    "            axes[i, 1].imshow(control_img, cmap=\"gray\")\n",
    "        else:\n",
    "            axes[i, 1].imshow(control_img)\n",
    "        axes[i, 1].set_title(f\"{control_type.title()} Control\")\n",
    "        axes[i, 1].axis(\"off\")\n",
    "\n",
    "        # Generated result\n",
    "        axes[i, 2].imshow(comparison_results[control_type][\"generated\"])\n",
    "        axes[i, 2].set_title(f\"Generated ({control_type})\")\n",
    "        axes[i, 2].axis(\"off\")\n",
    "    else:\n",
    "        for j in range(3):\n",
    "            axes[i, j].text(\n",
    "                0.5,\n",
    "                0.5,\n",
    "                f\"{control_type} failed\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                transform=axes[i, j].transAxes,\n",
    "            )\n",
    "            axes[i, j].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1e4ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 7: Conditioning Scale Experiments\n",
    "\n",
    "\n",
    "def test_conditioning_scales(\n",
    "    pipeline_dict: dict,\n",
    "    control_type: str,\n",
    "    prompt: str,\n",
    "    control_image: Image.Image,\n",
    "    scales: List[float] = [0.5, 1.0, 1.5, 2.0],\n",
    "    seed: int = 42,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Test different controlnet_conditioning_scale values\n",
    "    \"\"\"\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for scale in scales:\n",
    "        print(f\"Testing conditioning scale: {scale}\")\n",
    "\n",
    "        try:\n",
    "            result = generate_with_controlnet(\n",
    "                pipeline_dict,\n",
    "                control_type,\n",
    "                prompt=prompt,\n",
    "                control_image=control_image,\n",
    "                controlnet_conditioning_scale=scale,\n",
    "                seed=seed,\n",
    "                num_inference_steps=8 if not SMOKE_MODE else 3,\n",
    "            )\n",
    "            results[scale] = result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed at scale {scale}: {e}\")\n",
    "            results[scale] = None\n",
    "\n",
    "        # Clear memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Test conditioning scales with Canny\n",
    "print(\"📊 Testing conditioning scale effects...\")\n",
    "scale_results = test_conditioning_scales(\n",
    "    cn_pipelines,\n",
    "    \"canny\",\n",
    "    \"a watercolor painting of a mountain landscape\",\n",
    "    canny_control,\n",
    "    scales=[0.5, 1.0, 1.5, 2.0],\n",
    ")\n",
    "\n",
    "# Visualize scale effects\n",
    "fig, axes = plt.subplots(1, len(scale_results), figsize=(20, 5))\n",
    "for i, (scale, result) in enumerate(scale_results.items()):\n",
    "    if result:\n",
    "        axes[i].imshow(result)\n",
    "        axes[i].set_title(f\"Scale: {scale}\")\n",
    "        axes[i].axis(\"off\")\n",
    "    else:\n",
    "        axes[i].text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            f\"Failed\\nScale: {scale}\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            transform=axes[i].transAxes,\n",
    "        )\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d2699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 8: Memory Optimization & Error Handling\n",
    "\n",
    "\n",
    "def optimize_pipeline_memory(pipeline_dict: dict):\n",
    "    \"\"\"Apply aggressive memory optimizations for low-VRAM systems\"\"\"\n",
    "\n",
    "    for name, pipe in pipeline_dict.items():\n",
    "        print(f\"Optimizing {name} pipeline...\")\n",
    "\n",
    "        # Enable all memory optimizations\n",
    "        pipe.enable_model_cpu_offload()\n",
    "        pipe.enable_attention_slicing(\"max\")\n",
    "\n",
    "        # Enable sequential CPU offload for extreme memory savings\n",
    "        try:\n",
    "            pipe.enable_sequential_cpu_offload()\n",
    "            print(f\"✅ {name}: Sequential CPU offload enabled\")\n",
    "        except:\n",
    "            print(f\"⚠️ {name}: Sequential CPU offload not available\")\n",
    "\n",
    "        # VAE slicing for large images\n",
    "        if hasattr(pipe.vae, \"enable_slicing\"):\n",
    "            pipe.vae.enable_slicing()\n",
    "            print(f\"✅ {name}: VAE slicing enabled\")\n",
    "\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        cached = torch.cuda.memory_reserved() / 1e9\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(\n",
    "            f\"GPU Memory: {allocated:.1f}GB allocated, {cached:.1f}GB cached, {total:.1f}GB total\"\n",
    "        )\n",
    "        return allocated, cached, total\n",
    "    return 0, 0, 0\n",
    "\n",
    "\n",
    "# Apply optimizations\n",
    "print(\"🔧 Applying memory optimizations...\")\n",
    "optimize_pipeline_memory(cn_pipelines)\n",
    "get_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117d40b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 9: Batch Processing Function\n",
    "\n",
    "\n",
    "def batch_controlnet_generation(\n",
    "    pipeline_dict: dict,\n",
    "    prompts: List[str],\n",
    "    control_images: List[Image.Image],\n",
    "    control_type: str,\n",
    "    batch_size: int = 1,\n",
    "    **generation_kwargs,\n",
    ") -> List[Image.Image]:\n",
    "    \"\"\"\n",
    "    Process multiple prompts/images in batches to manage memory\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "    pipe = pipeline_dict[control_type]\n",
    "\n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        batch_prompts = prompts[i : i + batch_size]\n",
    "        batch_images = control_images[i : i + batch_size]\n",
    "\n",
    "        print(\n",
    "            f\"Processing batch {i//batch_size + 1}/{(len(prompts)-1)//batch_size + 1}\"\n",
    "        )\n",
    "\n",
    "        for prompt, control_img in zip(batch_prompts, batch_images):\n",
    "            try:\n",
    "                result = generate_with_controlnet(\n",
    "                    pipeline_dict,\n",
    "                    control_type,\n",
    "                    prompt=prompt,\n",
    "                    control_image=control_img,\n",
    "                    **generation_kwargs,\n",
    "                )\n",
    "                results.append(result)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Batch processing error: {e}\")\n",
    "                # Create placeholder for failed generation\n",
    "                placeholder = Image.new(\"RGB\", (512, 512), \"red\")\n",
    "                results.append(placeholder)\n",
    "\n",
    "            # Clear memory after each generation\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example batch processing\n",
    "batch_prompts = [\n",
    "    \"a serene lake at sunset\",\n",
    "    \"a bustling city street\",\n",
    "    \"a magical forest with glowing trees\",\n",
    "]\n",
    "batch_controls = [canny_control] * len(batch_prompts)\n",
    "\n",
    "if not SMOKE_MODE:  # Skip in smoke mode to save time\n",
    "    print(\"📦 Running batch processing example...\")\n",
    "    batch_results = batch_controlnet_generation(\n",
    "        cn_pipelines,\n",
    "        batch_prompts,\n",
    "        batch_controls,\n",
    "        \"canny\",\n",
    "        num_inference_steps=8,\n",
    "        seed=42,\n",
    "    )\n",
    "\n",
    "    # Display batch results\n",
    "    fig, axes = plt.subplots(1, len(batch_results), figsize=(15, 5))\n",
    "    for i, (result, prompt) in enumerate(zip(batch_results, batch_prompts)):\n",
    "        axes[i].imshow(result)\n",
    "        axes[i].set_title(prompt[:20] + \"...\")\n",
    "        axes[i].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38488942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 10: Smoke Test (SMOKE_MODE compatible)\n",
    "\n",
    "\n",
    "def smoke_test_controlnet():\n",
    "    \"\"\"Quick smoke test for CI/CD pipeline\"\"\"\n",
    "\n",
    "    print(\"🔥 Running ControlNet smoke test...\")\n",
    "\n",
    "    # Create minimal test image\n",
    "    test_img = Image.new(\"RGB\", (256, 256), \"white\")\n",
    "    draw = ImageDraw.Draw(test_img)\n",
    "    draw.rectangle([50, 50, 200, 200], outline=\"black\", width=2)\n",
    "\n",
    "    # Test single ControlNet\n",
    "    canny_test = preprocess_canny(test_img)\n",
    "\n",
    "    smoke_result = generate_with_controlnet(\n",
    "        cn_pipelines,\n",
    "        \"canny\",\n",
    "        prompt=\"simple test image\",\n",
    "        control_image=canny_test,\n",
    "        num_inference_steps=2,  # Minimal steps\n",
    "        seed=42,\n",
    "    )\n",
    "\n",
    "    # Verify result\n",
    "    assert isinstance(smoke_result, Image.Image), \"Generation failed\"\n",
    "    assert smoke_result.size == (512, 512), \"Wrong output size\"\n",
    "\n",
    "    print(\"✅ Smoke test passed!\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# Run smoke test\n",
    "try:\n",
    "    smoke_test_controlnet()\n",
    "    print(\"🎉 All systems operational!\")\n",
    "except Exception as e:\n",
    "    print(f\"💥 Smoke test failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7263ad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 11: Results Analysis & Best Practices\n",
    "\n",
    "\n",
    "def analyze_controlnet_results():\n",
    "    \"\"\"Analyze and document ControlNet behavior patterns\"\"\"\n",
    "\n",
    "    analysis = {\n",
    "        \"canny\": {\n",
    "            \"strengths\": [\n",
    "                \"Sharp edge preservation\",\n",
    "                \"Architectural details\",\n",
    "                \"Line art conversion\",\n",
    "            ],\n",
    "            \"weaknesses\": [\"May miss subtle textures\", \"Sensitive to noise\"],\n",
    "            \"best_for\": [\"Buildings\", \"Drawings\", \"Technical illustrations\"],\n",
    "            \"optimal_scale\": \"1.0-1.5\",\n",
    "        },\n",
    "        \"depth\": {\n",
    "            \"strengths\": [\n",
    "                \"3D structure preservation\",\n",
    "                \"Spatial relationships\",\n",
    "                \"Composition control\",\n",
    "            ],\n",
    "            \"weaknesses\": [\"May flatten textures\", \"Less detail preservation\"],\n",
    "            \"best_for\": [\"Landscapes\", \"Portraits\", \"3D scenes\"],\n",
    "            \"optimal_scale\": \"0.8-1.2\",\n",
    "        },\n",
    "        \"openpose\": {\n",
    "            \"strengths\": [\n",
    "                \"Human pose accuracy\",\n",
    "                \"Animation consistency\",\n",
    "                \"Character control\",\n",
    "            ],\n",
    "            \"weaknesses\": [\"Human-only\", \"Requires clear poses\"],\n",
    "            \"best_for\": [\"Human figures\", \"Animation\", \"Character art\"],\n",
    "            \"optimal_scale\": \"1.0-2.0\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    print(\"📋 ControlNet Analysis Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for control_type, info in analysis.items():\n",
    "        print(f\"\\n🎯 {control_type.upper()}:\")\n",
    "        print(f\"   Best for: {', '.join(info['best_for'])}\")\n",
    "        print(f\"   Optimal scale: {info['optimal_scale']}\")\n",
    "        print(f\"   Strengths: {', '.join(info['strengths'])}\")\n",
    "        print(f\"   Considerations: {', '.join(info['weaknesses'])}\")\n",
    "\n",
    "    return analysis\n",
    "\n",
    "\n",
    "# Memory optimization tips\n",
    "def print_optimization_tips():\n",
    "    \"\"\"Print memory optimization recommendations\"\"\"\n",
    "\n",
    "    tips = [\n",
    "        \"🔧 Use torch.float16 for all models\",\n",
    "        \"🔄 Enable model_cpu_offload() for <8GB VRAM\",\n",
    "        \"✂️ Enable attention_slicing('max') always\",\n",
    "        \"🧠 Clear torch.cuda.empty_cache() between generations\",\n",
    "        \"📦 Process in batches of 1 for low memory\",\n",
    "        \"⚡ Use UniPC scheduler for faster inference\",\n",
    "        \"🎯 Tune conditioning_scale: 0.8-1.5 usually optimal\",\n",
    "        \"🖼️ Resize images to 512x512 for best speed/quality balance\",\n",
    "    ]\n",
    "\n",
    "    print(\"\\n💡 Memory Optimization Tips:\")\n",
    "    print(\"=\" * 40)\n",
    "    for tip in tips:\n",
    "        print(f\"  {tip}\")\n",
    "\n",
    "\n",
    "# Run analysis\n",
    "analysis_results = analyze_controlnet_results()\n",
    "print_optimization_tips()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a86e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 12: Stage Summary & Next Steps\n",
    "\n",
    "\n",
    "def stage_summary():\n",
    "    \"\"\"Summarize completed learning objectives and next steps\"\"\"\n",
    "\n",
    "    completed = [\n",
    "        \"✅ Successfully loaded 3 ControlNet pipelines (Canny/Depth/OpenPose)\",\n",
    "        \"✅ Implemented preprocessing functions for all control types\",\n",
    "        \"✅ Tested conditioning scale effects (0.5-2.0 range)\",\n",
    "        \"✅ Applied memory optimizations for 8GB VRAM compatibility\",\n",
    "        \"✅ Created batch processing pipeline\",\n",
    "        \"✅ Established error handling and fallback strategies\",\n",
    "    ]\n",
    "\n",
    "    key_concepts = [\n",
    "        \"🧠 ControlNet = Condition + Diffusion guidance\",\n",
    "        \"🎛️ conditioning_scale controls strength (0.5-2.0 typical)\",\n",
    "        \"🖼️ Different preprocessors for different control types\",\n",
    "        \"💾 Memory management crucial for consumer GPUs\",\n",
    "        \"🔄 Sequential generation better than batch for low VRAM\",\n",
    "    ]\n",
    "\n",
    "    pitfalls = [\n",
    "        \"⚠️ High conditioning_scale (>2.0) may overpower prompt\",\n",
    "        \"⚠️ Canny sensitive to noise - may need preprocessing\",\n",
    "        \"⚠️ OpenPose only works with clear human poses\",\n",
    "        \"⚠️ Always test memory optimizations on target hardware\",\n",
    "        \"⚠️ Preprocessing quality directly affects generation quality\",\n",
    "    ]\n",
    "\n",
    "    next_steps = [\n",
    "        \"🔜 T2I-Adapter implementation (nb-cond-t2iadapter.ipynb)\",\n",
    "        \"🔜 IP-Adapter for style reference (nb-sd-ipadapter-style.ipynb)\",\n",
    "        \"🔜 Multi-ControlNet combinations and blending\",\n",
    "        \"🔜 Custom ControlNet training for specialized use cases\",\n",
    "        \"🔜 Integration with batch pipeline in Stage 4\",\n",
    "    ]\n",
    "\n",
    "    print(\"📊 STAGE 2 - ControlNet Basics COMPLETED\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    print(\"\\n✅ Completed Objectives:\")\n",
    "    for item in completed:\n",
    "        print(f\"  {item}\")\n",
    "\n",
    "    print(\"\\n🧠 Key Concepts Learned:\")\n",
    "    for concept in key_concepts:\n",
    "        print(f\"  {concept}\")\n",
    "\n",
    "    print(\"\\n⚠️ Common Pitfalls to Avoid:\")\n",
    "    for pitfall in pitfalls:\n",
    "        print(f\"  {pitfall}\")\n",
    "\n",
    "    print(\"\\n🔜 Next Steps (Stage 2 Continuation):\")\n",
    "    for step in next_steps:\n",
    "        print(f\"  {step}\")\n",
    "\n",
    "    # Save configuration for reproducibility\n",
    "    config = {\n",
    "        \"base_model\": \"runwayml/stable-diffusion-v1-5\",\n",
    "        \"controlnet_models\": {\n",
    "            \"canny\": \"lllyasviel/sd-controlnet-canny\",\n",
    "            \"depth\": \"lllyasviel/sd-controlnet-depth\",\n",
    "            \"openpose\": \"lllyasviel/sd-controlnet-openpose\",\n",
    "        },\n",
    "        \"default_params\": {\n",
    "            \"num_inference_steps\": 20,\n",
    "            \"guidance_scale\": 7.5,\n",
    "            \"controlnet_conditioning_scale\": 1.0,\n",
    "        },\n",
    "        \"memory_optimizations\": [\n",
    "            \"model_cpu_offload\",\n",
    "            \"attention_slicing\",\n",
    "            \"torch.float16\",\n",
    "            \"sequential_cpu_offload\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    import json\n",
    "\n",
    "    with open(\"controlnet_config.json\", \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "    print(f\"\\n💾 Configuration saved to: controlnet_config.json\")\n",
    "\n",
    "\n",
    "# Run summary\n",
    "stage_summary()\n",
    "\n",
    "\n",
    "# Final memory cleanup\n",
    "def cleanup_resources():\n",
    "    \"\"\"Clean up GPU memory and resources\"\"\"\n",
    "    for name, pipe in cn_pipelines.items():\n",
    "        try:\n",
    "            pipe.to(\"cpu\")\n",
    "            del pipe\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"🧹 Resources cleaned up\")\n",
    "\n",
    "\n",
    "if not SMOKE_MODE:  # Keep loaded for interactive use\n",
    "    print(\"\\n🔄 Pipelines ready for continued experimentation\")\n",
    "    print(\"💡 Call cleanup_resources() when finished to free VRAM\")\n",
    "else:\n",
    "    cleanup_resources()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
